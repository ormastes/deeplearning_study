{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Additional modules for docker environment\n",
    "# ------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "FLAG_FILE = Path(\"tmp/.3.1.Resonsing_Training\") \n",
    "if not FLAG_FILE.exists():\n",
    "    !mkdir tmp\n",
    "    !pip install datasets torch_optimizer lion_pytorch clang_repl_kernel --break-system-packages\n",
    "    !pip install --upgrade clang-repl-kernel --break-system-packages\n",
    "    !pip install torch_tb_profiler\n",
    "    from ClangReplInterface import ClangReplInterface\n",
    "    clang_repl = ClangReplInterface()\n",
    "    clang_repl.kernel.my_shell.del_loop()\n",
    "    clang_repl.kernel.my_shell.process.kill()\n",
    "    clang_repl = None\n",
    "    FLAG_FILE.touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a9182f-c9de-4acd-8cde-16ea2777306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1-min load: 1.21\n",
      "Number of cores: 32\n",
      "Load per core: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Check CPU load\n",
    "# ------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "# Read the 1-minute load average from /proc/loadavg\n",
    "try:\n",
    "    with open(\"/proc/loadavg\", \"r\") as f:\n",
    "        load_avg = float(f.read().split()[0])\n",
    "except Exception as e:\n",
    "    print(\"Error reading /proc/loadavg:\", e)\n",
    "else:\n",
    "    cores = os.cpu_count() or 1\n",
    "    load_per_core = load_avg / cores\n",
    "    print(f\"Total 1-min load: {load_avg:.2f}\")\n",
    "    print(f\"Number of cores: {cores}\")\n",
    "    print(f\"Load per core: {load_per_core:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd5e79d-c340-4a89-b3f2-efdb94bb6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  6 10:13:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:09:00.0 Off |                  Off |\n",
      "| 46%   71C    P0            104W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU load\n",
    "# ------------------------------------------------\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6e617b-71bd-4c7f-b6a3-d8aa3e83471e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import and setup\n",
    "# ------------------------------------------------\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "import inspect\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import signal\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import numpy as np\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from ClangReplInterface import ClangReplInterface, ObjectPool\n",
    "from Config import SimpleConfig\n",
    "from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.profiler import (\n",
    "    ProfilerActivity,\n",
    "    profile,\n",
    "    record_function,\n",
    "    tensorboard_trace_handler,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "# to clean up log\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# for memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# huggingface tokenizer deadlock workaround\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbc2603-df2d-41e0-bd87-cb21e21ec8ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pytorch random seed idiom\n",
    "# ------------------------------------------------\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bc6b7e-b497-46f5-a993-ed6c15a024c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall configuration\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "config = SimpleConfig()\n",
    "max_length = 512\n",
    "MAX_REWARD = 4.4\n",
    "\n",
    "\n",
    "class TrainLayers(Enum):\n",
    "    FULL_LAYER = 1\n",
    "    TWO_FRONT_LAYER = 2\n",
    "    ODD_LAYER = 3\n",
    "    EVEN_LAYER = 4\n",
    "    SWITCH_PAIR_LAYER = 5\n",
    "\n",
    "\n",
    "log_content = False\n",
    "log_step = False\n",
    "log_memory = False\n",
    "log_logits = False\n",
    "log_reward = False\n",
    "log_tensor = False\n",
    "\n",
    "checking_range = False\n",
    "checking_shape = False\n",
    "\n",
    "\n",
    "class HiperParam:\n",
    "    def __init__(self):\n",
    "        self.train_layer = TrainLayers.FULL_LAYER\n",
    "        self.group_size = 4\n",
    "        self.batch_size = 1  # 7\n",
    "        self.category_count_start = 1  # 9\n",
    "\n",
    "        self.num_epochs = 200\n",
    "        self.lr = 3.131e-05\n",
    "        self.kl_lambda = 8\n",
    "        self.epsilon = 0.26207\n",
    "        self.num_grpo = 1\n",
    "        self.save_epochs = 10\n",
    "        self.warming_up_step = 1\n",
    "        self.temperature = 0.53715\n",
    "        self.validation_interval = 3\n",
    "        self.expected_meean_reward = 2.0\n",
    "\n",
    "        self.try_old_model_update_in_batch_loop_update_divider = 2\n",
    "        self.try_old_model_update_in_batch_loop_count = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"lr: {self.lr}, kl_lambda: {self.kl_lambda}, \"\n",
    "            f\"epsilon: {self.epsilon}, num_grpo: {self.num_grpo}, \"\n",
    "            f\"warming_up_step: {self.warming_up_step}, \"\n",
    "            f\"temperature: {self.temperature}, category_count_start: {self.category_count_start}\"\n",
    "        )\n",
    "    def essence_str(self):\n",
    "        lr_str = f\"{self.lr:.5g}\"\n",
    "        kl_lambda_str = f\"{self.kl_lambda:.5g}\"\n",
    "        epsilon_str = f\"{self.epsilon:.5g}\"\n",
    "        temperature_str = f\"{self.temperature:.5g}\"\n",
    "        return f\"{lr_str}_{kl_lambda_str}_{epsilon_str}_{self.num_grpo}_{temperature_str}\"\n",
    "        \n",
    "    def choose_generator(self, cur, old, ref):\n",
    "        return cur;\n",
    "\n",
    "    def try_old_model_update_in_batch_loop(self, model, old_model, batch_step, batch_len, cur_category_count):\n",
    "        update_count = min(1, int((batch_len//cur_category_count)//self.try_old_model_update_in_batch_loop_update_divider))\n",
    "        if (self.try_old_model_update_in_batch_loop_count%update_count == 0) or old_model is None:\n",
    "            old_model = copy_inference_model(model)\n",
    "        self.try_old_model_update_in_batch_loop_count += 1\n",
    "        return old_model\n",
    "            \n",
    "    def get_optimizer(self, params, lr):\n",
    "        self.optimizer = Adafactor(params, lr=lr, relative_step=False, scale_parameter=False)\n",
    "        #self.optimizer = torch.optim.AdamW(params, lr=lr)\n",
    "        return self.optimizer\n",
    "        \n",
    "    def tensor_summary(self):\n",
    "        return {\n",
    "            \"lr\": self.lr,\n",
    "            \"kl_lambda\": self.kl_lambda,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"num_grpo\": self.num_grpo,\n",
    "            \"warming_up_step\": self.warming_up_step,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"category_count_start\": self.category_count_start,\n",
    "            \"optimzier\": type(self.optimizer).__name__,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, prefix, hparam):\n",
    "        self.global_step = 0\n",
    "        self.val_global_step = 0\n",
    "\n",
    "        self.use_reference_model = True\n",
    "        self.skip_validation_step = False\n",
    "        self.is_finding_opt = False\n",
    "        self.is_profile = False\n",
    "        self.is_original_struct = False\n",
    "        \n",
    "        self.log_prefix = prefix\n",
    "        self.ref_checkpoint_path = \"./saved_models/starcoder2-3b_exact_sample/checkpoint.pt\"\n",
    "        self.test_target_object_file = \"manual_data_set/ReasoningTestTarget.json\"\n",
    "        self.learning_name = f\"starcoder2-3b_reasoning_{prefix}_{hparam.train_layer.name}_\"\n",
    "        self.now = datetime.now().strftime(\"%d_%H-%M\")\n",
    "        self.last_checkpoint_path = f\"./saved_models/{self.learning_name}/checkpoint.pt\"\n",
    "        self.checkpoint_dir_pre = f\"./saved_models/{self.learning_name}/epoch_\"\n",
    "        \n",
    "        self.model_id = \"bigcode/starcoder2-3b\"\n",
    "        self.tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # open next url to see profile: http://192.168.1.117:6006/#pytorch_profiler\n",
    "        self.prof = profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n",
    "            on_trace_ready=tensorboard_trace_handler(\"prof_\" + log_dir),\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True,\n",
    "            ) if self.is_profile else None\n",
    "        \n",
    "    def prof_start(self):\n",
    "        if self.prof is not None:\n",
    "            self.prof.start()\n",
    "            \n",
    "    def prof_step(self):\n",
    "        if self.prof is not None:\n",
    "            self.prof.step()            \n",
    "\n",
    "    def prof_start(self):\n",
    "        if self.prof is not None:\n",
    "            self.prof.stop()    \n",
    "\n",
    "hparam = HiperParam()\n",
    "state = State(\"Adafactor\", hparam)\n",
    "\n",
    "\n",
    "def object_hiper_param(trial):\n",
    "    global hparam\n",
    "    # Shortened training for demonstration:\n",
    "    hparam.num_epochs = 1  # 4  # 2   # or 2–3, to save time during hyperparameter search\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    hparam.lr = 3.131e-05  # trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True) # 5e-6\n",
    "    hparam.kl_lambda = trial.suggest_float(\"kl_lambda\", 17, 32)  # 0.04 from\n",
    "    hparam.epsilon = 0.26207  # trial.suggest_float(\"epsilon\", 0.01, 0.3) # 0.1\n",
    "    hparam.num_grpo = 0.5  # 1 #trial.suggest_int(\"num_grpo\", 1, 2, step=1) # 1\n",
    "    hparam.warming_up_step = 1  # trial.suggest_int(\"warming_up_step\", 1, 1, step=1)\n",
    "    hparam.temperature = trial.suggest_float(\"temperature\", 0.5, 1.4)  # 1.0\n",
    "    hparam.category_count_start = 1  # trial.suggest_int(\"category_count_start\", 1, 2, step=1)\n",
    "\n",
    "    return hparam\n",
    "\n",
    "\n",
    "def shrink_dataset(reasoning_dataset, val_reasoning_dataset):\n",
    "    reasoning_dataset = reasoning_dataset\n",
    "    val_reasoning_dataset = val_reasoning_dataset\n",
    "    return reasoning_dataset, val_reasoning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed089d7a-5d79-4f28-877a-e525326c1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging or Print\n",
    "# ------------------------------------------------\n",
    "\n",
    "previous_tensor_info = {}\n",
    "\n",
    "\n",
    "def print_memory(tag):\n",
    "    global previous_tensor_info, log_memory\n",
    "    if not log_memory:\n",
    "        return\n",
    "    # Make sure you have a GPU device available.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Print current allocated and reserved memory in MB:\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)\n",
    "    print(tag)\n",
    "    print(f\"Memory allocated: {allocated:.2f} MB\")\n",
    "    print(f\"Memory reserved: {reserved:.2f} MB\")\n",
    "\n",
    "    if False:\n",
    "        print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "    if True:\n",
    "        # List all CUDA tensors and collect their details in a dictionary.\n",
    "        cuda_tensors = list_cuda_tensors()\n",
    "        current_tensor_info = {\n",
    "            id(tensor): (tensor.shape, tensor.device) for tensor in cuda_tensors\n",
    "        }\n",
    "\n",
    "        # Determine new and deleted tensor IDs.\n",
    "        current_tensor_ids = set(current_tensor_info.keys())\n",
    "        previous_tensor_ids = set(previous_tensor_info.keys())\n",
    "\n",
    "        print(\n",
    "            \"Total Tensors:\",\n",
    "            len(current_tensor_ids),\n",
    "            \", Changes:\",\n",
    "            len(current_tensor_ids) - len(previous_tensor_ids),\n",
    "        )\n",
    "\n",
    "        if False:\n",
    "            # Determine new tensors since the last call.\n",
    "            new_tensor_ids = current_tensor_ids - previous_tensor_ids\n",
    "            deleted_tensor_ids = previous_tensor_ids - current_tensor_ids\n",
    "            if new_tensor_ids:\n",
    "                print(\"New CUDA tensors created since the last call:\")\n",
    "                for tid in new_tensor_ids:\n",
    "                    shape, dev = current_tensor_info[tid]\n",
    "                    print(f\"Tensor id: {tid} | Shape: {shape} | Device: {dev}\")\n",
    "\n",
    "            if deleted_tensor_ids:\n",
    "                print(\"Deleted CUDA tensors since the last call:\")\n",
    "                for tid in deleted_tensor_ids:\n",
    "                    shape, dev = previous_tensor_info[tid]\n",
    "                    print(f\"Tensor id: {tid} | Shape: {shape} | Device: {dev}\")\n",
    "        previous_tensor_info = current_tensor_info.copy()\n",
    "\n",
    "\n",
    "def cur_memory_ids():\n",
    "    # List all CUDA tensors and collect their details in a dictionary.\n",
    "    cuda_tensors = list_cuda_tensors()\n",
    "    current_tensor_info = {\n",
    "        id(tensor): (tensor.shape, tensor.device) for tensor in cuda_tensors\n",
    "    }\n",
    "\n",
    "    # Determine new and deleted tensor IDs.\n",
    "    current_ids = set(current_tensor_info.keys())\n",
    "    return current_ids\n",
    "\n",
    "\n",
    "def compare_memory_ids(previous_tensor_ids):\n",
    "    current_tensor_ids = cur_memory_ids()\n",
    "    new_tensor_ids = current_tensor_ids - previous_tensor_ids\n",
    "    deleted_tensor_ids = previous_tensor_ids - current_tensor_ids\n",
    "    print(\"New tensor:\", new_tensor_ids)\n",
    "    print(\"Delted tensor:\", deleted_tensor_ids)\n",
    "\n",
    "\n",
    "def list_cuda_tensors():\n",
    "    cuda_tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                cuda_tensors.append(obj)\n",
    "        except Exception:\n",
    "            pass  # Some objects might not have the attributes we need.\n",
    "    return cuda_tensors\n",
    "\n",
    "\n",
    "def print_step(tag, main_step=False):\n",
    "    global log_memory\n",
    "    if log_memory:\n",
    "        print_memory(tag)\n",
    "    else:\n",
    "        if log_step or main_step:\n",
    "            print(tag)\n",
    "\n",
    "\n",
    "def check_optimizer_duplicates(optimizer):\n",
    "    seen_ids = set()\n",
    "    duplicates = []\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group[\"params\"]:\n",
    "            pid = id(param)\n",
    "            if pid in seen_ids:\n",
    "                duplicates.append(param)\n",
    "            else:\n",
    "                seen_ids.add(pid)\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def print_logits_ids(tag, logits, ids):\n",
    "    global log_logits\n",
    "    if log_logits:\n",
    "        logits_len = logits.shape[1]\n",
    "        ids_len = ids.shape[1]\n",
    "        if True:\n",
    "            logits_ids = torch.argmax(logits, dim=-1)\n",
    "            ids_text = [\n",
    "                tokenizer.decode(ids[i], skip_special_tokens=True)\n",
    "                for i in range(ids.size(0))\n",
    "            ]\n",
    "            logits_text = [\n",
    "                tokenizer.decode(logits_ids[i], skip_special_tokens=True)\n",
    "                for i in range(logits_ids.size(0))\n",
    "            ]\n",
    "\n",
    "            print(\n",
    "                \"##### \", tag, \"( logits_len:\", logits_len, \", ids_len:\", ids_len, \" )\"\n",
    "            )\n",
    "            print(\n",
    "                \"First five logits_ids:\",\n",
    "                logits_ids[0][:5].tolist(),\n",
    "                \", First five ids:\",\n",
    "                ids[0][:5].tolist(),\n",
    "            )\n",
    "            print(\"###### logit text:\", logits_text[0][:100])\n",
    "            print(\"###### ids_text:\", ids_text[0][:100])\n",
    "    print_tensor(logits, name=tag + \"(logits)\")\n",
    "    print_tensor(ids, name=tag + \"(ids)\")\n",
    "\n",
    "\n",
    "def print_tensor(tensor, name=None):\n",
    "    if log_tensor:\n",
    "        if name is None:\n",
    "            # Try to infer the variable name from the caller's local variables.\n",
    "            frame = inspect.currentframe().f_back\n",
    "            # Look for local variables that are the same object as tensor.\n",
    "            names = [\n",
    "                var_name\n",
    "                for var_name, var_val in frame.f_locals.items()\n",
    "                if var_val is tensor\n",
    "            ]\n",
    "            name = names[0] if names else \"tensor\"\n",
    "        if not torch.is_floating_point(tensor):\n",
    "            mean_val = tensor.float().mean().item()\n",
    "        else:\n",
    "            mean_val = tensor.mean().item()\n",
    "        print(\n",
    "            name,\n",
    "            tensor.shape,\n",
    "            \"(min=\",\n",
    "            tensor.min().item(),\n",
    "            \", avg=\",\n",
    "            mean_val,\n",
    "            \", max=\",\n",
    "            tensor.max().item(),\n",
    "            \")\",\n",
    "        )\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def match_shape(actual, expected):\n",
    "    if len(actual) != len(expected):\n",
    "        return False\n",
    "    return all(e == a or e is None or e == -1 for a, e in zip(actual, expected))\n",
    "\n",
    "\n",
    "def check_shape(self, expected_shape):\n",
    "    if checking_shape:\n",
    "        if not match_shape(self.shape, expected_shape):\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch! Got {self.shape}, expected {expected_shape}\"\n",
    "            )\n",
    "    return self\n",
    "\n",
    "\n",
    "def check_range(self, min_val, max_val, not_values=None):\n",
    "    if checking_range:\n",
    "        # Range check\n",
    "        in_range = (self >= min_val) & (self <= max_val)\n",
    "\n",
    "        # Optional exclusion check\n",
    "        if not_values is not None:\n",
    "            for v in not_values:\n",
    "                in_range &= self != v\n",
    "\n",
    "        if not torch.all(in_range):\n",
    "            raise ValueError(\n",
    "                f\"Tensor check_range failed: values not in range [{min_val}, {max_val}] or contain excluded {not_values}\"\n",
    "            )\n",
    "\n",
    "    return self\n",
    "\n",
    "\n",
    "torch.Tensor.log = print_tensor\n",
    "torch.Tensor.check_shape = check_shape\n",
    "torch.Tensor.check_range = check_range\n",
    "\n",
    "\n",
    "def samping(model, tokenizer, device, epoch, writer, sample_prompt, expected):\n",
    "    # Include attention_mask in the tokenization\n",
    "    sample_prompt = f\"### Instruction\\n\\n{sample_prompt}\\n\\n### Response\"\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Pass the attention_mask and explicitly set pad_token_id to eos_token_id for reliable generation\n",
    "    full_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    sample_text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    sample_text = sample_text.strip()\n",
    "    if log_content:\n",
    "        print(f\"Sample Output (Epoch {epoch + 1}): {sample_text}\")\n",
    "    if log_content:\n",
    "        print(\"Expected:\", expected)\n",
    "    writer.add_text(\"Sample Output\", f\"Epoch {epoch + 1}: {sample_text}\", epoch)\n",
    "\n",
    "\n",
    "def write_time_file(folder):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"{now}.txt\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(\"This is a dummy file.\\n\")\n",
    "\n",
    "    print(f\"Dummy file written: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f3ff14-2391-43ba-819f-58eaa713d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "\n",
    "if os.path.exists(state.tokenizer_save_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(state.tokenizer_save_dir)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    print_step(\"Loaded tokenizer from saved checkpoint.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(state.model_id)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23fb1d0-1e6e-4f22-a336-236e991415f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reward utility\n",
    "# ------------------------------------------------\n",
    "\n",
    "def reward_atag(front, end, response):\n",
    "    tag_len = len(front + end)\n",
    "    start = response.find(front + end)\n",
    "    end = response.find(front + '/' + end)\n",
    "    reward = 0\n",
    "    if start != -1: reward += 0.1\n",
    "    if end != -1: reward += 0.1\n",
    "    \n",
    "    if start + tag_len < end:\n",
    "        if len(response[start + tag_len:end].strip()) > 1:\n",
    "            reward += 0.1\n",
    "    return reward\n",
    "\n",
    "\n",
    "def remove_comments(code: str):\n",
    "    pattern = re.compile(r'//.*?$|/\\*.*?\\*/', re.DOTALL | re.MULTILINE)\n",
    "    return re.sub(pattern, '', code)\n",
    "\n",
    "\n",
    "def find_all_tag_indexes(text, tag):\n",
    "    \"\"\"Return a list of starting indexes where the tag occurs in the text.\"\"\"\n",
    "    indexes = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        idx = text.find(tag, start)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        indexes.append(idx)\n",
    "        start = idx + len(tag)\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_tag_start_end(idx, starts, ends, tag, full_text):\n",
    "    start = starts[idx]+len(tag)\n",
    "    end = ends[idx]\n",
    "    return full_text[start: end].strip()\n",
    "    \n",
    "\n",
    "def reward_correct(clang_repl, full_text, writer, log_group, step):\n",
    "    # handle only first answer\n",
    "    reward = 0.0\n",
    "    test_target_open = find_all_tag_indexes(full_text, \"<Test Target>\")\n",
    "    test_target_close = find_all_tag_indexes(full_text, \"</Test Target>\")\n",
    "    clang_repl_open = find_all_tag_indexes(full_text, \"<Clang-repl Test>\")\n",
    "    clang_repl_close = find_all_tag_indexes(full_text, \"</Clang-repl Test>\")\n",
    "    if len(test_target_open) == 0 or len(test_target_close) == 0 or len(clang_repl_open) == 0 or len(clang_repl_close) == 0:\n",
    "        return reward, '<Test Target> or <Clang-repl Test> not found'\n",
    "    if len(test_target_open) != len(test_target_close) or len(clang_repl_open) != len(clang_repl_close):\n",
    "        return reward , '<Test Target> or <Clang-repl Test> pair not match'\n",
    "    if not all(x < y for x, y in zip(test_target_open, test_target_close)):\n",
    "        return reward, '<Test Target> not closed properly'\n",
    "    if not all(x < y for x, y in zip(clang_repl_open, clang_repl_close)):\n",
    "        return reward, '<Clang-repl Test> not closed properly' \n",
    "    target_text = get_tag_start_end(-1, test_target_open, test_target_close, \"<Test Target>\", full_text)\n",
    "    target_text = remove_comments(target_text)\n",
    "    target_text = \">>> \"+target_text.replace('\\n', '')\n",
    "\n",
    "    for idx in range(len(clang_repl_open)):\n",
    "        clang_repl_test = get_tag_start_end(idx, clang_repl_open, clang_repl_close, \"<Clang-repl Test>\", full_text)\n",
    "        test_case_with_target = target_text+'\\n'+clang_repl_test\n",
    "        #print(test_case_with_target)\n",
    "        result, response = clang_repl.run_verify(test_case_with_target)\n",
    "        reward = 0.0\n",
    "        if result == 'ok':\n",
    "            reward = 2.0\n",
    "        elif result == 'fail':\n",
    "            reward = 1.0\n",
    "        elif result == 'error':\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            assert False\n",
    "        if log_reward and reward < 1.9:\n",
    "            writer.add_text(f\"{log_group}/reward_correct_context\", f\"Verify: {test_case_with_target}\\nResponse: {response}\", step)\n",
    "        return reward, response\n",
    "    else:\n",
    "        return reward, ''\n",
    "\n",
    "def reward(clang_repl, response_full, writer, log_group, step):\n",
    "    # reference:\n",
    "    # https://blog.gopenai.com/coding-grpo-from-scratch-a-guide-to-distributed-implementation-with-qwen2-5-1-5b-instruct-59b34227edacabs\n",
    "    need_more_test_idx = response_full.find('<Need More Test')\n",
    "    response = response_full if need_more_test_idx == -1 else response_full[:need_more_test_idx]\n",
    "    score = 0.0\n",
    "    score += reward_atag(\"<\", \"Test Object>\", response) # 0.3\n",
    "    score += reward_atag(\"<\", \"Input Data>\", response) # 0.3\n",
    "    score += reward_atag(\"<\", \"Expected Output>\", response) # 0.3\n",
    "    score += reward_atag(\"<\", \"Clang-repl Test>\", response) # 0.3\n",
    "    score += reward_atag(\"[\", \"REASON]\", response) * 2 # 0.3*2\n",
    "    score += reward_atag(\"[\", \"ANSWER]\", response) * 2 # 0.3*2\n",
    "    score = score / 10 # 0.24 *10 = 2.4\n",
    "    if score >= 0.24:\n",
    "        correct_reward, response = reward_correct(clang_repl, response, writer, log_group, step)\n",
    "    else:\n",
    "        correct_reward = 0.0\n",
    "        response = \"not formatted\"\n",
    "    reward = score + correct_reward\n",
    "    writer.add_scalar(f\"{log_group}/format_correct_sample\", score, step)\n",
    "    writer.add_scalar(f\"{log_group}/reward_correct_sample\", correct_reward, step)\n",
    "    return reward, response\n",
    "\n",
    "class RewardWorkPool(ObjectPool):     \n",
    "    def __init__(self, group_size):\n",
    "        super().__init__(ClangReplInterface, reward, group_size)\n",
    "\n",
    "    def reward(self, codes, writer=None, log_group=None, global_step=None):\n",
    "        global hparam\n",
    "        args = [ [code, writer, log_group, global_step*hparam.group_size + idx] for idx, code in enumerate(codes)]\n",
    "        self.start_tasks(args)\n",
    "        \n",
    "    def take_result(self):\n",
    "        return self.get_results()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0e71f6-c101-4b5f-96bd-ae13b19d7051",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dataset and dataloader\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "        train_examples.append({\"content\": content + \"<|endoftext|>\"})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "def load_sample_dataset(pk_file):\n",
    "    with open(config.dataset_file, \"rb\") as f:\n",
    "        global_samples = pickle.load(f)\n",
    "        sample_dataset = []\n",
    "        for sample in global_samples:\n",
    "            sample_dataset.append({\"content\": sample + \"<|endoftext|>\"})\n",
    "        return sample_dataset\n",
    "\n",
    "def get_test_target_content(full_text):\n",
    "    test_target_open = find_all_tag_indexes(full_text, \"<Test Target>\")\n",
    "    test_target_close = find_all_tag_indexes(full_text, \"</Test Target>\")\n",
    "    target_text = get_tag_start_end(-1, test_target_open, test_target_close, \"<Test Target>\", full_text)\n",
    "    return target_text\n",
    "\n",
    "def load_reasoning_dataset(test_target_object_file, seltected_categories):\n",
    "    with open(test_target_object_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        train = []\n",
    "        val = []\n",
    "        categories = set()\n",
    "        data_dic = {}\n",
    "        for item in data:\n",
    "            categories.add(item['category'])\n",
    "        for cat in categories:\n",
    "            data_dic[cat] = []\n",
    "        for item in data:\n",
    "            data_dic[item['category']].append(item['content'])\n",
    "\n",
    "        for cat in seltected_categories:\n",
    "            for idx, item in enumerate(data_dic[cat]):\n",
    "                content = f\"### Instruction\\n\\nn<Test Target>\\n{get_test_target_content(item)}\\n</Test Target>\\nWrtie a Clang-repl Test\\n### Response\\n\"\n",
    "                if idx >=14:\n",
    "                    val.append({\"content\":content})\n",
    "                else:\n",
    "                    train.append({\"content\":content})\n",
    "\n",
    "        return train, val\n",
    "\n",
    "\n",
    "def get_all_categories(state):\n",
    "    with open(state.test_target_object_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        train = []\n",
    "        val = []\n",
    "        categories = set()\n",
    "        category_list = []\n",
    "        data_dic = {}\n",
    "        for item in data:\n",
    "            # reserve order\n",
    "            if not item['category'] in categories:\n",
    "                category_list.append(item['category'])\n",
    "                categories.add(item['category'])\n",
    "    return category_list\n",
    "\n",
    "        \n",
    "def get_dataloader(categories):\n",
    "    global state, tokenizer\n",
    "    reasoning_dataset, val_reasoning_dataset = load_reasoning_dataset(state.test_target_object_file, categories)\n",
    "    reasoning_dataset, val_reasoning_dataset = shrink_dataset( reasoning_dataset, val_reasoning_dataset)\n",
    "    \n",
    "    # Create a Hugging Face Dataset from the list\n",
    "    train_dataset = Dataset.from_list(reasoning_dataset)\n",
    "    val_train_dataset = Dataset.from_list(val_reasoning_dataset)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        global max_length\n",
    "        return tokenizer(\n",
    "            examples[\"content\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            #padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    print_step(\"eos: \"+ str(tokenizer.eos_token) + str(tokenizer.eos_token_id))\n",
    "    \n",
    "    tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    val_tokenized_dataset = val_train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized_dataset = val_tokenized_dataset.remove_columns([\"content\"])\n",
    "    val_tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    # DataLoader\n",
    "    global hparam\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=hparam.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_tokenized_dataset,\n",
    "        batch_size=hparam.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    return dataloader, val_dataloader\n",
    "\n",
    "def get_train_dataloader(categories):\n",
    "    train, _ = get_dataloader(categories)\n",
    "    return train\n",
    "\n",
    "\n",
    "def get_val_dataloader(categories):\n",
    "    _, val = get_dataloader(categories)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66503da6-6aa2-4c25-9276-84c2ee33040c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "# ------------------------------------------------\n",
    "def write_weight_state(model, writer, step, log_group):\n",
    "    for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "        if param.requires_grad:\n",
    "            weight_mean = param.data.mean().item()\n",
    "            weight_std = param.data.std().item()\n",
    "\n",
    "            writer.add_scalar(f\"{log_group}/{idx}_{name}/mean\", weight_mean, step)\n",
    "            writer.add_scalar(f\"{log_group}/{idx}_{name}/std\", weight_std, step)\n",
    "\n",
    "def change_grad(model, layer_start, layer_end, multiple=0.01):\n",
    "    for idx, val in enumerate(model.named_parameters()):\n",
    "        name, param = val\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        if layer_start <= idx < layer_end:\n",
    "                param.grad.mul_(multiple)\n",
    "            \n",
    "class TrainLayerUpdater:\n",
    "    def __init__(self, model, train_layer):\n",
    "        self.model = model.model\n",
    "        self.train_layer = train_layer\n",
    "\n",
    "    def get_layer_params(self):\n",
    "        if self.train_layer == TrainLayers.FULL_LAYER:\n",
    "            params = list(self.model.parameters())\n",
    "        elif self.train_layer == TrainLayers.TWO_FRONT_LAYER:\n",
    "            return [p for layer in self.model.layers[:2] for p in layer.parameters()]\n",
    "        elif self.train_layer == TrainLayers.ODD_LAYER:\n",
    "            params = [p for idx, layer in enumerate(self.model.layers) if idx % 2 == 1 for p in layer.parameters()]\n",
    "        elif self.train_layer == TrainLayers.EVEN_LAYER:\n",
    "            params = [p for idx, layer in enumerate(self.model.layers) if idx % 2 == 0 for p in layer.parameters()]\n",
    "        elif self.train_layer == TrainLayers.SWITCH_PAIR_LAYER:\n",
    "            idx = self.config.current_layer_index\n",
    "            params = []\n",
    "            if idx < len(self.model.layers):\n",
    "                params.extend(list(self.model.layers[idx].parameters()))\n",
    "            if (idx + 1) < len(self.model.layers):\n",
    "                params.extend(list(self.model.layers[idx + 1].parameters()))\n",
    "            # Cycle the current_layer_index for the next update\n",
    "            self.config.current_layer_index = (idx + 2) % len(self.model.layers) \n",
    "        else:\n",
    "            raise ValueError(\"Invalid train_layer configuration\")\n",
    "\n",
    "        # add first two layer whatever\n",
    "        #params.extend([p for layer in self.model.layers[:2] for p in layer.parameters()]) # params\n",
    "        # remove first two layer\n",
    "        params = [x for x in params if x not in self.model.layers[:2]] # \n",
    "        return params\n",
    "\n",
    "    def update_optimizer_and_requires_grad(self, optimizer):\n",
    "        # Get the new set of parameters for training.\n",
    "        new_params = self.get_layer_params()\n",
    "        new_params_set = set(new_params)\n",
    "        \n",
    "        # Update requires_grad flags for all model parameters.\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = param in new_params_set\n",
    "\n",
    "        # Update the optimizer parameter group (assuming a single group).\n",
    "        optimizer.param_groups[0]['params'] = list(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f77f88a-17d5-402b-818e-8199bffe0759",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tensor utility\n",
    "# ------------------------------------------------\n",
    "\n",
    "def pad_to_match(tensor_a, tensor_b, padding_value=0):\n",
    "    # Determine the current sequence lengths\n",
    "    seq_len_a = tensor_a.size(1)\n",
    "    seq_len_b = tensor_b.size(1)\n",
    "\n",
    "    if seq_len_a > seq_len_b:\n",
    "        max_seq_len = max(seq_len_a, seq_len_b)\n",
    "    \n",
    "        # Define padding function\n",
    "        def pad_tensor(tensor, target_length):\n",
    "            pad_length = target_length - tensor.size(1)\n",
    "            if pad_length > 0:\n",
    "                padding = (0, 0) * (tensor.dim() - 2) + (0, pad_length)\n",
    "                tensor = F.pad(tensor, padding, value=padding_value)\n",
    "            return tensor\n",
    "    \n",
    "        # Pad both tensors to the maximum sequence length\n",
    "        tensor_a_padded = pad_tensor(tensor_a, max_seq_len)\n",
    "        tensor_b_padded = pad_tensor(tensor_b, max_seq_len)\n",
    "    else:\n",
    "        tensor_b_padded = tensor_b[:, :seq_len_a]\n",
    "        tensor_a_padded = tensor_a\n",
    "\n",
    "    return tensor_a_padded, tensor_b_padded\n",
    "\n",
    "def selective_log_softmax(logits, input_ids, tokenizer):\n",
    "    # Ensure input_ids are on the same device as logits\n",
    "    if input_ids.device != logits.device:\n",
    "        input_ids = input_ids.to(logits.device)\n",
    "\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    if input_ids.size(1) > log_probs.size(1):\n",
    "        input_ids = input_ids[:, :log_probs.size(1)]\n",
    "\n",
    "    # Gather log probabilities corresponding to input_ids\n",
    "    selected_log_probs = log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    if log_content:\n",
    "        input_text = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        print(\"Input Texts:\")\n",
    "        for text in input_text:\n",
    "            print(text)\n",
    "        logits_ids = logits.argmax(dim=-1)\n",
    "        logit_text = tokenizer.batch_decode(logits_ids, skip_special_tokens=True)\n",
    "        print(\"\\nLogit Texts:\")\n",
    "        for text in logit_text:\n",
    "            print(text)\n",
    "\n",
    "    return selected_log_probs\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    layer_index = 1\n",
    "    original_first_block = self.model.layers[layer_index]\n",
    "\n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "\n",
    "    self.model.layers.insert(layer_index, new_block)\n",
    "\n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def cut_tensors_by_min(a: torch.Tensor, b: torch.Tensor, dim: int):\n",
    "    assert a.dim() > dim and b.dim() > dim, \"Specified dim exceeds tensor rank\"\n",
    "\n",
    "    min_length = min(a.size(dim), b.size(dim))\n",
    "    a_cut = torch.narrow(a, dim, 0, min_length)\n",
    "    b_cut = torch.narrow(b, dim, 0, min_length)\n",
    "    return a_cut, b_cut\n",
    "\n",
    "def cut_ids_on_eos_tensor(full_ids, eos_token_id):\n",
    "    processed_ids = []\n",
    "    for seq in full_ids:\n",
    "        eos_positions = (seq == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        if eos_positions.numel() > 0:\n",
    "            first_eos_index = eos_positions[0].item()\n",
    "            processed_ids.append(seq[:first_eos_index])\n",
    "        else:\n",
    "            processed_ids.append(seq)\n",
    "    return processed_ids\n",
    "    \n",
    "def cut_ids_on_eos(generated_ids, eos_token_id):\n",
    "    processed_ids = []\n",
    "    for seq in generated_ids:\n",
    "        if eos_token_id in seq:\n",
    "            # Truncate the sequence at the first occurrence of the EOS token\n",
    "            first_eos_index = seq.index(eos_token_id)\n",
    "            processed_ids.append(seq[:first_eos_index])\n",
    "        else:\n",
    "            processed_ids.append(seq)\n",
    "    return processed_ids\n",
    "\n",
    "\n",
    "def shift_ids_with_logits(ids, shift_logits):\n",
    "    shift_ids = torch.cat([ids[:, 1:], torch.argmax(shift_logits[:, -1, :], dim=-1).unsqueeze(1)], dim=1)\n",
    "    return shift_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train scheduling\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "def copy_inference_model(model):\n",
    "    old_model = copy.deepcopy(model).half()\n",
    "    old_model.eval()\n",
    "    for param in old_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return old_model\n",
    "\n",
    "\n",
    "def try_to_update_category_count_and_ref_model(\n",
    "    model,\n",
    "    ref_model,\n",
    "    hparam,\n",
    "    state,\n",
    "    mean_reward,\n",
    "    cur_category_count,\n",
    "    category_size\n",
    "):\n",
    "    if mean_reward > hparam.expected_meean_reward:\n",
    "        print(\"Reference model updated\")\n",
    "        ref_model = copy_inference_model(model)\n",
    "        if cur_category_count < category_size:\n",
    "            if not state.is_finding_opt:\n",
    "                cur_category_count += 1\n",
    "    elif ref_model is None:\n",
    "        ref_model = copy_inference_model(model)\n",
    "\n",
    "    return cur_category_count, ref_model\n",
    "\n",
    "\n",
    "_switch_pair_layer_counter = 0\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "        model,\n",
    "        hparam,\n",
    "        state,\n",
    "        optimizer,\n",
    "        tokenizer,\n",
    "        scaler,\n",
    "        scheduler,\n",
    "        writer):\n",
    "    reward_work = RewardWorkPool(hparam.group_size * hparam.batch_size)\n",
    "    ref_model = None\n",
    "\n",
    "    # --- Generate sample output text after each epoch ---\n",
    "    model.eval()  # Set to eval mode for generation.\n",
    "    with torch.no_grad():\n",
    "        samping(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            state.device,\n",
    "            0,\n",
    "            writer,\n",
    "            \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\",\n",
    "            \"```\\n>>> (prompt)\\n```\",\n",
    "        )\n",
    "        samping(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            state.device,\n",
    "            0,\n",
    "            writer,\n",
    "            \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\",\n",
    "            \"Custom Clang-repl takes only one line input.\",\n",
    "        )\n",
    "    model.train()  # Switch back to training mode.\n",
    "\n",
    "    mean_reward = 0\n",
    "    categories = get_all_categories(state)\n",
    "    category_size = len(categories)\n",
    "    state.cur_category_count = hparam.category_count_start\n",
    "    last_category_count = state.cur_category_count\n",
    "    switch_pair_layer = 0\n",
    "\n",
    "    print(\"Total categories:\", category_size, categories)\n",
    "\n",
    "    dataloader = get_train_dataloader(categories[:state.cur_category_count])\n",
    "    print(\"Data counts:\", len(dataloader))\n",
    "    val_dataloader = get_val_dataloader(categories)\n",
    "\n",
    "    for epoch in range(state.start_epoch, hparam.num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print_step(f\"Epoch {epoch+1}/{hparam.num_epochs} - Validation\", main_step=True)\n",
    "\n",
    "        if hparam.train_layer == TrainLayers.SWITCH_PAIR_LAYER:\n",
    "            global _switch_pair_layer_counter\n",
    "            _switch_pair_layer_counter += 1\n",
    "            if _switch_pair_layer_counter % 4 == 0:\n",
    "                state.cur_category_count, ref_model = try_to_update_category_count_and_ref_model(\n",
    "                        model,\n",
    "                        ref_model,\n",
    "                        hparam,\n",
    "                        state,\n",
    "                        mean_reward,\n",
    "                        state.cur_category_count,\n",
    "                        category_size\n",
    "                    )\n",
    "        else:\n",
    "            state.cur_category_count, ref_model = try_to_update_category_count_and_ref_model(\n",
    "                model,\n",
    "                ref_model,\n",
    "                hparam,\n",
    "                state,\n",
    "                mean_reward,\n",
    "                state.cur_category_count,\n",
    "                category_size\n",
    "            )\n",
    "\n",
    "        print_memory(20)\n",
    "        old_model = None\n",
    "\n",
    "        if not state.skip_validation_step and (epoch % hparam.validation_interval) == 0:\n",
    "            with torch.no_grad():\n",
    "                print(\"Validation Start ....\")\n",
    "                # Run validation (no parameter updates).\n",
    "                val_mean_loss, val_mean_reward, state.val_global_step = run(model, old_model, ref_model,\n",
    "                    val_dataloader, optimizer, tokenizer,\n",
    "                    hparam, state,\n",
    "                    scaler, writer, reward_work, \n",
    "                    log_group=\"Validation\",\n",
    "                    scheduler=None,\n",
    "                    global_step=state.val_global_step, num_grpo=1, group_size=1, is_validation=True)\n",
    "                writer.add_scalar(\"Val_Epoch/mean_reward\", val_mean_reward, epoch // hparam.validation_interval)\n",
    "                writer.add_scalar(\"Val_Epoch/mean_loss\", val_mean_loss, epoch // hparam.validation_interval)\n",
    "                print(\"Validation End ....\")\n",
    "\n",
    "        print(\"Training Start ....\")\n",
    "        # Loop over gradient groups for training.\n",
    "        print_step(\n",
    "            f\"Epoch {epoch+1}/{hparam.num_epochs} - category include ={categories[state.cur_category_count-1]}\",\n",
    "            main_step=True,\n",
    "        )\n",
    "\n",
    "        if last_category_count != state.cur_category_count:\n",
    "            last_category_count = state.cur_category_count\n",
    "            dataloader = get_train_dataloader(categories[:state.cur_category_count])\n",
    "            print(\"Data counts:\", len(dataloader))\n",
    "\n",
    "        mean_loss, mean_reward, state.global_step = run(model, old_model, ref_model,\n",
    "            dataloader, optimizer, tokenizer,\n",
    "            hparam, state,\n",
    "            scaler, writer, reward_work, \n",
    "            log_group=\"Training\",\n",
    "            scheduler=scheduler,\n",
    "            global_step=state.global_step, num_grpo=hparam.num_grpo, group_size=hparam.group_size, is_validation=False\n",
    "        )\n",
    "\n",
    "\n",
    "        old_model = None\n",
    "        print_step(\"7. End Epoch\")\n",
    "        writer.add_scalar(\"Epoch/mean_reward\", mean_reward, epoch + 1)\n",
    "        writer.add_scalar(\"Epoch/mean_loss\", mean_loss, epoch + 1)\n",
    "        print(\"Training End ....\")\n",
    "\n",
    "        # Save latest checkpoint.\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": state.global_step,\n",
    "            \"val_global_step\": state.val_global_step,\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(state.last_checkpoint_path), exist_ok=True)\n",
    "        torch.save(checkpoint, state.last_checkpoint_path)\n",
    "\n",
    "        # Optionally save checkpoint on specific epochs.\n",
    "        if epoch % state.save_epochs == 0:\n",
    "            checkpoint_dir = state.checkpoint_dir_pre + str(epoch + 1)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print_step(f\"Checkpoint saved at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "            # Save tokenizer once if not already saved.\n",
    "            tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "            if not os.path.exists(tokenizer_save_dir):\n",
    "                os.makedirs(tokenizer_save_dir, exist_ok=True)\n",
    "                tokenizer.save_pretrained(tokenizer_save_dir)\n",
    "                print_step(\"Tokenizer saved.\")\n",
    "\n",
    "    state.prof_stop()\n",
    "    writer.close()\n",
    "    return mean_reward\n",
    "\n",
    "def load(checkpoint_path, hparam, state, start_epoch=None):\n",
    "    print_memory(1)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "    _model = AutoModelForCausalLM.from_pretrained(state.model_id)\n",
    "    config = copy.deepcopy(_model.config)\n",
    "    _model = None\n",
    "    print_memory(2)\n",
    "    config.num_hidden_layers += 2\n",
    "    config.max_position_embeddings = 512\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    print_memory(3)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(state.device)\n",
    "    print_memory(4)\n",
    "    train_layer_updater = TrainLayerUpdater(model, hparam.train_layer)\n",
    "    optimizer = hparam.get_optimizer(train_layer_updater.get_layer_params(), lr=hparam.lr)\n",
    "    train_layer_updater.update_optimizer_and_requires_grad(optimizer)\n",
    "\n",
    "    print_step( f\"Loaded checkpoint from reference {checkpoint_path} at epoch {start_epoch if start_epoch is not None else checkpoint.get(\"epoch\", 0)}\")\n",
    "    print_memory(7)\n",
    "\n",
    "    return model, optimizer, checkpoint\n",
    "\n",
    "def train(hparam, state, tokenizer):\n",
    "    log_dir = f\"runs/{state.log_prefix}_{state.learning_name}_{hparam.essence_str()}\"\n",
    "    write_time_file(log_dir)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    state.prof_start()\n",
    "\n",
    "    # Check if a latest checkpoint exists to load model and optimizer states\n",
    "    if (\n",
    "        os.path.exists(state.last_checkpoint_path)\n",
    "        and not state.is_finding_opt\n",
    "        and not state.use_reference_model\n",
    "    ):\n",
    "        print(\"==USING CHECK POINT MODEL==\")\n",
    "        model, optimizer, checkpoint = load(state.last_checkpoint_path, hparam, state)\n",
    "        state.start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "        state.global_step = checkpoint.get(\"global_step\", 0)\n",
    "        state.val_global_step = checkpoint.get(\"val_global_step\", 0)\n",
    "    else:\n",
    "        if os.path.exists(state.ref_checkpoint_path):\n",
    "            print(\"==USING REFERENCE MODEL==\")\n",
    "            model, optimizer, checkpoint = load(state.ref_checkpoint_path, hparam, state, start_epoch=0)\n",
    "            state.start_epoch = 0  \n",
    "            state.global_step = 0\n",
    "            state.val_global_step = 0\n",
    "        else:\n",
    "            assert False, \"prompt_last_checkpoint_path must exist\"\n",
    "\n",
    "    dups = check_optimizer_duplicates(optimizer)\n",
    "    if dups:\n",
    "        print(\"Warning: The optimizer contains duplicate parameters!\")\n",
    "        print(f\"Duplicate parameter count: {len(dups)}\")\n",
    "    else:\n",
    "        print(\"No duplicate parameters found in the optimizer.\")\n",
    "\n",
    "    # Clear cached memory that is no longer used\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print_memory(9)\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def lr_schedule(step):\n",
    "        # Linear warm-up to 1.0, then constant\n",
    "        return min(1.0, step / hparam.warming_up_step)\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_schedule)\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model,\n",
    "        hparam,\n",
    "        state,\n",
    "        optimizer,\n",
    "        tokenizer,\n",
    "        scaler=scaler,\n",
    "        scheduler=scheduler,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global state, tokenizer\n",
    "    hparam = object_hiper_param(trial)\n",
    "    state.skip_validation_step=True\n",
    "    state.is_finding_opt=True\n",
    "\n",
    "    print(\n",
    "        f\"[Optuna] Trial hyperparameters -> {hparam}\"\n",
    "    )\n",
    "    return train(hparam, state, tokenizer)\n",
    "\n",
    "def main(hparam, state):\n",
    "    if state.is_finding_opt:\n",
    "        # Create study to minimize final loss\n",
    "        global objective\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "\n",
    "        print(\"Study completed!\")\n",
    "        print(\"Best trial:\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Value: {best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"#    {key}: {value}\")\n",
    "        with open(\"runs/hiper_param.json\", \"w\") as f:\n",
    "            json.dump(dict(best_trial.params.items()), f, indent=4)\n",
    "    else:\n",
    "        train(hparam, state, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcb6833-2b7e-41bd-9166-a97df258dadc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reasoning logic....\n",
      "Dummy file written: runs/Adafactor_starcoder2-3b_reasoning_Adafactor_FULL_LAYER__3.131e-05_8_0.26207_1_0.53715/2025-04-06_10-13-17.txt\n",
      "==USING REFERENCE MODEL==\n",
      "No duplicate parameters found in the optimizer.\n",
      "Total categories: 9 ['simple arithmetic', 'simple if', 'simple loop', 'loop and if', 'simple state', 'recursive function', 'pointer manipulation', 'string manipulation', 'sort algorithm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9c57c0f4234d48a6ab89c8b8e9097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943e74ab590145a1ad4f424ff3bbc681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data counts: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6234ea70f174506ad665d2aa73f3043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e794c2a3ec44189be97118ef1ef8eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Validation\n",
      "Validation Start ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_494449/167354134.py:257: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  std_rewards = advantages.std(dim=1).repeat_interleave(group_size).check_shape([batch_size]).check_range(0, float('inf'))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:58<00:00, 13.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation End ....\n",
      "Training Start ....\n",
      "Epoch 1/200 - category include =simple arithmetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▏                                                                               | 1/14 [00:34<07:26, 34.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 359\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart reasoning logic....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 359\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 305\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hparam, state)\u001b[0m\n\u001b[1;32m    303\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mdict\u001b[39m(best_trial\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mitems()), f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 262\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(hparam, state, tokenizer)\u001b[0m\n\u001b[1;32m    259\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mlr_schedule)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Train & get final metric\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m final_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Return the final average loss to Optuna\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_avg_loss\n",
      "Cell \u001b[0;32mIn[13], line 143\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, hparam, state, optimizer, tokenizer, scaler, scheduler, writer)\u001b[0m\n\u001b[1;32m    140\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m get_train_dataloader(categories[:state\u001b[38;5;241m.\u001b[39mcur_category_count])\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataloader))\n\u001b[0;32m--> 143\u001b[0m mean_loss, mean_reward, state\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_work\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_grpo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_grpo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m old_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m print_step(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7. End Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 146\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, old_model, ref_model, dataloader, optimizer, tokenizer, hparam, state, scaler, writer, reward_work, log_group, scheduler, global_step, num_grpo, group_size, is_validation)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    145\u001b[0m     model_for_generating_result \u001b[38;5;241m=\u001b[39m hparam\u001b[38;5;241m.\u001b[39mchoose_generator(model, old_model, ref_model)\n\u001b[0;32m--> 146\u001b[0m     full_ids, truncated_ids, respone_ids, prompt_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_for_generating_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     full_ids\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;66;03m# FULL_IDS\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     truncated_ids\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;66;03m# TRUNCATED_IDS\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mgenerate_ids\u001b[0;34m(model, batch, tokenizer, temperature)\u001b[0m\n\u001b[1;32m     22\u001b[0m     prompt_lengths\u001b[38;5;241m.\u001b[39mappend(first_eos)\n\u001b[1;32m     24\u001b[0m print_memory(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt lengths per batch element: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(prompt_lengths))\n\u001b[0;32m---> 26\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m full_ids \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     38\u001b[0m truncated_ids \u001b[38;5;241m=\u001b[39m cut_ids_on_eos_tensor(full_ids, tokenizer\u001b[38;5;241m.\u001b[39meos_token_id)    \n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/starcoder2/modeling_starcoder2.py:839\u001b[0m, in \u001b[0;36mStarcoder2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/starcoder2/modeling_starcoder2.py:562\u001b[0m, in \u001b[0;36mStarcoder2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    560\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 562\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/starcoder2/modeling_starcoder2.py:267\u001b[0m, in \u001b[0;36mStarcoder2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    266\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 267\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    269\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2902\u001b[0m         layer_norm,\n\u001b[1;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2909\u001b[0m     )\n\u001b[0;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Core logic\n",
    "# ------------------------------------------------\n",
    "\n",
    "def generate_ids(model, batch, tokenizer, temperature):\n",
    "    global max_length\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Determine prompt length for each example in the batch based on the first occurrence of EOS.\n",
    "    prompt_lengths = []\n",
    "    for i in range(input_ids.size(0)):\n",
    "        seq = input_ids[i]\n",
    "        # Find indices where the token equals the eos_token_id.\n",
    "        eos_positions = (seq == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        # If there's at least one occurrence, use its index + 1 (if you want to include the EOS in the prompt).\n",
    "        # Otherwise, fallback to the full sequence length.\n",
    "        if eos_positions.numel() > 0:\n",
    "            first_eos = eos_positions[0].item() + 1\n",
    "        else:\n",
    "            first_eos = seq.size(0)\n",
    "        prompt_lengths.append(first_eos)\n",
    "    \n",
    "    print_memory(\"Prompt lengths per batch element: \" + str(prompt_lengths))\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,  \n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    full_ids = output.sequences.detach()\n",
    "    truncated_ids = cut_ids_on_eos_tensor(full_ids, tokenizer.eos_token_id)    \n",
    "    respone_ids =  pad_sequence([truncated_ids[idx][p_len:] for idx, p_len in enumerate(prompt_lengths)],\n",
    "                                               batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    truncated_ids = pad_sequence(truncated_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    full_ids = pad_sequence(full_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    output = None\n",
    "    print_memory(\"full_ids.shape[-1]: \" + str(full_ids.shape[-1]))\n",
    "    return full_ids, truncated_ids, respone_ids, prompt_lengths\n",
    "\n",
    "\n",
    "def compute_logits(model, full_ids, prompt_lengths, respone_ids, tokenizer, detach_out=False):\n",
    "    # Pad the list of full_ids to a whole tensor with shape (batch, max_seq_length)\n",
    "    full_ids = pad_sequence(full_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(dtype=torch.int32)\n",
    "    \n",
    "    # Create an attention mask where non-pad tokens are 1 and pad tokens are 0\n",
    "    full_ids_mask = (full_ids != tokenizer.pad_token_id).to(dtype=torch.int32, device=full_ids.device)\n",
    "    \n",
    "    # Compute logits for the whole padded tensor.\n",
    "    logits = model(input_ids=full_ids, attention_mask=full_ids_mask, early_stop=False).logits\n",
    "    \n",
    "    truncated_response_ids_list = []\n",
    "    truncated_response_logits_list = []\n",
    "    batch_size = full_ids.size(0)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        p_len = prompt_lengths[i]\n",
    "        # Determine the true sequence length (ignoring padding) for this batch element.\n",
    "        actual_length = full_ids_mask[i].sum().item()\n",
    "        # Ensure prompt length does not exceed actual length.\n",
    "        if p_len > actual_length:\n",
    "            p_len = actual_length\n",
    "\n",
    "        # Extract completion token IDs for this example.\n",
    "        comp_ids = full_ids[i, p_len:actual_length].detach()\n",
    "        # For logits, if you want to include the token just before the completion, slice from p_len-1.\n",
    "        comp_logits = logits[i, p_len-1:actual_length-1, :]\n",
    "        \n",
    "        # Optionally, adjust lengths to be consistent (if needed by downstream code)\n",
    "        #comp_ids, comp_logits = cut_tensors_by_min(comp_ids, comp_logits, 0)\n",
    "        expected_len = respone_ids.shape[1]\n",
    "        truncated_response_ids_list.append((comp_ids.detach() if detach_out else comp_ids)[:expected_len])\n",
    "        truncated_response_logits_list.append((comp_logits.detach() if detach_out else comp_logits)[:expected_len, :])\n",
    "\n",
    "    truncated_response_logits = pad_sequence(truncated_response_logits_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    truncated_response_ids = pad_sequence(truncated_response_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return logits, truncated_response_logits, truncated_response_ids\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def run(model, old_model, ref_model,\n",
    "            dataloader, optimizer, tokenizer,\n",
    "            hparam, state,\n",
    "            scaler, writer, reward_work, log_group, scheduler, \n",
    "            global_step, num_grpo, group_size, is_validation):\n",
    "    running_loss = 0.0\n",
    "    mean_reward = 0.0\n",
    "    sum_reward = 0.0\n",
    "    mean_reward_list = []\n",
    "    mean_loss_list = []\n",
    "    run_start_global_step=global_step\n",
    "    print_memory(\"_.1. run() enter\")\n",
    "    # For accumulation mode, ensure gradients are zeroed at the start.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, total=len(dataloader)), start=1):\n",
    "        print_step(f\"Processing batch {step}/{len(dataloader)}: Start Loop\")\n",
    "        state.prof_step()\n",
    "        old_model = hparam.try_old_model_update_in_batch_loop(model, old_model, step, len(dataloader), state.cur_category_count)\n",
    "\n",
    "  \n",
    "        # Move batch to device and expand the tensors.\n",
    "        batch = {k: v.to(device=state.device, dtype=torch.int32) for k, v in batch.items()}\n",
    "        batch_size = len(batch)\n",
    "        input_ids = batch['input_ids'].repeat_interleave(group_size, dim=0)\n",
    "        attention_mask = batch['attention_mask'].repeat_interleave(group_size, dim=0)\n",
    "        batch_size = input_ids.size(0)\n",
    "        batch['input_ids'] = input_ids\n",
    "        batch['attention_mask'] = attention_mask\n",
    "\n",
    "        start_tensor_ids = cur_memory_ids()\n",
    "\n",
    "        # LOGIT sample (min= -34.09375 , avg= 0.364013671875 , max= 42.25 )\n",
    "        # LOG_LOGIT sample: (min= -18.188087463378906 , avg= -0.3128775656223297 , max= 0.0 )\n",
    "        # LOG_PROBE sample: (min= -22.589847564697266 , avg= -0.4811277985572815 , max= 0.0 )\n",
    "        # kl_div sample: (min= 0.07530781626701355 , avg= 0.0904245376586914 , max= 0.10227474570274353 )\n",
    "\n",
    "        # std_rewards: 0.5049999952316284 \n",
    "        # advantages: (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
    "        # A_hat: (min= -1.4997029304504395 , avg= 1.4901161193847656e-08 , max= 0.4999009966850281 )\n",
    "        # unclipped_objective: (min= -483673.78125 , avg= -212.85939025878906 , max= 221674.734375 )\n",
    "        # clipped_objective: (min= 0.7576461434364319 , avg= 0.7710149884223938 , max= 1.2423537969589233 )\n",
    "        # ppo_loss: -0.5694103240966797 , kl_div: 0.0904245376586914\n",
    "\n",
    "        # std_rewards: 0.0 Rewards: [1.24, 1.24, 1.24, 1.24]\n",
    "        # advantages: (min= 1.2400000095367432 , avg= 1.2400000095367432 , max= 1.2400000095367432 )\n",
    "        # A_hat: (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
    "        # unclipped_objective: (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
    "        # clipped_objective: (min= 0.7576461434364319 , avg= 0.7576462030410767 , max= 0.7576461434364319 )\n",
    "        # ppo_loss: 0.0 , kl_div: 0.0454302616417408\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            # 1. Model forward pass for generation.\n",
    "            print_step(\"1. Model train\")\n",
    "            with torch.no_grad():\n",
    "                model_for_generating_result = hparam.choose_generator(model, old_model, ref_model)\n",
    "                full_ids, truncated_ids, respone_ids, prompt_lengths = generate_ids(model_for_generating_result, batch, tokenizer, hparam.temperature)\n",
    "                full_ids.log() # FULL_IDS\n",
    "                truncated_ids.log() # TRUNCATED_IDS\n",
    "                respone_ids.log() # RESPONSE_IDS\n",
    "                full_text_lists = tokenizer.batch_decode(truncated_ids, skip_special_tokens=True)\n",
    "                reward_work.reward(full_text_lists, writer, log_group, global_step)\n",
    "                # Release unused tensors from generation.\n",
    "                full_text_lists = None\n",
    "            full_shift_logits, response_truncated_logits, _ = compute_logits(model, full_ids, prompt_lengths, respone_ids, tokenizer) \n",
    "            full_shift_logits.log() # FULL_LOGITS\n",
    "            response_truncated_logits.log() # RESPONSE_LOGITS\n",
    "\n",
    "            full_shift_ids = shift_ids_with_logits(full_ids, full_shift_logits)\n",
    "            print_logits_ids(\"model full\", full_shift_logits, full_shift_ids)  # good format confirmed, full_shift_logits: FULL_LOGITS, full_shift_ids: FULL_IDS\n",
    "            \n",
    "            FULL_IDS = full_ids.shape  # [batch, full_ids_len], sample: batch=4, full_ids=512\n",
    "            TRUNCATED_IDS = truncated_ids.shape # [batch, truncated_ids_len],  sample: truncated_ids_len=512 or less. FULL_IDS with cut out end parts after eos\n",
    "            RESPONSE_IDS = respone_ids.shape  # [batch, respone_ids_len], sample: respone_ids_len = 466 = truncated_ids_len-prompt_length\n",
    "            FULL_LOGITS = full_shift_logits.shape # [batch, full_ids_len, embedding_len], sample: embedding_len=49152\n",
    "            RESPONSE_LOGITS = response_truncated_logits.shape #  [batch, respone_ids_len, embedding_len]\n",
    "            # GROUPED_BATCH  = advantages.shape # [grouped_batch, group_size], example grouped_batch=1, group_size=4 see advantages creation\n",
    "            \n",
    "\n",
    "            # 2. Run legacy models (old and reference models).\n",
    "            print_step(\"2. Legacy Models Run\")\n",
    "            with torch.no_grad():\n",
    "                _, old_response_truncated_logits, _ = compute_logits(old_model, truncated_ids, prompt_lengths, respone_ids, tokenizer, detach_out=True)\n",
    "                ref_full_shift_logits, _, _ = compute_logits(ref_model, full_ids, prompt_lengths, respone_ids, tokenizer, detach_out=True)\n",
    "                print_logits_ids(\"ref model full\", ref_full_shift_logits, full_shift_ids) # good format confirmed, ref_full_shift_logits: FULL_LOGITS, full_shift_ids: FULL_IDS\n",
    "            truncated_ids = None\n",
    "            prompt_lengths = None\n",
    "            full_ids = None\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                print_logits_ids(\"model response\", response_truncated_logits, respone_ids) # good format confirmed, response_truncated_logits: RESPONSE_LOGITS, respone_ids: RESPONSE_IDS\n",
    "                #model_log_logits = F.log_softmax(response_truncated_logits, dim=2)\n",
    "                model_log_logits = selective_log_softmax(response_truncated_logits, respone_ids, tokenizer).check_shape(RESPONSE_IDS)\n",
    "                model_log_logits.log()\n",
    "                print_logits_ids(\"old model response\", old_response_truncated_logits, respone_ids) # good format confirmed, old_response_truncated_logits: RESPONSE_LOGITS, respone_ids: RESPONSE_IDS\n",
    "                #old_model_log_logits = F.log_softmax(old_response_truncated_logits, dim=2)\n",
    "                old_model_log_logits = selective_log_softmax(old_response_truncated_logits, respone_ids, tokenizer).check_shape(RESPONSE_IDS)\n",
    "                old_model_log_logits.log() \n",
    "                probability_ratio = torch.exp(model_log_logits - old_model_log_logits).check_shape(RESPONSE_IDS) #.mean(dim=2)\n",
    "                probability_ratio.log() \n",
    "                \n",
    "                # Remove legacy model intermediates (no longer needed)\n",
    "                full_truncated_full_logits = None\n",
    "                response_truncated_logits = None\n",
    "                old_response_truncated_logits = None\n",
    "                ref_completion_ids = None\n",
    "                model_log_logits = None\n",
    "                old_model_log_logits = None\n",
    "\n",
    "\n",
    "            # 3. kl_div Loss Calc\n",
    "            print_step(\"3. kl_div Loss Calc\")    \n",
    "            # Calculate token-level log probabilities.\n",
    "            model_log_probs = selective_log_softmax(full_shift_logits, full_shift_ids, tokenizer)\n",
    "            model_log_probs.log() # RESPONSE_IDS\n",
    "            ref_log_probs = selective_log_softmax(ref_full_shift_logits, full_shift_ids, tokenizer)\n",
    "            ref_log_probs.log() # RESPONSE_IDS\n",
    "            \n",
    "            # Compute token-level KL divergence.\n",
    "            token_kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='none', log_target=True).check_shape(FULL_IDS) # it is not an ids but parts of logits content. the shape is just like ids)\n",
    "            token_kl_div.log()\n",
    "            kl_div = token_kl_div.mean(dim=-1).check_shape([batch_size])\n",
    "            kl_div.log() # average over tokens. range (0, infite) but for output of similar model. It is very small. sample: kl_div=0.09\n",
    "            \n",
    "            # Save scalar values for logging before clearing.\n",
    "            kl_div_val=kl_div.mean().item()\n",
    "            \n",
    "            # Remove now-unused intermediate tensors.\n",
    "            ref_log_probs = None\n",
    "            ref_full_shift_logits = None\n",
    "            full_shift_logits = None\n",
    "            full_shift_ids = None\n",
    "            model_log_probs = None\n",
    "            token_kl = None \n",
    "            \n",
    "            # 4. Calculate rewards.\n",
    "            print_step(\"4. Reward calc\")\n",
    "            \n",
    "            reward_work_result = reward_work.take_result()\n",
    "            if not reward_work_result:  # reward list is empty\n",
    "                response_texts = tokenizer.batch_decode(respone_ids, skip_special_tokens=True)\n",
    "                writer.add_text(f\"{log_group}/reward_empty_response\", str(response_texts), global_step=step)\n",
    "                rewards = [0.0]*group_size\n",
    "                if False:\n",
    "                    # Remove now-unused intermediate tensors.\n",
    "                    kl_div = None\n",
    "                    respone_ids = None\n",
    "                    probability_ratio = None\n",
    "                    continue  # skip to next batch\n",
    "            else:\n",
    "                rewards, responses = reward_work_result\n",
    "            sum_reward += sum(rewards)\n",
    "            # rewards list[batch]\n",
    "            \n",
    "            if all(reward > 2.0 for reward in rewards):\n",
    "                # perfect no loss in grouped_ppo\n",
    "                grouped_ppo_loss = -(torch.ones(len(rewards), dtype=torch.float32, device=state.device) + hparam.epsilon).check_shape([batch_size])\n",
    "                grouped_ppo_loss.log()\n",
    "            else:\n",
    "                # Convert rewards to tensor\n",
    "                grouped_batch_size = len(rewards) // group_size\n",
    "                advantages = torch.tensor(rewards, dtype=torch.float32, device=state.device).view(grouped_batch_size, group_size).check_range(0, 2.24)\n",
    "                advantages.log() \n",
    "                \n",
    "                # Calculate mean and std per batch (along dim=1) and repeat to match original size\n",
    "                mean_rewards = advantages.mean(dim=1).repeat_interleave(group_size).check_shape([batch_size]).check_range(0, 2.24)\n",
    "                mean_rewards.log()\n",
    "                std_rewards = advantages.std(dim=1).repeat_interleave(group_size).check_shape([batch_size]).check_range(0, float('inf'))\n",
    "                std_rewards.log() \n",
    "    \n",
    "                # Reshape back to original form\n",
    "                advantages = advantages.view(-1)\n",
    "                advantages.check_shape([batch_size]).log(\"advantages before A_hat\")\n",
    "                A_hat = ((advantages - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1).check_shape([batch_size, 1]) \n",
    "                A_hat = torch.clamp(A_hat, -5, 5)\n",
    "                A_hat.log() \n",
    "                \n",
    "                # Clear rewards intermediates.\n",
    "                advantages = None\n",
    "                # 5. grouped_ppo Loss Calc\n",
    "                print_step(\"5. Grouped ppo Loss Calc\")            \n",
    "                # PPO objective calculations.\n",
    "                unclipped_objective = probability_ratio\n",
    "                unclipped_objective.check_shape(RESPONSE_IDS).log()\n",
    "                epsilon_high = torch.full_like(unclipped_objective, 1 + hparam.epsilon).check_shape(RESPONSE_IDS)\n",
    "                _grouped_ppo_loss = - torch.minimum(unclipped_objective, epsilon_high)\n",
    "                _grouped_ppo_loss.check_shape(RESPONSE_IDS).log(\"before A_hat multiply\")\n",
    "                _grouped_ppo_loss = _grouped_ppo_loss * A_hat\n",
    "                grouped_ppo_loss = _grouped_ppo_loss.mean(dim=-1).check_shape([batch_size])\n",
    "                grouped_ppo_loss.log() # sample epsilon=0.2\n",
    "    \n",
    "                # Remove now-unused intermediate tensors.\n",
    "                A_hat = None\n",
    "                unclipped_objective = None\n",
    "                clipped_ratio = None\n",
    "                clipped_objective = None\n",
    "            \n",
    "            \n",
    "            # kl_lambda is a scaling factor for the KL term\n",
    "            _grpo_loss = grouped_ppo_loss + hparam.kl_lambda * kl_div\n",
    "            _grpo_loss.check_shape([batch_size]).log() \n",
    "            grpo_loss = _grpo_loss.mean()\n",
    "            grpo_loss.log() # []\n",
    "\n",
    "            # Save scalar values for logging before clearing.\n",
    "            ppo_loss_val = grouped_ppo_loss.mean().item()\n",
    "            grpo_loss_val = grpo_loss.mean().item()\n",
    "\n",
    "            # Remove now-unused intermediate tensors.\n",
    "            respone_ids = None\n",
    "            grouped_ppo_loss = None\n",
    "            kl_div = None\n",
    "            probability_ratio = None\n",
    "\n",
    "            # Remove now-unused intermediate tensors.\n",
    "            per_token_loss = None\n",
    "            \n",
    "            # 6. Backpropagation and parameter update (only if not in validation mode).\n",
    "            print_step(\"6. Backpropagation and parameter update\") \n",
    "            is_param_updated = False\n",
    "\n",
    "        if not is_validation:\n",
    "            scaler.scale(grpo_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            write_weight_state(model, writer, step, log_group+'_weights')\n",
    "            if False:\n",
    "                change_grad(model, 0, 2, multiple=0.01)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            is_param_updated = True\n",
    "            # Remove unused variables from the current iteration.\n",
    "            grpo_loss = None\n",
    "            #compare_memory_ids(start_tensor_ids)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(f\"{log_group}/lr\", current_lr, step)\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            grpo_loss = None\n",
    "\n",
    "        running_loss += grpo_loss_val\n",
    "\n",
    "        # 7. Logging with dynamic log group.\n",
    "        print_step(\"7. Logging\")\n",
    "        mean_reward = sum(rewards) / len(rewards)\n",
    "        metrics = {\n",
    "            'mean_reward': mean_reward,\n",
    "            'loss': grpo_loss_val\n",
    "        }\n",
    "        #writer.add_hparams(hparam_dict=hparams, metric_dict=metrics, global_step=global_step)\n",
    "        writer.add_scalar(f\"{log_group}/grpo_loss\", grpo_loss_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/ppo_loss\", ppo_loss_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/kl_div\", kl_div_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/mean_reward\", mean_reward, global_step)\n",
    "        mean_reward_list.append(mean_reward)\n",
    "        mean_loss_list.append(grpo_loss_val)\n",
    "        if is_param_updated:\n",
    "            writer.add_scalar(f\"{log_group}/model_update_grpo_loss\", grpo_loss_val, global_step)\n",
    "\n",
    "        print_step(\"8. End Loop\")\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_mean_reward =  sum(mean_reward_list) / len(mean_reward_list)\n",
    "    mean_loss =  sum(mean_loss_list) / len(mean_loss_list)\n",
    "    print_step(\"_.1. run() exit\")\n",
    "    return running_loss, epoch_mean_reward, global_step\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start reasoning logic....\")\n",
    "    main(hparam, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9d8ad-168d-493f-8bb2-b4ea85872d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3302ee1-5348-49e1-b4de-b6048e6ad978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
