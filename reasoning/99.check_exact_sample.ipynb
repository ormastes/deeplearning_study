{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchtext-0.18.0a0+9bed85d-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchaudio-2.6.0a0+d883142-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch_optimizer\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
      "Collecting lion_pytorch\n",
      "  Downloading lion_pytorch-0.2.3-py3-none-any.whl.metadata (616 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (2.7.0a0+ecf3bae40a.nv25.2)\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "Downloading lion_pytorch-0.2.3-py3-none-any.whl (6.6 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, pytorch-ranger, lion_pytorch, torch_optimizer, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-3.4.1 dill-0.3.8 lion_pytorch-0.2.3 multiprocess-0.70.16 pytorch-ranger-0.1.1 torch_optimizer-0.3.0 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets torch_optimizer lion_pytorch --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3132c047-7f79-4954-8474-f801f45e9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, labels\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining step complete. Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Set pad_token to eos_token if it's not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Option 1: Adafactor\n",
    "# -------------------------------\n",
    "# Adafactor is available from the transformers library.\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "optimizer_adafactor = Adafactor(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,               # Learning rate can be tuned\n",
    "    relative_step=False,   # Set to True to use relative step sizes\n",
    "    scale_parameter=False  # Adjust scaling based on model size\n",
    ")\n",
    "\n",
    "# Example: Using one of the optimizers in a simple training loop\n",
    "# (Select the optimizer you want to use; here we choose optimizer_lamb)\n",
    "optimizer = optimizer_adafactor  # or optimizer_adafactor, optimizer_lion\n",
    "\n",
    "# Dummy input for illustration (normally you'd use your DataLoader)\n",
    "input_text = \"def hello_world():\\n    print('Hello, world!')\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=3096, truncation=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Training step complete. Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26468d1a-5e28-4343-bade-583cffde7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeb466a3c1f4dbc8f7a6392c7d66945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1700/2351177385.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_1700/2351177385.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1/8 Loss: 2.5770\n",
      "Epoch 1 Step 2/8 Loss: 1.2689\n",
      "Epoch 1 Step 3/8 Loss: 1.7873\n",
      "Epoch 1 Step 4/8 Loss: 2.1491\n",
      "Epoch 1 Step 5/8 Loss: 4.1185\n",
      "Epoch 1 Step 6/8 Loss: 3.1159\n",
      "Epoch 1 Step 7/8 Loss: 2.3917\n",
      "Epoch 1 Step 8/8 Loss: 3.9782\n",
      "Epoch 1 Step 1/8 Loss: 3.4981\n",
      "Epoch 1 Step 2/8 Loss: 3.8573\n",
      "Epoch 1 Step 3/8 Loss: 2.6436\n",
      "Epoch 1 Step 4/8 Loss: 3.5175\n",
      "Epoch 1 Step 5/8 Loss: 3.5117\n",
      "Epoch 1 Step 6/8 Loss: 4.6365\n",
      "Epoch 1 Step 7/8 Loss: 1.2986\n",
      "Epoch 1 Step 8/8 Loss: 1.7384\n",
      "Epoch 1 Step 1/8 Loss: 3.7704\n",
      "Epoch 1 Step 2/8 Loss: 2.7534\n",
      "Epoch 1 Step 3/8 Loss: 2.0086\n",
      "Epoch 1 Step 4/8 Loss: 3.1233\n",
      "Epoch 1 Step 5/8 Loss: 2.0856\n",
      "Epoch 1 Step 6/8 Loss: 3.0158\n",
      "Epoch 1 Step 7/8 Loss: 2.3204\n",
      "Epoch 1 Step 8/8 Loss: 1.4704\n",
      "Epoch 1 Step 1/8 Loss: 2.9487\n",
      "Epoch 1 Step 2/8 Loss: 2.7743\n",
      "Epoch 1 Step 3/8 Loss: 2.9133\n",
      "Epoch 1 Step 4/8 Loss: 2.3362\n",
      "Epoch 1 Step 5/8 Loss: 2.3366\n",
      "Epoch 1 Step 6/8 Loss: 2.4879\n",
      "Epoch 1 Step 7/8 Loss: 3.5574\n",
      "Epoch 1 Step 8/8 Loss: 2.8481\n",
      "Epoch 1 Step 1/8 Loss: 2.1511\n",
      "Epoch 1 Step 2/8 Loss: 2.0930\n",
      "Epoch 1 Step 3/8 Loss: 2.7516\n",
      "Epoch 1 Step 4/8 Loss: 3.5480\n",
      "Epoch 1 Step 5/8 Loss: 3.7439\n",
      "Epoch 1 Step 6/8 Loss: 1.2767\n",
      "Epoch 1 Step 7/8 Loss: 2.3962\n",
      "Epoch 1 Step 8/8 Loss: 4.2617\n",
      "Epoch 1 Step 1/8 Loss: 3.9554\n",
      "Epoch 1 Step 2/8 Loss: 2.5220\n",
      "Epoch 1 Step 3/8 Loss: 3.2970\n",
      "Epoch 1 Step 4/8 Loss: 2.6418\n",
      "Epoch 1 Step 5/8 Loss: 3.8465\n",
      "Epoch 1 Step 6/8 Loss: 2.2856\n",
      "Epoch 1 Step 7/8 Loss: 1.5832\n",
      "Epoch 1 Step 8/8 Loss: 1.8620\n",
      "Epoch 1 Step 1/8 Loss: 4.7586\n",
      "Epoch 1 Step 2/8 Loss: 1.8429\n",
      "Epoch 1 Step 3/8 Loss: 4.2836\n",
      "Epoch 1 Step 4/8 Loss: 1.3215\n",
      "Epoch 1 Step 5/8 Loss: 2.9130\n",
      "Epoch 1 Step 6/8 Loss: 2.2947\n",
      "Epoch 1 Step 7/8 Loss: 2.2903\n",
      "Epoch 1 Step 8/8 Loss: 3.1774\n",
      "Epoch 1 Step 1/8 Loss: 2.5557\n",
      "Epoch 1 Step 2/8 Loss: 2.6051\n",
      "Epoch 1 Step 3/8 Loss: 2.8013\n",
      "Epoch 1 Step 4/8 Loss: 2.5616\n",
      "Epoch 1 Step 5/8 Loss: 3.8039\n",
      "Epoch 1 Step 6/8 Loss: 2.7697\n",
      "Epoch 1 Step 7/8 Loss: 2.9991\n",
      "Epoch 1 Step 8/8 Loss: 1.2408\n",
      "Epoch 1 Step 1/8 Loss: 2.8954\n",
      "Epoch 1 Step 2/8 Loss: 2.6914\n",
      "Epoch 1 Step 3/8 Loss: 2.5354\n",
      "Epoch 1 Step 4/8 Loss: 3.2265\n",
      "Epoch 1 Step 5/8 Loss: 3.7303\n",
      "Epoch 1 Step 6/8 Loss: 1.8581\n",
      "Epoch 1 Step 7/8 Loss: 3.4126\n",
      "Epoch 1 Step 8/8 Loss: 2.1318\n",
      "Epoch 1 Step 1/8 Loss: 2.8976\n",
      "Epoch 1 Step 2/8 Loss: 3.1672\n",
      "Epoch 1 Step 3/8 Loss: 3.9434\n",
      "Epoch 1 Step 4/8 Loss: 1.4218\n",
      "Epoch 1 Step 5/8 Loss: 1.6545\n",
      "Epoch 1 Step 6/8 Loss: 2.9514\n",
      "Epoch 1 Step 7/8 Loss: 3.1865\n",
      "Epoch 1 Step 8/8 Loss: 2.1224\n",
      "Epoch 1 Step 1/8 Loss: 3.2297\n",
      "Epoch 1 Step 2/8 Loss: 3.1303\n",
      "Epoch 1 Step 3/8 Loss: 2.0211\n",
      "Epoch 1 Step 4/8 Loss: 2.3049\n",
      "Epoch 1 Step 5/8 Loss: 1.9418\n",
      "Epoch 1 Step 6/8 Loss: 3.2366\n",
      "Epoch 1 Step 7/8 Loss: 4.5552\n",
      "Epoch 1 Step 8/8 Loss: 1.7976\n",
      "Epoch 1 Step 1/8 Loss: 2.6152\n",
      "Epoch 1 Step 2/8 Loss: 1.5624\n",
      "Epoch 1 Step 3/8 Loss: 4.2727\n",
      "Epoch 1 Step 4/8 Loss: 4.2998\n",
      "Epoch 1 Step 5/8 Loss: 2.9353\n",
      "Epoch 1 Step 6/8 Loss: 2.7368\n",
      "Epoch 1 Step 7/8 Loss: 2.3044\n",
      "Epoch 1 Step 8/8 Loss: 2.9647\n",
      "Epoch 1 Step 1/8 Loss: 2.3143\n",
      "Epoch 1 Step 2/8 Loss: 2.1823\n",
      "Epoch 1 Step 3/8 Loss: 1.3192\n",
      "Epoch 1 Step 4/8 Loss: 2.3855\n",
      "Epoch 1 Step 5/8 Loss: 3.2766\n",
      "Epoch 1 Step 6/8 Loss: 3.2444\n",
      "Epoch 1 Step 7/8 Loss: 2.4843\n",
      "Epoch 1 Step 8/8 Loss: 2.2186\n",
      "Epoch 1 Step 1/8 Loss: 2.3804\n",
      "Epoch 1 Step 2/8 Loss: 2.5898\n",
      "Epoch 1 Step 3/8 Loss: 2.9837\n",
      "Epoch 1 Step 4/8 Loss: 1.2988\n",
      "Epoch 1 Step 5/8 Loss: 3.1778\n",
      "Epoch 1 Step 6/8 Loss: 3.5451\n",
      "Epoch 1 Step 7/8 Loss: 2.4830\n",
      "Epoch 1 Step 8/8 Loss: 1.9037\n",
      "Epoch 1 Step 1/8 Loss: 2.9274\n",
      "Epoch 1 Step 2/8 Loss: 3.1432\n",
      "Epoch 1 Step 3/8 Loss: 3.5764\n",
      "Epoch 1 Step 4/8 Loss: 2.0785\n",
      "Epoch 1 Step 5/8 Loss: 3.4157\n",
      "Epoch 1 Step 6/8 Loss: 1.2828\n",
      "Epoch 1 Step 7/8 Loss: 3.3211\n",
      "Epoch 1 Step 8/8 Loss: 2.0207\n",
      "Epoch 1 Step 1/8 Loss: 4.9472\n",
      "Epoch 1 Step 2/8 Loss: 1.7419\n",
      "Epoch 1 Step 3/8 Loss: 3.8744\n",
      "Epoch 1 Step 4/8 Loss: 4.3192\n",
      "Epoch 1 Step 5/8 Loss: 0.9873\n",
      "Epoch 1 Step 6/8 Loss: 2.8553\n",
      "Epoch 1 Step 7/8 Loss: 2.2392\n",
      "Epoch 1 Step 8/8 Loss: 2.8672\n",
      "Epoch 1 Step 1/8 Loss: 4.2845\n",
      "Epoch 1 Step 2/8 Loss: 2.4019\n",
      "Epoch 1 Step 3/8 Loss: 1.2293\n",
      "Epoch 1 Step 4/8 Loss: 2.1815\n",
      "Epoch 1 Step 5/8 Loss: 1.9848\n",
      "Epoch 1 Step 6/8 Loss: 3.2660\n",
      "Epoch 1 Step 7/8 Loss: 2.5924\n",
      "Epoch 1 Step 8/8 Loss: 2.7812\n",
      "Epoch 1 Step 1/8 Loss: 2.0764\n",
      "Epoch 1 Step 2/8 Loss: 3.1447\n",
      "Epoch 1 Step 3/8 Loss: 1.9649\n",
      "Epoch 1 Step 4/8 Loss: 1.2150\n",
      "Epoch 1 Step 5/8 Loss: 3.7990\n",
      "Epoch 1 Step 6/8 Loss: 3.6637\n",
      "Epoch 1 Step 7/8 Loss: 1.7520\n",
      "Epoch 1 Step 8/8 Loss: 3.3003\n",
      "Epoch 1 Step 1/8 Loss: 1.3344\n",
      "Epoch 1 Step 2/8 Loss: 2.1499\n",
      "Epoch 1 Step 3/8 Loss: 4.2951\n",
      "Epoch 1 Step 4/8 Loss: 2.7495\n",
      "Epoch 1 Step 5/8 Loss: 4.3411\n",
      "Epoch 1 Step 6/8 Loss: 1.9678\n",
      "Epoch 1 Step 7/8 Loss: 1.1575\n",
      "Epoch 1 Step 8/8 Loss: 3.6397\n",
      "Epoch 1 Step 1/8 Loss: 2.7447\n",
      "Epoch 1 Step 2/8 Loss: 1.9036\n",
      "Epoch 1 Step 3/8 Loss: 1.0702\n",
      "Epoch 1 Step 4/8 Loss: 1.5455\n",
      "Epoch 1 Step 5/8 Loss: 2.8784\n",
      "Epoch 1 Step 6/8 Loss: 1.6859\n",
      "Epoch 1 Step 7/8 Loss: 3.1098\n",
      "Epoch 1 Step 8/8 Loss: 3.1576\n",
      "Epoch 1 Step 1/8 Loss: 3.9459\n",
      "Epoch 1 Step 2/8 Loss: 2.7048\n",
      "Epoch 1 Step 3/8 Loss: 3.9354\n",
      "Epoch 1 Step 4/8 Loss: 3.2505\n",
      "Epoch 1 Step 5/8 Loss: 1.2666\n",
      "Epoch 1 Step 6/8 Loss: 2.4094\n",
      "Epoch 1 Step 7/8 Loss: 3.7539\n",
      "Epoch 1 Step 8/8 Loss: 3.7250\n",
      "Epoch 1 Step 1/8 Loss: 3.1181\n",
      "Epoch 1 Step 2/8 Loss: 1.7433\n",
      "Epoch 1 Step 3/8 Loss: 2.0625\n",
      "Epoch 1 Step 4/8 Loss: 2.3892\n",
      "Epoch 1 Step 5/8 Loss: 1.2257\n",
      "Epoch 1 Step 6/8 Loss: 3.3643\n",
      "Epoch 1 Step 7/8 Loss: 4.1368\n",
      "Epoch 1 Step 8/8 Loss: 2.2989\n",
      "Epoch 1 Step 1/8 Loss: 2.3432\n",
      "Epoch 1 Step 2/8 Loss: 1.1895\n",
      "Epoch 1 Step 3/8 Loss: 2.4585\n",
      "Epoch 1 Step 4/8 Loss: 2.1811\n",
      "Epoch 1 Step 5/8 Loss: 2.2084\n",
      "Epoch 1 Step 6/8 Loss: 3.3514\n",
      "Epoch 1 Step 7/8 Loss: 3.2523\n",
      "Epoch 1 Step 8/8 Loss: 3.7346\n",
      "Epoch 1 Step 1/8 Loss: 2.3186\n",
      "Epoch 1 Step 2/8 Loss: 2.6552\n",
      "Epoch 1 Step 3/8 Loss: 1.9866\n",
      "Epoch 1 Step 4/8 Loss: 1.5604\n",
      "Epoch 1 Step 5/8 Loss: 4.7004\n",
      "Epoch 1 Step 6/8 Loss: 1.2438\n",
      "Epoch 1 Step 7/8 Loss: 2.6922\n",
      "Epoch 1 Step 8/8 Loss: 2.6061\n",
      "Epoch 1 Step 1/8 Loss: 1.3693\n",
      "Epoch 1 Step 2/8 Loss: 1.2409\n",
      "Epoch 1 Step 3/8 Loss: 1.5182\n",
      "Epoch 1 Step 4/8 Loss: 3.7587\n",
      "Epoch 1 Step 5/8 Loss: 3.8075\n",
      "Epoch 1 Step 6/8 Loss: 2.0943\n",
      "Epoch 1 Step 7/8 Loss: 2.4884\n",
      "Epoch 1 Step 8/8 Loss: 3.8033\n",
      "Epoch 1 Step 1/8 Loss: 3.0333\n",
      "Epoch 1 Step 2/8 Loss: 3.5924\n",
      "Epoch 1 Step 3/8 Loss: 3.9896\n",
      "Epoch 1 Step 4/8 Loss: 1.1976\n",
      "Epoch 1 Step 5/8 Loss: 2.4739\n",
      "Epoch 1 Step 6/8 Loss: 2.0551\n",
      "Epoch 1 Step 7/8 Loss: 2.5728\n",
      "Epoch 1 Step 8/8 Loss: 2.4215\n",
      "Epoch 1 Step 1/8 Loss: 1.3498\n",
      "Epoch 1 Step 2/8 Loss: 2.7909\n",
      "Epoch 1 Step 3/8 Loss: 2.2794\n",
      "Epoch 1 Step 4/8 Loss: 1.3953\n",
      "Epoch 1 Step 5/8 Loss: 2.2890\n",
      "Epoch 1 Step 6/8 Loss: 2.1874\n",
      "Epoch 1 Step 7/8 Loss: 2.5304\n",
      "Epoch 1 Step 8/8 Loss: 2.3440\n",
      "Epoch 1 Step 1/8 Loss: 4.2810\n",
      "Epoch 1 Step 2/8 Loss: 3.5861\n",
      "Epoch 1 Step 3/8 Loss: 2.3234\n",
      "Epoch 1 Step 4/8 Loss: 2.5718\n",
      "Epoch 1 Step 5/8 Loss: 4.1888\n",
      "Epoch 1 Step 6/8 Loss: 3.6074\n",
      "Epoch 1 Step 7/8 Loss: 3.6144\n",
      "Epoch 1 Step 8/8 Loss: 2.6407\n",
      "Epoch 1 Step 1/8 Loss: 3.0154\n",
      "Epoch 1 Step 2/8 Loss: 3.2462\n",
      "Epoch 1 Step 3/8 Loss: 3.5369\n",
      "Epoch 1 Step 4/8 Loss: 2.5763\n",
      "Epoch 1 Step 5/8 Loss: 2.6529\n",
      "Epoch 1 Step 6/8 Loss: 3.2266\n",
      "Epoch 1 Step 7/8 Loss: 3.5636\n",
      "Epoch 1 Step 8/8 Loss: 2.3763\n",
      "Epoch 1 Step 1/8 Loss: 2.3323\n",
      "Epoch 1 Step 2/8 Loss: 2.9421\n",
      "Epoch 1 Step 3/8 Loss: 3.0211\n",
      "Epoch 1 Step 4/8 Loss: 2.5727\n",
      "Epoch 1 Step 5/8 Loss: 1.2129\n",
      "Epoch 1 Step 6/8 Loss: 2.6016\n",
      "Epoch 1 Step 7/8 Loss: 2.6201\n",
      "Epoch 1 Step 8/8 Loss: 3.0819\n",
      "Epoch 1 Step 1/8 Loss: 1.0095\n",
      "Epoch 1 Step 2/8 Loss: 3.2525\n",
      "Epoch 1 Step 3/8 Loss: 2.1593\n",
      "Epoch 1 Step 4/8 Loss: 2.1524\n",
      "Epoch 1 Step 5/8 Loss: 3.4207\n",
      "Epoch 1 Step 6/8 Loss: 2.7500\n",
      "Epoch 1 Step 7/8 Loss: 3.1311\n",
      "Epoch 1 Step 8/8 Loss: 2.9447\n",
      "Epoch 1 Step 1/8 Loss: 1.1431\n",
      "Epoch 1 Step 2/8 Loss: 2.5148\n",
      "Epoch 1 Step 3/8 Loss: 2.9847\n",
      "Epoch 1 Step 4/8 Loss: 2.5167\n",
      "Epoch 1 Step 5/8 Loss: 2.2012\n",
      "Epoch 1 Step 6/8 Loss: 2.7128\n",
      "Epoch 1 Step 7/8 Loss: 2.5495\n",
      "Epoch 1 Step 8/8 Loss: 2.7145\n",
      "Epoch 1 Step 1/8 Loss: 1.1910\n",
      "Epoch 1 Step 2/8 Loss: 2.9299\n",
      "Epoch 1 Step 3/8 Loss: 3.2005\n",
      "Epoch 1 Step 4/8 Loss: 1.7307\n",
      "Epoch 1 Step 5/8 Loss: 2.3610\n",
      "Epoch 1 Step 6/8 Loss: 2.7500\n",
      "Epoch 1 Step 7/8 Loss: 3.1841\n",
      "Epoch 1 Step 8/8 Loss: 3.1405\n",
      "Epoch 1 Step 1/8 Loss: 3.3551\n",
      "Epoch 1 Step 2/8 Loss: 3.9187\n",
      "Epoch 1 Step 3/8 Loss: 2.0621\n",
      "Epoch 1 Step 4/8 Loss: 2.4254\n",
      "Epoch 1 Step 5/8 Loss: 2.0916\n",
      "Epoch 1 Step 6/8 Loss: 3.1246\n",
      "Epoch 1 Step 7/8 Loss: 1.6947\n",
      "Epoch 1 Step 8/8 Loss: 1.7732\n",
      "Epoch 1 Step 1/8 Loss: 1.0997\n",
      "Epoch 1 Step 2/8 Loss: 3.3316\n",
      "Epoch 1 Step 3/8 Loss: 1.7944\n",
      "Epoch 1 Step 4/8 Loss: 1.3563\n",
      "Epoch 1 Step 5/8 Loss: 2.7914\n",
      "Epoch 1 Step 6/8 Loss: 3.5179\n",
      "Epoch 1 Step 7/8 Loss: 2.4853\n",
      "Epoch 1 Step 8/8 Loss: 2.9360\n",
      "Epoch 1 Step 1/8 Loss: 2.5509\n",
      "Epoch 1 Step 2/8 Loss: 3.5415\n",
      "Epoch 1 Step 3/8 Loss: 2.3330\n",
      "Epoch 1 Step 4/8 Loss: 2.6031\n",
      "Epoch 1 Step 5/8 Loss: 1.2954\n",
      "Epoch 1 Step 6/8 Loss: 5.5904\n",
      "Epoch 1 Step 7/8 Loss: 2.2150\n",
      "Epoch 1 Step 8/8 Loss: 2.5809\n",
      "Epoch 1 Step 1/8 Loss: 2.7511\n",
      "Epoch 1 Step 2/8 Loss: 1.3953\n",
      "Epoch 1 Step 3/8 Loss: 3.1329\n",
      "Epoch 1 Step 4/8 Loss: 2.8657\n",
      "Epoch 1 Step 5/8 Loss: 2.9353\n",
      "Epoch 1 Step 6/8 Loss: 3.3829\n",
      "Epoch 1 Step 7/8 Loss: 1.5750\n",
      "Epoch 1 Step 8/8 Loss: 1.9243\n",
      "Epoch 1 Step 1/8 Loss: 3.4707\n",
      "Epoch 1 Step 2/8 Loss: 3.2446\n",
      "Epoch 1 Step 3/8 Loss: 2.7127\n",
      "Epoch 1 Step 4/8 Loss: 2.7861\n",
      "Epoch 1 Step 5/8 Loss: 2.5667\n",
      "Epoch 1 Step 6/8 Loss: 2.7856\n",
      "Epoch 1 Step 7/8 Loss: 1.1586\n",
      "Epoch 1 Step 8/8 Loss: 2.2538\n",
      "Epoch 1 Step 1/8 Loss: 2.1696\n",
      "Epoch 1 Step 2/8 Loss: 2.3822\n",
      "Epoch 1 Step 3/8 Loss: 1.2661\n",
      "Epoch 1 Step 4/8 Loss: 1.5295\n",
      "Epoch 1 Step 5/8 Loss: 2.8745\n",
      "Epoch 1 Step 6/8 Loss: 2.9741\n",
      "Epoch 1 Step 7/8 Loss: 2.0756\n",
      "Epoch 1 Step 8/8 Loss: 3.1934\n",
      "Epoch 1 Step 1/8 Loss: 2.1234\n",
      "Epoch 1 Step 2/8 Loss: 2.0219\n",
      "Epoch 1 Step 3/8 Loss: 1.8296\n",
      "Epoch 1 Step 4/8 Loss: 2.8842\n",
      "Epoch 1 Step 5/8 Loss: 3.1064\n",
      "Epoch 1 Step 6/8 Loss: 2.9012\n",
      "Epoch 1 Step 7/8 Loss: 3.0377\n",
      "Epoch 1 Step 8/8 Loss: 3.9629\n",
      "Epoch 1 Step 1/8 Loss: 4.3761\n",
      "Epoch 1 Step 2/8 Loss: 2.6178\n",
      "Epoch 1 Step 3/8 Loss: 0.8624\n",
      "Epoch 1 Step 4/8 Loss: 2.7147\n",
      "Epoch 1 Step 5/8 Loss: 2.8495\n",
      "Epoch 1 Step 6/8 Loss: 2.3221\n",
      "Epoch 1 Step 7/8 Loss: 2.6777\n",
      "Epoch 1 Step 8/8 Loss: 3.3350\n",
      "Epoch 1 Step 1/8 Loss: 1.4075\n",
      "Epoch 1 Step 2/8 Loss: 2.1587\n",
      "Epoch 1 Step 3/8 Loss: 1.3938\n",
      "Epoch 1 Step 4/8 Loss: 1.5195\n",
      "Epoch 1 Step 5/8 Loss: 3.5298\n",
      "Epoch 1 Step 6/8 Loss: 1.9163\n",
      "Epoch 1 Step 7/8 Loss: 2.3052\n",
      "Epoch 1 Step 8/8 Loss: 2.7676\n",
      "Epoch 1 Step 1/8 Loss: 2.4163\n",
      "Epoch 1 Step 2/8 Loss: 2.2261\n",
      "Epoch 1 Step 3/8 Loss: 2.4042\n",
      "Epoch 1 Step 4/8 Loss: 3.2373\n",
      "Epoch 1 Step 5/8 Loss: 2.2738\n",
      "Epoch 1 Step 6/8 Loss: 2.9009\n",
      "Epoch 1 Step 7/8 Loss: 3.5691\n",
      "Epoch 1 Step 8/8 Loss: 1.3166\n",
      "Epoch 1 Step 1/8 Loss: 1.6385\n",
      "Epoch 1 Step 2/8 Loss: 2.9760\n",
      "Epoch 1 Step 3/8 Loss: 2.6545\n",
      "Epoch 1 Step 4/8 Loss: 2.3548\n",
      "Epoch 1 Step 5/8 Loss: 2.6595\n",
      "Epoch 1 Step 6/8 Loss: 2.2909\n",
      "Epoch 1 Step 7/8 Loss: 4.1456\n",
      "Epoch 1 Step 8/8 Loss: 4.3360\n",
      "Epoch 1 Step 1/8 Loss: 2.0982\n",
      "Epoch 1 Step 2/8 Loss: 3.2691\n",
      "Epoch 1 Step 3/8 Loss: 3.5516\n",
      "Epoch 1 Step 4/8 Loss: 2.4343\n",
      "Epoch 1 Step 5/8 Loss: 2.8736\n",
      "Epoch 1 Step 6/8 Loss: 4.2075\n",
      "Epoch 1 Step 7/8 Loss: 2.3437\n",
      "Epoch 1 Step 8/8 Loss: 2.9687\n",
      "Epoch 1 Step 1/8 Loss: 1.5928\n",
      "Epoch 1 Step 2/8 Loss: 1.5229\n",
      "Epoch 1 Step 3/8 Loss: 2.8443\n",
      "Epoch 1 Step 4/8 Loss: 1.9884\n",
      "Epoch 1 Step 5/8 Loss: 2.0778\n",
      "Epoch 1 Step 6/8 Loss: 2.8796\n",
      "Epoch 1 Step 7/8 Loss: 3.2829\n",
      "Epoch 1 Step 8/8 Loss: 2.3420\n",
      "Epoch 1 Step 1/8 Loss: 1.9756\n",
      "Epoch 1 Step 2/8 Loss: 4.5315\n",
      "Epoch 1 Step 3/8 Loss: 4.3775\n",
      "Epoch 1 Step 4/8 Loss: 1.2556\n",
      "Epoch 1 Step 5/8 Loss: 1.8261\n",
      "Epoch 1 Step 6/8 Loss: 3.7449\n",
      "Epoch 1 Step 7/8 Loss: 1.8437\n",
      "Epoch 1 Step 8/8 Loss: 2.4813\n",
      "Epoch 1 Step 1/8 Loss: 2.8380\n",
      "Epoch 1 Step 2/8 Loss: 2.5670\n",
      "Epoch 1 Step 3/8 Loss: 2.0634\n",
      "Epoch 1 Step 4/8 Loss: 4.5795\n",
      "Epoch 1 Step 5/8 Loss: 1.6905\n",
      "Epoch 1 Step 6/8 Loss: 2.5069\n",
      "Epoch 1 Step 7/8 Loss: 2.5600\n",
      "Epoch 1 Step 8/8 Loss: 3.2730\n",
      "Epoch 1 Step 1/8 Loss: 3.8524\n",
      "Epoch 1 Step 2/8 Loss: 1.5045\n",
      "Epoch 1 Step 3/8 Loss: 2.3742\n",
      "Epoch 1 Step 4/8 Loss: 2.7658\n",
      "Epoch 1 Step 5/8 Loss: 2.4447\n",
      "Epoch 1 Step 6/8 Loss: 2.0411\n",
      "Epoch 1 Step 7/8 Loss: 3.9395\n",
      "Epoch 1 Step 8/8 Loss: 3.0751\n",
      "Epoch 1 Step 1/8 Loss: 3.5863\n",
      "Epoch 1 Step 2/8 Loss: 2.2810\n",
      "Epoch 1 Step 3/8 Loss: 1.2305\n",
      "Epoch 1 Step 4/8 Loss: 1.5179\n",
      "Epoch 1 Step 5/8 Loss: 3.1002\n",
      "Epoch 1 Step 6/8 Loss: 2.6679\n",
      "Epoch 1 Step 7/8 Loss: 2.4624\n",
      "Epoch 1 Step 8/8 Loss: 4.5826\n",
      "Epoch 1 Step 1/8 Loss: 4.6097\n",
      "Epoch 1 Step 2/8 Loss: 3.2749\n",
      "Epoch 1 Step 3/8 Loss: 1.4235\n",
      "Epoch 1 Step 4/8 Loss: 2.3820\n",
      "Epoch 1 Step 5/8 Loss: 2.0862\n",
      "Epoch 1 Step 6/8 Loss: 2.4618\n",
      "Epoch 1 Step 7/8 Loss: 1.5134\n",
      "Epoch 1 Step 8/8 Loss: 2.5764\n",
      "Epoch 1 Step 1/8 Loss: 2.4563\n",
      "Epoch 1 Step 2/8 Loss: 1.6314\n",
      "Epoch 1 Step 3/8 Loss: 2.9484\n",
      "Epoch 1 Step 4/8 Loss: 0.8779\n",
      "Epoch 1 Step 5/8 Loss: 3.3815\n",
      "Epoch 1 Step 6/8 Loss: 2.1531\n",
      "Epoch 1 Step 7/8 Loss: 2.4594\n",
      "Epoch 1 Step 8/8 Loss: 2.3800\n",
      "Epoch 1 Step 1/8 Loss: 3.1881\n",
      "Epoch 1 Step 2/8 Loss: 2.2026\n",
      "Epoch 1 Step 3/8 Loss: 1.5281\n",
      "Epoch 1 Step 4/8 Loss: 3.1447\n",
      "Epoch 1 Step 5/8 Loss: 3.0305\n",
      "Epoch 1 Step 6/8 Loss: 2.8280\n",
      "Epoch 1 Step 7/8 Loss: 2.4354\n",
      "Epoch 1 Step 8/8 Loss: 2.6408\n",
      "Epoch 1 Step 1/8 Loss: 2.4518\n",
      "Epoch 1 Step 2/8 Loss: 1.9876\n",
      "Epoch 1 Step 3/8 Loss: 3.9229\n",
      "Epoch 1 Step 4/8 Loss: 4.2061\n",
      "Epoch 1 Step 5/8 Loss: 2.3647\n",
      "Epoch 1 Step 6/8 Loss: 1.0107\n",
      "Epoch 1 Step 7/8 Loss: 2.6171\n",
      "Epoch 1 Step 8/8 Loss: 3.1442\n",
      "Epoch 1 Step 1/8 Loss: 2.0541\n",
      "Epoch 1 Step 2/8 Loss: 1.3747\n",
      "Epoch 1 Step 3/8 Loss: 4.4283\n",
      "Epoch 1 Step 4/8 Loss: 4.0942\n",
      "Epoch 1 Step 5/8 Loss: 3.5288\n",
      "Epoch 1 Step 6/8 Loss: 1.4968\n",
      "Epoch 1 Step 7/8 Loss: 1.5590\n",
      "Epoch 1 Step 8/8 Loss: 2.8139\n",
      "Epoch 1 Step 1/8 Loss: 2.3923\n",
      "Epoch 1 Step 2/8 Loss: 2.3170\n",
      "Epoch 1 Step 3/8 Loss: 3.3133\n",
      "Epoch 1 Step 4/8 Loss: 3.0630\n",
      "Epoch 1 Step 5/8 Loss: 4.0600\n",
      "Epoch 1 Step 6/8 Loss: 2.4082\n",
      "Epoch 1 Step 7/8 Loss: 5.7414\n",
      "Epoch 1 Step 8/8 Loss: 2.3886\n",
      "Epoch 1 Step 1/8 Loss: 2.0732\n",
      "Epoch 1 Step 2/8 Loss: 1.5312\n",
      "Epoch 1 Step 3/8 Loss: 2.2526\n",
      "Epoch 1 Step 4/8 Loss: 3.6578\n",
      "Epoch 1 Step 5/8 Loss: 3.3845\n",
      "Epoch 1 Step 6/8 Loss: 2.1245\n",
      "Epoch 1 Step 7/8 Loss: 2.6311\n",
      "Epoch 1 Step 8/8 Loss: 3.5666\n",
      "Epoch 1 Step 1/8 Loss: 3.2246\n",
      "Epoch 1 Step 2/8 Loss: 1.9240\n",
      "Epoch 1 Step 3/8 Loss: 1.2834\n",
      "Epoch 1 Step 4/8 Loss: 4.0509\n",
      "Epoch 1 Step 5/8 Loss: 1.9323\n",
      "Epoch 1 Step 6/8 Loss: 1.3934\n",
      "Epoch 1 Step 7/8 Loss: 2.2964\n",
      "Epoch 1 Step 8/8 Loss: 2.1390\n",
      "Epoch 1 Step 1/8 Loss: 2.9675\n",
      "Epoch 1 Step 2/8 Loss: 2.4524\n",
      "Epoch 1 Step 3/8 Loss: 2.6136\n",
      "Epoch 1 Step 4/8 Loss: 3.5050\n",
      "Epoch 1 Step 5/8 Loss: 1.0109\n",
      "Epoch 1 Step 6/8 Loss: 2.4157\n",
      "Epoch 1 Step 7/8 Loss: 2.8291\n",
      "Epoch 1 Step 8/8 Loss: 3.5417\n",
      "Epoch 1 Step 1/8 Loss: 2.5079\n",
      "Epoch 1 Step 2/8 Loss: 1.1214\n",
      "Epoch 1 Step 3/8 Loss: 2.3462\n",
      "Epoch 1 Step 4/8 Loss: 2.2764\n",
      "Epoch 1 Step 5/8 Loss: 2.7763\n",
      "Epoch 1 Step 6/8 Loss: 2.5293\n",
      "Epoch 1 Step 7/8 Loss: 2.4108\n",
      "Epoch 1 Step 8/8 Loss: 3.3806\n",
      "Epoch 1 Step 1/8 Loss: 2.8471\n",
      "Epoch 1 Step 2/8 Loss: 2.1855\n",
      "Epoch 1 Step 3/8 Loss: 2.0732\n",
      "Epoch 1 Step 4/8 Loss: 2.2541\n",
      "Epoch 1 Step 5/8 Loss: 1.8668\n",
      "Epoch 1 Step 6/8 Loss: 4.1929\n",
      "Epoch 1 Step 7/8 Loss: 2.0867\n",
      "Epoch 1 Step 8/8 Loss: 3.5352\n",
      "Epoch 1 Step 1/8 Loss: 1.2252\n",
      "Epoch 1 Step 2/8 Loss: 1.2615\n",
      "Epoch 1 Step 3/8 Loss: 2.6398\n",
      "Epoch 1 Step 4/8 Loss: 3.0373\n",
      "Epoch 1 Step 5/8 Loss: 4.1518\n",
      "Epoch 1 Step 6/8 Loss: 2.6481\n",
      "Epoch 1 Step 7/8 Loss: 1.0806\n",
      "Epoch 1 Step 8/8 Loss: 2.1535\n",
      "Epoch 1 Step 1/8 Loss: 3.3041\n",
      "Epoch 1 Step 2/8 Loss: 2.3550\n",
      "Epoch 1 Step 3/8 Loss: 3.0779\n",
      "Epoch 1 Step 4/8 Loss: 1.1236\n",
      "Epoch 1 Step 5/8 Loss: 1.4817\n",
      "Epoch 1 Step 6/8 Loss: 3.2817\n",
      "Epoch 1 Step 7/8 Loss: 2.1945\n",
      "Epoch 1 Step 8/8 Loss: 3.0048\n",
      "Epoch 1 Step 1/8 Loss: 3.6076\n",
      "Epoch 1 Step 2/8 Loss: 3.4457\n",
      "Epoch 1 Step 3/8 Loss: 3.9067\n",
      "Epoch 1 Step 4/8 Loss: 2.6814\n",
      "Epoch 1 Step 5/8 Loss: 3.2035\n",
      "Epoch 1 Step 6/8 Loss: 1.0849\n",
      "Epoch 1 Step 7/8 Loss: 1.8560\n",
      "Epoch 1 Step 8/8 Loss: 1.7059\n",
      "Epoch 1 Step 1/8 Loss: 1.3694\n",
      "Epoch 1 Step 2/8 Loss: 3.3346\n",
      "Epoch 1 Step 3/8 Loss: 3.9486\n",
      "Epoch 1 Step 4/8 Loss: 2.9365\n",
      "Epoch 1 Step 5/8 Loss: 3.2178\n",
      "Epoch 1 Step 6/8 Loss: 2.0648\n",
      "Epoch 1 Step 7/8 Loss: 2.5978\n",
      "Epoch 1 Step 8/8 Loss: 1.3226\n",
      "Epoch 1 Step 1/8 Loss: 2.2945\n",
      "Epoch 1 Step 2/8 Loss: 2.4510\n",
      "Epoch 1 Step 3/8 Loss: 3.6382\n",
      "Epoch 1 Step 4/8 Loss: 3.9632\n",
      "Epoch 1 Step 5/8 Loss: 2.1169\n",
      "Epoch 1 Step 6/8 Loss: 1.8856\n",
      "Epoch 1 Step 7/8 Loss: 2.5788\n",
      "Epoch 1 Step 8/8 Loss: 2.3368\n",
      "Epoch 1 Step 1/8 Loss: 2.3138\n",
      "Epoch 1 Step 2/8 Loss: 1.2535\n",
      "Epoch 1 Step 3/8 Loss: 3.2343\n",
      "Epoch 1 Step 4/8 Loss: 3.0279\n",
      "Epoch 1 Step 5/8 Loss: 2.3755\n",
      "Epoch 1 Step 6/8 Loss: 1.1068\n",
      "Epoch 1 Step 7/8 Loss: 2.4711\n",
      "Epoch 1 Step 8/8 Loss: 4.0198\n",
      "Epoch 1 Step 1/8 Loss: 2.7154\n",
      "Epoch 1 Step 2/8 Loss: 2.3324\n",
      "Epoch 1 Step 3/8 Loss: 2.8380\n",
      "Epoch 1 Step 4/8 Loss: 4.1353\n",
      "Epoch 1 Step 5/8 Loss: 3.3753\n",
      "Epoch 1 Step 6/8 Loss: 3.2370\n",
      "Epoch 1 Step 7/8 Loss: 1.3341\n",
      "Epoch 1 Step 8/8 Loss: 1.9967\n",
      "Epoch 1 Step 1/8 Loss: 3.5316\n",
      "Epoch 1 Step 2/8 Loss: 4.1718\n",
      "Epoch 1 Step 3/8 Loss: 2.4316\n",
      "Epoch 1 Step 4/8 Loss: 4.0150\n",
      "Epoch 1 Step 5/8 Loss: 2.0709\n",
      "Epoch 1 Step 6/8 Loss: 2.3010\n",
      "Epoch 1 Step 7/8 Loss: 3.3557\n",
      "Epoch 1 Step 8/8 Loss: 3.3569\n",
      "Epoch 1 Step 1/8 Loss: 2.4698\n",
      "Epoch 1 Step 2/8 Loss: 2.4324\n",
      "Epoch 1 Step 3/8 Loss: 2.2718\n",
      "Epoch 1 Step 4/8 Loss: 1.4953\n",
      "Epoch 1 Step 5/8 Loss: 1.3160\n",
      "Epoch 1 Step 6/8 Loss: 3.3251\n",
      "Epoch 1 Step 7/8 Loss: 4.3239\n",
      "Epoch 1 Step 8/8 Loss: 3.3899\n",
      "Epoch 1 Step 1/8 Loss: 1.2188\n",
      "Epoch 1 Step 2/8 Loss: 2.8703\n",
      "Epoch 1 Step 3/8 Loss: 1.0597\n",
      "Epoch 1 Step 4/8 Loss: 1.8500\n",
      "Epoch 1 Step 5/8 Loss: 3.1534\n",
      "Epoch 1 Step 6/8 Loss: 3.1120\n",
      "Epoch 1 Step 7/8 Loss: 2.9154\n",
      "Epoch 1 Step 8/8 Loss: 2.6671\n",
      "Epoch 1 Step 1/8 Loss: 4.1004\n",
      "Epoch 1 Step 2/8 Loss: 3.0422\n",
      "Epoch 1 Step 3/8 Loss: 3.0061\n",
      "Epoch 1 Step 4/8 Loss: 2.3085\n",
      "Epoch 1 Step 5/8 Loss: 2.0061\n",
      "Epoch 1 Step 6/8 Loss: 1.2711\n",
      "Epoch 1 Step 7/8 Loss: 3.2719\n",
      "Epoch 1 Step 8/8 Loss: 2.9340\n",
      "Epoch 1 Step 1/8 Loss: 2.4617\n",
      "Epoch 1 Step 2/8 Loss: 1.2635\n",
      "Epoch 1 Step 3/8 Loss: 2.3188\n",
      "Epoch 1 Step 4/8 Loss: 3.7996\n",
      "Epoch 1 Step 5/8 Loss: 2.9347\n",
      "Epoch 1 Step 6/8 Loss: 2.6991\n",
      "Epoch 1 Step 7/8 Loss: 1.9907\n",
      "Epoch 1 Step 8/8 Loss: 1.0686\n",
      "Epoch 1 Step 1/8 Loss: 3.2442\n",
      "Epoch 1 Step 2/8 Loss: 2.2767\n",
      "Epoch 1 Step 3/8 Loss: 2.2652\n",
      "Epoch 1 Step 4/8 Loss: 1.0802\n",
      "Epoch 1 Step 5/8 Loss: 2.3911\n",
      "Epoch 1 Step 6/8 Loss: 0.9885\n",
      "Epoch 1 Step 7/8 Loss: 2.4640\n",
      "Epoch 1 Step 8/8 Loss: 3.2803\n",
      "Epoch 1 Step 1/8 Loss: 3.4166\n",
      "Epoch 1 Step 2/8 Loss: 2.7402\n",
      "Epoch 1 Step 3/8 Loss: 3.7648\n",
      "Epoch 1 Step 4/8 Loss: 3.4557\n",
      "Epoch 1 Step 5/8 Loss: 2.4675\n",
      "Epoch 1 Step 6/8 Loss: 2.8873\n",
      "Epoch 1 Step 7/8 Loss: 1.2050\n",
      "Epoch 1 Step 8/8 Loss: 2.4764\n",
      "Epoch 1 Step 1/8 Loss: 4.0915\n",
      "Epoch 1 Step 2/8 Loss: 4.0677\n",
      "Epoch 1 Step 3/8 Loss: 1.4317\n",
      "Epoch 1 Step 4/8 Loss: 1.1949\n",
      "Epoch 1 Step 5/8 Loss: 2.2194\n",
      "Epoch 1 Step 6/8 Loss: 3.4082\n",
      "Epoch 1 Step 7/8 Loss: 4.8282\n",
      "Epoch 1 Step 8/8 Loss: 3.1423\n",
      "Epoch 1 Step 1/8 Loss: 2.4334\n",
      "Epoch 1 Step 2/8 Loss: 2.4692\n",
      "Epoch 1 Step 3/8 Loss: 2.4597\n",
      "Epoch 1 Step 4/8 Loss: 2.8056\n",
      "Epoch 1 Step 5/8 Loss: 2.8163\n",
      "Epoch 1 Step 6/8 Loss: 2.1326\n",
      "Epoch 1 Step 7/8 Loss: 2.2261\n",
      "Epoch 1 Step 8/8 Loss: 2.2179\n",
      "Epoch 1 Step 1/8 Loss: 2.5582\n",
      "Epoch 1 Step 2/8 Loss: 3.1430\n",
      "Epoch 1 Step 3/8 Loss: 2.5050\n",
      "Epoch 1 Step 4/8 Loss: 2.1935\n",
      "Epoch 1 Step 5/8 Loss: 1.1635\n",
      "Epoch 1 Step 6/8 Loss: 1.2628\n",
      "Epoch 1 Step 7/8 Loss: 1.5934\n",
      "Epoch 1 Step 8/8 Loss: 3.7552\n",
      "Epoch 1 Step 1/8 Loss: 2.4998\n",
      "Epoch 1 Step 2/8 Loss: 4.0282\n",
      "Epoch 1 Step 3/8 Loss: 3.2219\n",
      "Epoch 1 Step 4/8 Loss: 2.8878\n",
      "Epoch 1 Step 5/8 Loss: 2.3202\n",
      "Epoch 1 Step 6/8 Loss: 1.3736\n",
      "Epoch 1 Step 7/8 Loss: 0.9222\n",
      "Epoch 1 Step 8/8 Loss: 1.9873\n",
      "Epoch 1 Step 1/8 Loss: 2.1067\n",
      "Epoch 1 Step 2/8 Loss: 2.2407\n",
      "Epoch 1 Step 3/8 Loss: 2.3131\n",
      "Epoch 1 Step 4/8 Loss: 1.3694\n",
      "Epoch 1 Step 5/8 Loss: 2.3885\n",
      "Epoch 1 Step 6/8 Loss: 3.1360\n",
      "Epoch 1 Step 7/8 Loss: 3.6925\n",
      "Epoch 1 Step 8/8 Loss: 4.3643\n",
      "Epoch 1 Step 1/8 Loss: 3.1510\n",
      "Epoch 1 Step 2/8 Loss: 2.6240\n",
      "Epoch 1 Step 3/8 Loss: 5.0161\n",
      "Epoch 1 Step 4/8 Loss: 2.9509\n",
      "Epoch 1 Step 5/8 Loss: 2.5886\n",
      "Epoch 1 Step 6/8 Loss: 3.5078\n",
      "Epoch 1 Step 7/8 Loss: 4.5440\n",
      "Epoch 1 Step 8/8 Loss: 1.6739\n",
      "Epoch 1 Step 1/8 Loss: 2.6918\n",
      "Epoch 1 Step 2/8 Loss: 3.3549\n",
      "Epoch 1 Step 3/8 Loss: 2.5663\n",
      "Epoch 1 Step 4/8 Loss: 3.8599\n",
      "Epoch 1 Step 5/8 Loss: 2.0973\n",
      "Epoch 1 Step 6/8 Loss: 2.8786\n",
      "Epoch 1 Step 7/8 Loss: 2.7126\n",
      "Epoch 1 Step 8/8 Loss: 2.9479\n",
      "Epoch 1 Step 1/8 Loss: 0.9107\n",
      "Epoch 1 Step 2/8 Loss: 2.7144\n",
      "Epoch 1 Step 3/8 Loss: 2.9027\n",
      "Epoch 1 Step 4/8 Loss: 3.3520\n",
      "Epoch 1 Step 5/8 Loss: 1.6482\n",
      "Epoch 1 Step 6/8 Loss: 1.3448\n",
      "Epoch 1 Step 7/8 Loss: 2.6529\n",
      "Epoch 1 Step 8/8 Loss: 3.9207\n",
      "Epoch 1 Step 1/8 Loss: 3.5438\n",
      "Epoch 1 Step 2/8 Loss: 4.1399\n",
      "Epoch 1 Step 3/8 Loss: 2.4409\n",
      "Epoch 1 Step 4/8 Loss: 3.0887\n",
      "Epoch 1 Step 5/8 Loss: 2.1993\n",
      "Epoch 1 Step 6/8 Loss: 2.5641\n",
      "Epoch 1 Step 7/8 Loss: 2.1233\n",
      "Epoch 1 Step 8/8 Loss: 2.6528\n",
      "Epoch 1 Step 1/8 Loss: 0.8422\n",
      "Epoch 1 Step 2/8 Loss: 2.6816\n",
      "Epoch 1 Step 3/8 Loss: 2.0727\n",
      "Epoch 1 Step 4/8 Loss: 2.6764\n",
      "Epoch 1 Step 5/8 Loss: 4.3599\n",
      "Epoch 1 Step 6/8 Loss: 1.1612\n",
      "Epoch 1 Step 7/8 Loss: 2.4225\n",
      "Epoch 1 Step 8/8 Loss: 2.0671\n",
      "Epoch 1 Step 1/8 Loss: 2.8549\n",
      "Epoch 1 Step 2/8 Loss: 2.2555\n",
      "Epoch 1 Step 3/8 Loss: 3.0856\n",
      "Epoch 1 Step 4/8 Loss: 2.5843\n",
      "Epoch 1 Step 5/8 Loss: 1.9934\n",
      "Epoch 1 Step 6/8 Loss: 3.2736\n",
      "Epoch 1 Step 7/8 Loss: 3.8794\n",
      "Epoch 1 Step 8/8 Loss: 2.7085\n",
      "Epoch 1 Step 1/8 Loss: 3.0974\n",
      "Epoch 1 Step 2/8 Loss: 2.8928\n",
      "Epoch 1 Step 3/8 Loss: 1.1822\n",
      "Epoch 1 Step 4/8 Loss: 2.7244\n",
      "Epoch 1 Step 5/8 Loss: 2.7747\n",
      "Epoch 1 Step 6/8 Loss: 2.5804\n",
      "Epoch 1 Step 7/8 Loss: 1.7447\n",
      "Epoch 1 Step 8/8 Loss: 3.2291\n",
      "Epoch 1 Step 1/8 Loss: 2.0722\n",
      "Epoch 1 Step 2/8 Loss: 4.1325\n",
      "Epoch 1 Step 3/8 Loss: 3.1335\n",
      "Epoch 1 Step 4/8 Loss: 2.3422\n",
      "Epoch 1 Step 5/8 Loss: 2.5424\n",
      "Epoch 1 Step 6/8 Loss: 2.8913\n",
      "Epoch 1 Step 7/8 Loss: 2.3885\n",
      "Epoch 1 Step 8/8 Loss: 1.3600\n",
      "Epoch 1 Step 1/8 Loss: 2.2996\n",
      "Epoch 1 Step 2/8 Loss: 3.6663\n",
      "Epoch 1 Step 3/8 Loss: 3.7762\n",
      "Epoch 1 Step 4/8 Loss: 1.8996\n",
      "Epoch 1 Step 5/8 Loss: 2.1205\n",
      "Epoch 1 Step 6/8 Loss: 1.7075\n",
      "Epoch 1 Step 7/8 Loss: 4.8433\n",
      "Epoch 1 Step 8/8 Loss: 2.3802\n",
      "Epoch 1 Step 1/8 Loss: 3.9584\n",
      "Epoch 1 Step 2/8 Loss: 2.5542\n",
      "Epoch 1 Step 3/8 Loss: 4.9063\n",
      "Epoch 1 Step 4/8 Loss: 1.1163\n",
      "Epoch 1 Step 5/8 Loss: 1.9367\n",
      "Epoch 1 Step 6/8 Loss: 1.4453\n",
      "Epoch 1 Step 7/8 Loss: 3.1570\n",
      "Epoch 1 Step 8/8 Loss: 2.2050\n",
      "Epoch 1 Step 1/8 Loss: 3.7327\n",
      "Epoch 1 Step 2/8 Loss: 2.6502\n",
      "Epoch 1 Step 3/8 Loss: 2.4231\n",
      "Epoch 1 Step 4/8 Loss: 4.2224\n",
      "Epoch 1 Step 5/8 Loss: 1.7039\n",
      "Epoch 1 Step 6/8 Loss: 2.5098\n",
      "Epoch 1 Step 7/8 Loss: 2.6323\n",
      "Epoch 1 Step 8/8 Loss: 2.8699\n",
      "Epoch 1 Step 1/8 Loss: 3.6450\n",
      "Epoch 1 Step 2/8 Loss: 2.6027\n",
      "Epoch 1 Step 3/8 Loss: 2.8989\n",
      "Epoch 1 Step 4/8 Loss: 2.3339\n",
      "Epoch 1 Step 5/8 Loss: 2.1972\n",
      "Epoch 1 Step 6/8 Loss: 2.6523\n",
      "Epoch 1 Step 7/8 Loss: 1.9764\n",
      "Epoch 1 Step 8/8 Loss: 4.1168\n",
      "Epoch 1 Step 1/8 Loss: 2.8232\n",
      "Epoch 1 Step 2/8 Loss: 5.1487\n",
      "Epoch 1 Step 3/8 Loss: 1.3267\n",
      "Epoch 1 Step 4/8 Loss: 1.4277\n",
      "Epoch 1 Step 5/8 Loss: 3.0051\n",
      "Epoch 1 Step 6/8 Loss: 2.6129\n",
      "Epoch 1 Step 7/8 Loss: 2.7468\n",
      "Epoch 1 Step 8/8 Loss: 2.5367\n",
      "Epoch 1 Step 1/8 Loss: 1.1344\n",
      "Epoch 1 Step 2/8 Loss: 3.4604\n",
      "Epoch 1 Step 3/8 Loss: 2.3983\n",
      "Epoch 1 Step 4/8 Loss: 3.3666\n",
      "Epoch 1 Step 5/8 Loss: 2.6125\n",
      "Epoch 1 Step 6/8 Loss: 4.4523\n",
      "Epoch 1 Step 7/8 Loss: 1.7532\n",
      "Epoch 1 Step 8/8 Loss: 1.5857\n",
      "Epoch 1 Step 1/8 Loss: 1.9063\n",
      "Epoch 1 Step 2/8 Loss: 1.5046\n",
      "Epoch 1 Step 3/8 Loss: 2.4141\n",
      "Epoch 1 Step 4/8 Loss: 1.8157\n",
      "Epoch 1 Step 5/8 Loss: 2.9494\n",
      "Epoch 1 Step 6/8 Loss: 3.1212\n",
      "Epoch 1 Step 7/8 Loss: 2.7802\n",
      "Epoch 1 Step 8/8 Loss: 3.2048\n",
      "Epoch 1 Step 1/8 Loss: 2.1785\n",
      "Epoch 1 Step 2/8 Loss: 3.1866\n",
      "Epoch 1 Step 3/8 Loss: 3.4798\n",
      "Epoch 1 Step 4/8 Loss: 2.8332\n",
      "Epoch 1 Step 5/8 Loss: 3.5599\n",
      "Epoch 1 Step 6/8 Loss: 2.3258\n",
      "Epoch 1 Step 7/8 Loss: 1.4624\n",
      "Epoch 1 Step 8/8 Loss: 2.4320\n",
      "Epoch 1 Step 1/8 Loss: 1.0984\n",
      "Epoch 1 Step 2/8 Loss: 2.3323\n",
      "Epoch 1 Step 3/8 Loss: 2.4956\n",
      "Epoch 1 Step 4/8 Loss: 3.9694\n",
      "Epoch 1 Step 5/8 Loss: 3.4912\n",
      "Epoch 1 Step 6/8 Loss: 2.0286\n",
      "Epoch 1 Step 7/8 Loss: 2.3153\n",
      "Epoch 1 Step 8/8 Loss: 2.7171\n",
      "Epoch 1 Step 1/8 Loss: 2.6573\n",
      "Epoch 1 Step 2/8 Loss: 1.5544\n",
      "Epoch 1 Step 3/8 Loss: 1.9212\n",
      "Epoch 1 Step 4/8 Loss: 2.9295\n",
      "Epoch 1 Step 5/8 Loss: 2.9355\n",
      "Epoch 1 Step 6/8 Loss: 3.2529\n",
      "Epoch 1 Step 7/8 Loss: 3.2533\n",
      "Epoch 1 Step 8/8 Loss: 1.7680\n",
      "Epoch 1 Step 1/8 Loss: 2.8335\n",
      "Epoch 1 Step 2/8 Loss: 3.8401\n",
      "Epoch 1 Step 3/8 Loss: 3.1894\n",
      "Epoch 1 Step 4/8 Loss: 0.9534\n",
      "Epoch 1 Step 5/8 Loss: 2.5272\n",
      "Epoch 1 Step 6/8 Loss: 2.7309\n",
      "Epoch 1 Step 7/8 Loss: 3.3362\n",
      "Epoch 1 Step 8/8 Loss: 3.0845\n",
      "Epoch 1 Step 1/8 Loss: 3.3840\n",
      "Epoch 1 Step 2/8 Loss: 2.9399\n",
      "Epoch 1 Step 3/8 Loss: 2.7030\n",
      "Epoch 1 Step 4/8 Loss: 2.9930\n",
      "Epoch 1 Step 5/8 Loss: 2.6557\n",
      "Epoch 1 Step 6/8 Loss: 1.6118\n",
      "Epoch 1 Step 7/8 Loss: 1.9747\n",
      "Epoch 1 Step 8/8 Loss: 4.0054\n",
      "Epoch 1 completed. Average Loss: nan\n",
      "Epoch 2 Step 1/8 Loss: 3.2922\n",
      "Epoch 2 Step 2/8 Loss: 3.1557\n",
      "Epoch 2 Step 3/8 Loss: 3.1953\n",
      "Epoch 2 Step 4/8 Loss: 2.3098\n",
      "Epoch 2 Step 5/8 Loss: 2.1168\n",
      "Epoch 2 Step 6/8 Loss: 3.4999\n",
      "Epoch 2 Step 7/8 Loss: 3.2902\n",
      "Epoch 2 Step 8/8 Loss: 2.4492\n",
      "Epoch 2 Step 1/8 Loss: 2.6272\n",
      "Epoch 2 Step 2/8 Loss: 0.9069\n",
      "Epoch 2 Step 3/8 Loss: 2.4867\n",
      "Epoch 2 Step 4/8 Loss: 3.0026\n",
      "Epoch 2 Step 5/8 Loss: 1.8449\n",
      "Epoch 2 Step 6/8 Loss: 1.8527\n",
      "Epoch 2 Step 7/8 Loss: 1.9538\n",
      "Epoch 2 Step 8/8 Loss: 1.0703\n",
      "Epoch 2 Step 1/8 Loss: 2.0215\n",
      "Epoch 2 Step 2/8 Loss: 2.6244\n",
      "Epoch 2 Step 3/8 Loss: 0.8797\n",
      "Epoch 2 Step 4/8 Loss: 2.5062\n",
      "Epoch 2 Step 5/8 Loss: 2.0286\n",
      "Epoch 2 Step 6/8 Loss: 2.5944\n",
      "Epoch 2 Step 7/8 Loss: 3.4297\n",
      "Epoch 2 Step 8/8 Loss: 2.8044\n",
      "Epoch 2 Step 1/8 Loss: 2.1009\n",
      "Epoch 2 Step 2/8 Loss: 1.1715\n",
      "Epoch 2 Step 3/8 Loss: 2.2347\n",
      "Epoch 2 Step 4/8 Loss: 1.6983\n",
      "Epoch 2 Step 5/8 Loss: 2.9496\n",
      "Epoch 2 Step 6/8 Loss: 4.9705\n",
      "Epoch 2 Step 7/8 Loss: 2.8605\n",
      "Epoch 2 Step 8/8 Loss: 2.6609\n",
      "Epoch 2 Step 1/8 Loss: 3.0479\n",
      "Epoch 2 Step 2/8 Loss: 1.1970\n",
      "Epoch 2 Step 3/8 Loss: 2.4802\n",
      "Epoch 2 Step 4/8 Loss: 2.9401\n",
      "Epoch 2 Step 5/8 Loss: 1.9128\n",
      "Epoch 2 Step 6/8 Loss: 3.8082\n",
      "Epoch 2 Step 7/8 Loss: 1.7906\n",
      "Epoch 2 Step 8/8 Loss: 2.8878\n",
      "Epoch 2 Step 1/8 Loss: 3.1745\n",
      "Epoch 2 Step 2/8 Loss: 3.3254\n",
      "Epoch 2 Step 3/8 Loss: 1.2564\n",
      "Epoch 2 Step 4/8 Loss: 2.4912\n",
      "Epoch 2 Step 5/8 Loss: 1.9992\n",
      "Epoch 2 Step 6/8 Loss: 2.1269\n",
      "Epoch 2 Step 7/8 Loss: 2.7508\n",
      "Epoch 2 Step 8/8 Loss: 4.5554\n",
      "Epoch 2 Step 1/8 Loss: 1.3006\n",
      "Epoch 2 Step 2/8 Loss: 2.6090\n",
      "Epoch 2 Step 3/8 Loss: 1.5628\n",
      "Epoch 2 Step 4/8 Loss: 4.2924\n",
      "Epoch 2 Step 5/8 Loss: 2.8832\n",
      "Epoch 2 Step 6/8 Loss: 4.3231\n",
      "Epoch 2 Step 7/8 Loss: 4.4261\n",
      "Epoch 2 Step 8/8 Loss: 2.1135\n",
      "Epoch 2 Step 1/8 Loss: 3.3783\n",
      "Epoch 2 Step 2/8 Loss: 1.6627\n",
      "Epoch 2 Step 3/8 Loss: 1.3898\n",
      "Epoch 2 Step 4/8 Loss: 3.4922\n",
      "Epoch 2 Step 5/8 Loss: 2.9466\n",
      "Epoch 2 Step 6/8 Loss: 1.6635\n",
      "Epoch 2 Step 7/8 Loss: 4.0390\n",
      "Epoch 2 Step 8/8 Loss: 3.0881\n",
      "Epoch 2 Step 1/8 Loss: 2.0958\n",
      "Epoch 2 Step 2/8 Loss: 2.0811\n",
      "Epoch 2 Step 3/8 Loss: 4.6867\n",
      "Epoch 2 Step 4/8 Loss: 1.3371\n",
      "Epoch 2 Step 5/8 Loss: 4.4251\n",
      "Epoch 2 Step 6/8 Loss: 2.4960\n",
      "Epoch 2 Step 7/8 Loss: 2.8881\n",
      "Epoch 2 Step 8/8 Loss: 3.7981\n",
      "Epoch 2 Step 1/8 Loss: 2.1585\n",
      "Epoch 2 Step 2/8 Loss: 2.8556\n",
      "Epoch 2 Step 3/8 Loss: 2.0785\n",
      "Epoch 2 Step 4/8 Loss: 1.8002\n",
      "Epoch 2 Step 5/8 Loss: 2.9928\n",
      "Epoch 2 Step 6/8 Loss: 1.4657\n",
      "Epoch 2 Step 7/8 Loss: 1.9780\n",
      "Epoch 2 Step 8/8 Loss: 4.1049\n",
      "Epoch 2 Step 1/8 Loss: 1.8586\n",
      "Epoch 2 Step 2/8 Loss: 2.5587\n",
      "Epoch 2 Step 3/8 Loss: 1.4642\n",
      "Epoch 2 Step 4/8 Loss: 3.0645\n",
      "Epoch 2 Step 5/8 Loss: 3.1136\n",
      "Epoch 2 Step 6/8 Loss: 4.4957\n",
      "Epoch 2 Step 7/8 Loss: 2.1258\n",
      "Epoch 2 Step 8/8 Loss: 2.9545\n",
      "Epoch 2 Step 1/8 Loss: 3.5837\n",
      "Epoch 2 Step 2/8 Loss: 2.7458\n",
      "Epoch 2 Step 3/8 Loss: 3.7734\n",
      "Epoch 2 Step 4/8 Loss: 1.4661\n",
      "Epoch 2 Step 5/8 Loss: 2.0552\n",
      "Epoch 2 Step 6/8 Loss: 2.2013\n",
      "Epoch 2 Step 7/8 Loss: 2.1013\n",
      "Epoch 2 Step 8/8 Loss: 4.0549\n",
      "Epoch 2 Step 1/8 Loss: 1.2858\n",
      "Epoch 2 Step 2/8 Loss: 3.2786\n",
      "Epoch 2 Step 3/8 Loss: 2.7233\n",
      "Epoch 2 Step 4/8 Loss: 2.1324\n",
      "Epoch 2 Step 5/8 Loss: 3.7885\n",
      "Epoch 2 Step 6/8 Loss: 2.3480\n",
      "Epoch 2 Step 7/8 Loss: 1.1912\n",
      "Epoch 2 Step 8/8 Loss: 3.3338\n",
      "Epoch 2 Step 1/8 Loss: 1.3651\n",
      "Epoch 2 Step 2/8 Loss: 3.0117\n",
      "Epoch 2 Step 3/8 Loss: 2.2935\n",
      "Epoch 2 Step 4/8 Loss: 3.1197\n",
      "Epoch 2 Step 5/8 Loss: 2.5190\n",
      "Epoch 2 Step 6/8 Loss: 3.7742\n",
      "Epoch 2 Step 7/8 Loss: 1.7253\n",
      "Epoch 2 Step 8/8 Loss: 2.1020\n",
      "Epoch 2 Step 1/8 Loss: 2.1033\n",
      "Epoch 2 Step 2/8 Loss: 3.6856\n",
      "Epoch 2 Step 3/8 Loss: 2.5987\n",
      "Epoch 2 Step 4/8 Loss: 0.9298\n",
      "Epoch 2 Step 5/8 Loss: 1.5911\n",
      "Epoch 2 Step 6/8 Loss: 2.6339\n",
      "Epoch 2 Step 7/8 Loss: 2.8414\n",
      "Epoch 2 Step 8/8 Loss: 3.2180\n",
      "Epoch 2 Step 1/8 Loss: 3.2814\n",
      "Epoch 2 Step 2/8 Loss: 3.2579\n",
      "Epoch 2 Step 3/8 Loss: 1.0838\n",
      "Epoch 2 Step 4/8 Loss: 3.2911\n",
      "Epoch 2 Step 5/8 Loss: 2.8794\n",
      "Epoch 2 Step 6/8 Loss: 1.9715\n",
      "Epoch 2 Step 7/8 Loss: 2.6700\n",
      "Epoch 2 Step 8/8 Loss: 1.1208\n",
      "Epoch 2 Step 1/8 Loss: 1.5400\n",
      "Epoch 2 Step 2/8 Loss: 2.9716\n",
      "Epoch 2 Step 3/8 Loss: 2.5932\n",
      "Epoch 2 Step 4/8 Loss: 2.8730\n",
      "Epoch 2 Step 5/8 Loss: 2.3526\n",
      "Epoch 2 Step 6/8 Loss: 1.5255\n",
      "Epoch 2 Step 7/8 Loss: 2.3660\n",
      "Epoch 2 Step 8/8 Loss: 2.2572\n",
      "Epoch 2 Step 1/8 Loss: 2.5911\n",
      "Epoch 2 Step 2/8 Loss: 2.2972\n",
      "Epoch 2 Step 3/8 Loss: 1.1827\n",
      "Epoch 2 Step 4/8 Loss: 2.5224\n",
      "Epoch 2 Step 5/8 Loss: 2.4404\n",
      "Epoch 2 Step 6/8 Loss: 5.2048\n",
      "Epoch 2 Step 7/8 Loss: 2.9693\n",
      "Epoch 2 Step 8/8 Loss: 0.9222\n",
      "Epoch 2 Step 1/8 Loss: 2.6818\n",
      "Epoch 2 Step 2/8 Loss: 2.2507\n",
      "Epoch 2 Step 3/8 Loss: 1.3129\n",
      "Epoch 2 Step 4/8 Loss: 3.1837\n",
      "Epoch 2 Step 5/8 Loss: 2.4725\n",
      "Epoch 2 Step 6/8 Loss: 2.4020\n",
      "Epoch 2 Step 7/8 Loss: 2.1157\n",
      "Epoch 2 Step 8/8 Loss: 2.4736\n",
      "Epoch 2 Step 1/8 Loss: 2.8107\n",
      "Epoch 2 Step 2/8 Loss: 4.3652\n",
      "Epoch 2 Step 3/8 Loss: 4.7630\n",
      "Epoch 2 Step 4/8 Loss: 2.7550\n",
      "Epoch 2 Step 5/8 Loss: 2.6440\n",
      "Epoch 2 Step 6/8 Loss: 2.1791\n",
      "Epoch 2 Step 7/8 Loss: 1.4696\n",
      "Epoch 2 Step 8/8 Loss: 1.2664\n",
      "Epoch 2 Step 1/8 Loss: 3.5073\n",
      "Epoch 2 Step 2/8 Loss: 2.3839\n",
      "Epoch 2 Step 3/8 Loss: 1.4794\n",
      "Epoch 2 Step 4/8 Loss: 3.0860\n",
      "Epoch 2 Step 5/8 Loss: 5.2729\n",
      "Epoch 2 Step 6/8 Loss: 3.5882\n",
      "Epoch 2 Step 7/8 Loss: 2.7842\n",
      "Epoch 2 Step 8/8 Loss: 2.5686\n",
      "Epoch 2 Step 1/8 Loss: 2.0052\n",
      "Epoch 2 Step 2/8 Loss: 3.0412\n",
      "Epoch 2 Step 3/8 Loss: 2.9432\n",
      "Epoch 2 Step 4/8 Loss: 2.6950\n",
      "Epoch 2 Step 5/8 Loss: 2.6030\n",
      "Epoch 2 Step 6/8 Loss: 2.1225\n",
      "Epoch 2 Step 7/8 Loss: 2.5221\n",
      "Epoch 2 Step 8/8 Loss: 2.5424\n",
      "Epoch 2 Step 1/8 Loss: 3.7346\n",
      "Epoch 2 Step 2/8 Loss: 2.4931\n",
      "Epoch 2 Step 3/8 Loss: 2.0667\n",
      "Epoch 2 Step 4/8 Loss: 1.4428\n",
      "Epoch 2 Step 5/8 Loss: 3.3952\n",
      "Epoch 2 Step 6/8 Loss: 4.4392\n",
      "Epoch 2 Step 7/8 Loss: 2.2445\n",
      "Epoch 2 Step 8/8 Loss: 4.1080\n",
      "Epoch 2 Step 1/8 Loss: 2.4096\n",
      "Epoch 2 Step 2/8 Loss: 1.7601\n",
      "Epoch 2 Step 3/8 Loss: 2.9472\n",
      "Epoch 2 Step 4/8 Loss: 3.9617\n",
      "Epoch 2 Step 5/8 Loss: 2.3171\n",
      "Epoch 2 Step 6/8 Loss: 1.6006\n",
      "Epoch 2 Step 7/8 Loss: 1.9179\n",
      "Epoch 2 Step 8/8 Loss: 1.1161\n",
      "Epoch 2 Step 1/8 Loss: 1.2147\n",
      "Epoch 2 Step 2/8 Loss: 1.5705\n",
      "Epoch 2 Step 3/8 Loss: 3.1189\n",
      "Epoch 2 Step 4/8 Loss: 2.1615\n",
      "Epoch 2 Step 5/8 Loss: 2.2220\n",
      "Epoch 2 Step 6/8 Loss: 3.0083\n",
      "Epoch 2 Step 7/8 Loss: 4.1792\n",
      "Epoch 2 Step 8/8 Loss: 2.7153\n",
      "Epoch 2 Step 1/8 Loss: 2.3695\n",
      "Epoch 2 Step 2/8 Loss: 2.2949\n",
      "Epoch 2 Step 3/8 Loss: 2.8265\n",
      "Epoch 2 Step 4/8 Loss: 2.7958\n",
      "Epoch 2 Step 5/8 Loss: 2.4095\n",
      "Epoch 2 Step 6/8 Loss: 1.3393\n",
      "Epoch 2 Step 7/8 Loss: 2.2662\n",
      "Epoch 2 Step 8/8 Loss: 2.5972\n",
      "Epoch 2 Step 1/8 Loss: 1.0859\n",
      "Epoch 2 Step 2/8 Loss: 1.5677\n",
      "Epoch 2 Step 3/8 Loss: 1.2157\n",
      "Epoch 2 Step 4/8 Loss: 2.6035\n",
      "Epoch 2 Step 5/8 Loss: 2.5697\n",
      "Epoch 2 Step 6/8 Loss: 3.8193\n",
      "Epoch 2 Step 7/8 Loss: 3.6476\n",
      "Epoch 2 Step 8/8 Loss: 4.2500\n",
      "Epoch 2 Step 1/8 Loss: 3.3718\n",
      "Epoch 2 Step 2/8 Loss: 4.1539\n",
      "Epoch 2 Step 3/8 Loss: 2.9723\n",
      "Epoch 2 Step 4/8 Loss: 2.9184\n",
      "Epoch 2 Step 5/8 Loss: 2.5074\n",
      "Epoch 2 Step 6/8 Loss: 1.3347\n",
      "Epoch 2 Step 7/8 Loss: 2.1051\n",
      "Epoch 2 Step 8/8 Loss: 2.0746\n",
      "Epoch 2 Step 1/8 Loss: 2.8745\n",
      "Epoch 2 Step 2/8 Loss: 3.9957\n",
      "Epoch 2 Step 3/8 Loss: 2.7525\n",
      "Epoch 2 Step 4/8 Loss: 2.1629\n",
      "Epoch 2 Step 5/8 Loss: 1.4130\n",
      "Epoch 2 Step 6/8 Loss: 2.1695\n",
      "Epoch 2 Step 7/8 Loss: 3.1669\n",
      "Epoch 2 Step 8/8 Loss: 2.9491\n",
      "Epoch 2 Step 1/8 Loss: 3.4272\n",
      "Epoch 2 Step 2/8 Loss: 1.8843\n",
      "Epoch 2 Step 3/8 Loss: 2.2765\n",
      "Epoch 2 Step 4/8 Loss: 2.3512\n",
      "Epoch 2 Step 5/8 Loss: 1.1844\n",
      "Epoch 2 Step 6/8 Loss: 2.3139\n",
      "Epoch 2 Step 7/8 Loss: 2.0551\n",
      "Epoch 2 Step 8/8 Loss: 3.1196\n",
      "Epoch 2 Step 1/8 Loss: 2.1060\n",
      "Epoch 2 Step 2/8 Loss: 2.4640\n",
      "Epoch 2 Step 3/8 Loss: 2.7525\n",
      "Epoch 2 Step 4/8 Loss: 2.8975\n",
      "Epoch 2 Step 5/8 Loss: 4.8826\n",
      "Epoch 2 Step 6/8 Loss: 3.2802\n",
      "Epoch 2 Step 7/8 Loss: 1.2336\n",
      "Epoch 2 Step 8/8 Loss: 1.2775\n",
      "Epoch 2 Step 1/8 Loss: 3.8122\n",
      "Epoch 2 Step 2/8 Loss: 2.4980\n",
      "Epoch 2 Step 3/8 Loss: 2.6162\n",
      "Epoch 2 Step 4/8 Loss: 2.2338\n",
      "Epoch 2 Step 5/8 Loss: 1.2077\n",
      "Epoch 2 Step 6/8 Loss: 3.0588\n",
      "Epoch 2 Step 7/8 Loss: 1.8520\n",
      "Epoch 2 Step 8/8 Loss: 3.3675\n",
      "Epoch 2 Step 1/8 Loss: 2.7878\n",
      "Epoch 2 Step 2/8 Loss: 3.4112\n",
      "Epoch 2 Step 3/8 Loss: 2.3895\n",
      "Epoch 2 Step 4/8 Loss: 1.0219\n",
      "Epoch 2 Step 5/8 Loss: 1.2096\n",
      "Epoch 2 Step 6/8 Loss: 4.2726\n",
      "Epoch 2 Step 7/8 Loss: 2.5662\n",
      "Epoch 2 Step 8/8 Loss: 4.1156\n",
      "Epoch 2 Step 1/8 Loss: 1.3313\n",
      "Epoch 2 Step 2/8 Loss: 2.8062\n",
      "Epoch 2 Step 3/8 Loss: 2.3557\n",
      "Epoch 2 Step 4/8 Loss: 3.2546\n",
      "Epoch 2 Step 5/8 Loss: 3.2174\n",
      "Epoch 2 Step 6/8 Loss: 1.5377\n",
      "Epoch 2 Step 7/8 Loss: 2.6576\n",
      "Epoch 2 Step 8/8 Loss: 2.2228\n",
      "Epoch 2 Step 1/8 Loss: 1.8465\n",
      "Epoch 2 Step 2/8 Loss: 4.1633\n",
      "Epoch 2 Step 3/8 Loss: 1.1363\n",
      "Epoch 2 Step 4/8 Loss: 2.8363\n",
      "Epoch 2 Step 5/8 Loss: 3.5278\n",
      "Epoch 2 Step 6/8 Loss: 3.4809\n",
      "Epoch 2 Step 7/8 Loss: 2.5995\n",
      "Epoch 2 Step 8/8 Loss: 1.2079\n",
      "Epoch 2 Step 1/8 Loss: 3.2642\n",
      "Epoch 2 Step 2/8 Loss: 2.6336\n",
      "Epoch 2 Step 3/8 Loss: 2.7480\n",
      "Epoch 2 Step 4/8 Loss: 3.2035\n",
      "Epoch 2 Step 5/8 Loss: 1.3219\n",
      "Epoch 2 Step 6/8 Loss: 2.0303\n",
      "Epoch 2 Step 7/8 Loss: 4.2227\n",
      "Epoch 2 Step 8/8 Loss: 1.4076\n",
      "Epoch 2 Step 1/8 Loss: 1.2729\n",
      "Epoch 2 Step 2/8 Loss: 2.9232\n",
      "Epoch 2 Step 3/8 Loss: 1.2797\n",
      "Epoch 2 Step 4/8 Loss: 2.6744\n",
      "Epoch 2 Step 5/8 Loss: 3.3904\n",
      "Epoch 2 Step 6/8 Loss: 3.2017\n",
      "Epoch 2 Step 7/8 Loss: 1.8619\n",
      "Epoch 2 Step 8/8 Loss: 2.8957\n",
      "Epoch 2 Step 1/8 Loss: 3.1458\n",
      "Epoch 2 Step 2/8 Loss: 2.9819\n",
      "Epoch 2 Step 3/8 Loss: 2.4782\n",
      "Epoch 2 Step 4/8 Loss: 2.4993\n",
      "Epoch 2 Step 5/8 Loss: 3.1913\n",
      "Epoch 2 Step 6/8 Loss: 1.0995\n",
      "Epoch 2 Step 7/8 Loss: 3.0294\n",
      "Epoch 2 Step 8/8 Loss: 2.8809\n",
      "Epoch 2 Step 1/8 Loss: 2.4795\n",
      "Epoch 2 Step 2/8 Loss: 2.3058\n",
      "Epoch 2 Step 3/8 Loss: 1.3806\n",
      "Epoch 2 Step 4/8 Loss: 1.0607\n",
      "Epoch 2 Step 5/8 Loss: 1.9330\n",
      "Epoch 2 Step 6/8 Loss: 1.4308\n",
      "Epoch 2 Step 7/8 Loss: 4.3056\n",
      "Epoch 2 Step 8/8 Loss: 3.5021\n",
      "Epoch 2 Step 1/8 Loss: 2.6650\n",
      "Epoch 2 Step 2/8 Loss: 1.2822\n",
      "Epoch 2 Step 3/8 Loss: 1.2693\n",
      "Epoch 2 Step 4/8 Loss: 3.1609\n",
      "Epoch 2 Step 5/8 Loss: 2.0837\n",
      "Epoch 2 Step 6/8 Loss: 3.8104\n",
      "Epoch 2 Step 7/8 Loss: 2.4052\n",
      "Epoch 2 Step 8/8 Loss: 3.2248\n",
      "Epoch 2 Step 1/8 Loss: 2.6529\n",
      "Epoch 2 Step 2/8 Loss: 4.2504\n",
      "Epoch 2 Step 3/8 Loss: 1.9927\n",
      "Epoch 2 Step 4/8 Loss: 2.7230\n",
      "Epoch 2 Step 5/8 Loss: 2.1172\n",
      "Epoch 2 Step 6/8 Loss: 2.2027\n",
      "Epoch 2 Step 7/8 Loss: 2.6204\n",
      "Epoch 2 Step 8/8 Loss: 1.8636\n",
      "Epoch 2 Step 1/8 Loss: 1.6109\n",
      "Epoch 2 Step 2/8 Loss: 0.9084\n",
      "Epoch 2 Step 3/8 Loss: 4.2585\n",
      "Epoch 2 Step 4/8 Loss: 4.2580\n",
      "Epoch 2 Step 5/8 Loss: 2.6179\n",
      "Epoch 2 Step 6/8 Loss: 1.2983\n",
      "Epoch 2 Step 7/8 Loss: 3.0816\n",
      "Epoch 2 Step 8/8 Loss: 2.0292\n",
      "Epoch 2 Step 1/8 Loss: 2.4686\n",
      "Epoch 2 Step 2/8 Loss: 1.6382\n",
      "Epoch 2 Step 3/8 Loss: 3.1377\n",
      "Epoch 2 Step 4/8 Loss: 1.7527\n",
      "Epoch 2 Step 5/8 Loss: 3.2213\n",
      "Epoch 2 Step 6/8 Loss: 2.6149\n",
      "Epoch 2 Step 7/8 Loss: 3.6424\n",
      "Epoch 2 Step 8/8 Loss: 0.9255\n",
      "Epoch 2 Step 1/8 Loss: 3.7776\n",
      "Epoch 2 Step 2/8 Loss: 2.4736\n",
      "Epoch 2 Step 3/8 Loss: 2.8392\n",
      "Epoch 2 Step 4/8 Loss: 3.8743\n",
      "Epoch 2 Step 5/8 Loss: 1.1275\n",
      "Epoch 2 Step 6/8 Loss: 2.4498\n",
      "Epoch 2 Step 7/8 Loss: 2.5660\n",
      "Epoch 2 Step 8/8 Loss: 2.4026\n",
      "Epoch 2 Step 1/8 Loss: 2.7902\n",
      "Epoch 2 Step 2/8 Loss: 2.0858\n",
      "Epoch 2 Step 3/8 Loss: 2.3276\n",
      "Epoch 2 Step 4/8 Loss: 2.7203\n",
      "Epoch 2 Step 5/8 Loss: 1.4267\n",
      "Epoch 2 Step 6/8 Loss: 5.3172\n",
      "Epoch 2 Step 7/8 Loss: 3.7549\n",
      "Epoch 2 Step 8/8 Loss: 3.1803\n",
      "Epoch 2 Step 1/8 Loss: 2.2938\n",
      "Epoch 2 Step 2/8 Loss: 2.3380\n",
      "Epoch 2 Step 3/8 Loss: 3.0486\n",
      "Epoch 2 Step 4/8 Loss: 1.2340\n",
      "Epoch 2 Step 5/8 Loss: 2.5772\n",
      "Epoch 2 Step 6/8 Loss: 3.0724\n",
      "Epoch 2 Step 7/8 Loss: 4.5971\n",
      "Epoch 2 Step 8/8 Loss: 2.3491\n",
      "Epoch 2 Step 1/8 Loss: 2.7631\n",
      "Epoch 2 Step 2/8 Loss: 1.2018\n",
      "Epoch 2 Step 3/8 Loss: 4.2720\n",
      "Epoch 2 Step 4/8 Loss: 2.1709\n",
      "Epoch 2 Step 5/8 Loss: 1.7214\n",
      "Epoch 2 Step 6/8 Loss: 1.2671\n",
      "Epoch 2 Step 7/8 Loss: 3.2236\n",
      "Epoch 2 Step 8/8 Loss: 2.8881\n",
      "Epoch 2 Step 1/8 Loss: 2.5913\n",
      "Epoch 2 Step 2/8 Loss: 1.9593\n",
      "Epoch 2 Step 3/8 Loss: 5.0631\n",
      "Epoch 2 Step 4/8 Loss: 1.9088\n",
      "Epoch 2 Step 5/8 Loss: 1.0581\n",
      "Epoch 2 Step 6/8 Loss: 2.9830\n",
      "Epoch 2 Step 7/8 Loss: 2.8250\n",
      "Epoch 2 Step 8/8 Loss: 4.1729\n",
      "Epoch 2 Step 1/8 Loss: 2.7328\n",
      "Epoch 2 Step 2/8 Loss: 1.2491\n",
      "Epoch 2 Step 3/8 Loss: 4.4324\n",
      "Epoch 2 Step 4/8 Loss: 2.7074\n",
      "Epoch 2 Step 5/8 Loss: 2.4472\n",
      "Epoch 2 Step 6/8 Loss: 3.8348\n",
      "Epoch 2 Step 7/8 Loss: 3.3874\n",
      "Epoch 2 Step 8/8 Loss: 2.0638\n",
      "Epoch 2 Step 1/8 Loss: 3.5611\n",
      "Epoch 2 Step 2/8 Loss: 3.4431\n",
      "Epoch 2 Step 3/8 Loss: 2.0061\n",
      "Epoch 2 Step 4/8 Loss: 2.7455\n",
      "Epoch 2 Step 5/8 Loss: 2.6211\n",
      "Epoch 2 Step 6/8 Loss: 2.4053\n",
      "Epoch 2 Step 7/8 Loss: 2.8716\n",
      "Epoch 2 Step 8/8 Loss: 2.1142\n",
      "Epoch 2 Step 1/8 Loss: 2.3984\n",
      "Epoch 2 Step 2/8 Loss: 2.2258\n",
      "Epoch 2 Step 3/8 Loss: 4.1942\n",
      "Epoch 2 Step 4/8 Loss: 2.5971\n",
      "Epoch 2 Step 5/8 Loss: 1.1347\n",
      "Epoch 2 Step 6/8 Loss: 2.2472\n",
      "Epoch 2 Step 7/8 Loss: 2.1051\n",
      "Epoch 2 Step 8/8 Loss: 1.2773\n",
      "Epoch 2 Step 1/8 Loss: 2.0151\n",
      "Epoch 2 Step 2/8 Loss: 3.9328\n",
      "Epoch 2 Step 3/8 Loss: 3.4410\n",
      "Epoch 2 Step 4/8 Loss: 2.4757\n",
      "Epoch 2 Step 5/8 Loss: 2.2975\n",
      "Epoch 2 Step 6/8 Loss: 2.3763\n",
      "Epoch 2 Step 7/8 Loss: 2.0683\n",
      "Epoch 2 Step 8/8 Loss: 3.3447\n",
      "Epoch 2 Step 1/8 Loss: 2.9627\n",
      "Epoch 2 Step 2/8 Loss: 3.8998\n",
      "Epoch 2 Step 3/8 Loss: 2.8383\n",
      "Epoch 2 Step 4/8 Loss: 1.1269\n",
      "Epoch 2 Step 5/8 Loss: 2.8242\n",
      "Epoch 2 Step 6/8 Loss: 1.7306\n",
      "Epoch 2 Step 7/8 Loss: 1.9636\n",
      "Epoch 2 Step 8/8 Loss: 3.2095\n",
      "Epoch 2 Step 1/8 Loss: 4.3644\n",
      "Epoch 2 Step 2/8 Loss: 2.1966\n",
      "Epoch 2 Step 3/8 Loss: 4.1046\n",
      "Epoch 2 Step 4/8 Loss: 3.9580\n",
      "Epoch 2 Step 5/8 Loss: 3.0690\n",
      "Epoch 2 Step 6/8 Loss: 2.4665\n",
      "Epoch 2 Step 7/8 Loss: 3.8967\n",
      "Epoch 2 Step 8/8 Loss: 1.5116\n",
      "Epoch 2 Step 1/8 Loss: 2.8950\n",
      "Epoch 2 Step 2/8 Loss: 5.5040\n",
      "Epoch 2 Step 3/8 Loss: 2.1958\n",
      "Epoch 2 Step 4/8 Loss: 3.4586\n",
      "Epoch 2 Step 5/8 Loss: 2.2421\n",
      "Epoch 2 Step 6/8 Loss: 1.4856\n",
      "Epoch 2 Step 7/8 Loss: 2.6786\n",
      "Epoch 2 Step 8/8 Loss: 2.1840\n",
      "Epoch 2 Step 1/8 Loss: 3.7027\n",
      "Epoch 2 Step 2/8 Loss: 2.4793\n",
      "Epoch 2 Step 3/8 Loss: 1.5830\n",
      "Epoch 2 Step 4/8 Loss: 0.9300\n",
      "Epoch 2 Step 5/8 Loss: 1.5299\n",
      "Epoch 2 Step 6/8 Loss: 2.1478\n",
      "Epoch 2 Step 7/8 Loss: 3.1690\n",
      "Epoch 2 Step 8/8 Loss: 2.1135\n",
      "Epoch 2 Step 1/8 Loss: 2.6363\n",
      "Epoch 2 Step 2/8 Loss: 5.8710\n",
      "Epoch 2 Step 3/8 Loss: 1.5094\n",
      "Epoch 2 Step 4/8 Loss: 2.2870\n",
      "Epoch 2 Step 5/8 Loss: 2.1197\n",
      "Epoch 2 Step 6/8 Loss: 2.8835\n",
      "Epoch 2 Step 7/8 Loss: 3.0999\n",
      "Epoch 2 Step 8/8 Loss: 2.9741\n",
      "Epoch 2 Step 1/8 Loss: 2.3293\n",
      "Epoch 2 Step 2/8 Loss: 2.4585\n",
      "Epoch 2 Step 3/8 Loss: 2.3633\n",
      "Epoch 2 Step 4/8 Loss: 2.9139\n",
      "Epoch 2 Step 5/8 Loss: 2.7070\n",
      "Epoch 2 Step 6/8 Loss: 1.5451\n",
      "Epoch 2 Step 7/8 Loss: 2.2948\n",
      "Epoch 2 Step 8/8 Loss: 1.0698\n",
      "Epoch 2 Step 1/8 Loss: 2.9315\n",
      "Epoch 2 Step 2/8 Loss: 4.4363\n",
      "Epoch 2 Step 3/8 Loss: 2.1034\n",
      "Epoch 2 Step 4/8 Loss: 3.1332\n",
      "Epoch 2 Step 5/8 Loss: 2.3130\n",
      "Epoch 2 Step 6/8 Loss: 2.7420\n",
      "Epoch 2 Step 7/8 Loss: 4.3015\n",
      "Epoch 2 Step 8/8 Loss: 2.1463\n",
      "Epoch 2 Step 1/8 Loss: 2.0126\n",
      "Epoch 2 Step 2/8 Loss: 2.2076\n",
      "Epoch 2 Step 3/8 Loss: 3.3872\n",
      "Epoch 2 Step 4/8 Loss: 2.5763\n",
      "Epoch 2 Step 5/8 Loss: 1.3308\n",
      "Epoch 2 Step 6/8 Loss: 1.9523\n",
      "Epoch 2 Step 7/8 Loss: 2.3927\n",
      "Epoch 2 Step 8/8 Loss: 2.5410\n",
      "Epoch 2 Step 1/8 Loss: 2.1679\n",
      "Epoch 2 Step 2/8 Loss: 2.7353\n",
      "Epoch 2 Step 3/8 Loss: 1.7093\n",
      "Epoch 2 Step 4/8 Loss: 2.5960\n",
      "Epoch 2 Step 5/8 Loss: 1.9896\n",
      "Epoch 2 Step 6/8 Loss: 1.4448\n",
      "Epoch 2 Step 7/8 Loss: 2.9167\n",
      "Epoch 2 Step 8/8 Loss: 4.8329\n",
      "Epoch 2 Step 1/8 Loss: 3.4449\n",
      "Epoch 2 Step 2/8 Loss: 1.1596\n",
      "Epoch 2 Step 3/8 Loss: 3.0731\n",
      "Epoch 2 Step 4/8 Loss: 2.2669\n",
      "Epoch 2 Step 5/8 Loss: 2.7784\n",
      "Epoch 2 Step 6/8 Loss: 2.7525\n",
      "Epoch 2 Step 7/8 Loss: 2.2222\n",
      "Epoch 2 Step 8/8 Loss: 1.8803\n",
      "Epoch 2 Step 1/8 Loss: 4.0388\n",
      "Epoch 2 Step 2/8 Loss: 2.4906\n",
      "Epoch 2 Step 3/8 Loss: 2.2178\n",
      "Epoch 2 Step 4/8 Loss: 1.2610\n",
      "Epoch 2 Step 5/8 Loss: 3.0155\n",
      "Epoch 2 Step 6/8 Loss: 3.4252\n",
      "Epoch 2 Step 7/8 Loss: 2.3166\n",
      "Epoch 2 Step 8/8 Loss: 3.5461\n",
      "Epoch 2 Step 1/8 Loss: 2.0488\n",
      "Epoch 2 Step 2/8 Loss: 1.0996\n",
      "Epoch 2 Step 3/8 Loss: 3.3303\n",
      "Epoch 2 Step 4/8 Loss: 4.6309\n",
      "Epoch 2 Step 5/8 Loss: 3.5919\n",
      "Epoch 2 Step 6/8 Loss: 2.7621\n",
      "Epoch 2 Step 7/8 Loss: 2.0188\n",
      "Epoch 2 Step 8/8 Loss: 2.8425\n",
      "Epoch 2 Step 1/8 Loss: 1.2039\n",
      "Epoch 2 Step 2/8 Loss: 4.6697\n",
      "Epoch 2 Step 3/8 Loss: 2.5634\n",
      "Epoch 2 Step 4/8 Loss: 1.8180\n",
      "Epoch 2 Step 5/8 Loss: 2.6763\n",
      "Epoch 2 Step 6/8 Loss: 1.2345\n",
      "Epoch 2 Step 7/8 Loss: 3.1231\n",
      "Epoch 2 Step 8/8 Loss: 2.7731\n",
      "Epoch 2 Step 1/8 Loss: 3.3166\n",
      "Epoch 2 Step 2/8 Loss: 2.5029\n",
      "Epoch 2 Step 3/8 Loss: 2.2720\n",
      "Epoch 2 Step 4/8 Loss: 2.4730\n",
      "Epoch 2 Step 5/8 Loss: 3.9802\n",
      "Epoch 2 Step 6/8 Loss: 2.2101\n",
      "Epoch 2 Step 7/8 Loss: 2.8302\n",
      "Epoch 2 Step 8/8 Loss: 2.1496\n",
      "Epoch 2 Step 1/8 Loss: 3.9147\n",
      "Epoch 2 Step 2/8 Loss: 1.0755\n",
      "Epoch 2 Step 3/8 Loss: 2.6867\n",
      "Epoch 2 Step 4/8 Loss: 2.8804\n",
      "Epoch 2 Step 5/8 Loss: 3.4245\n",
      "Epoch 2 Step 6/8 Loss: 2.3889\n",
      "Epoch 2 Step 7/8 Loss: 2.9102\n",
      "Epoch 2 Step 8/8 Loss: 1.5592\n",
      "Epoch 2 Step 1/8 Loss: 1.3675\n",
      "Epoch 2 Step 2/8 Loss: 3.1894\n",
      "Epoch 2 Step 3/8 Loss: 2.7587\n",
      "Epoch 2 Step 4/8 Loss: 2.4424\n",
      "Epoch 2 Step 5/8 Loss: 4.7135\n",
      "Epoch 2 Step 6/8 Loss: 3.0767\n",
      "Epoch 2 Step 7/8 Loss: 2.5265\n",
      "Epoch 2 Step 8/8 Loss: 4.0006\n",
      "Epoch 2 Step 1/8 Loss: 1.5943\n",
      "Epoch 2 Step 2/8 Loss: 3.6162\n",
      "Epoch 2 Step 3/8 Loss: 2.9781\n",
      "Epoch 2 Step 4/8 Loss: 2.6958\n",
      "Epoch 2 Step 5/8 Loss: 3.3151\n",
      "Epoch 2 Step 6/8 Loss: 2.6827\n",
      "Epoch 2 Step 7/8 Loss: 2.3605\n",
      "Epoch 2 Step 8/8 Loss: 2.4616\n",
      "Epoch 2 Step 1/8 Loss: 2.1405\n",
      "Epoch 2 Step 2/8 Loss: 2.6895\n",
      "Epoch 2 Step 3/8 Loss: 2.7658\n",
      "Epoch 2 Step 4/8 Loss: 2.8435\n",
      "Epoch 2 Step 5/8 Loss: 1.6467\n",
      "Epoch 2 Step 6/8 Loss: 2.5798\n",
      "Epoch 2 Step 7/8 Loss: 2.5841\n",
      "Epoch 2 Step 8/8 Loss: 2.1450\n",
      "Epoch 2 Step 1/8 Loss: 2.3606\n",
      "Epoch 2 Step 2/8 Loss: 4.2595\n",
      "Epoch 2 Step 3/8 Loss: 1.4841\n",
      "Epoch 2 Step 4/8 Loss: 1.8893\n",
      "Epoch 2 Step 5/8 Loss: 2.9872\n",
      "Epoch 2 Step 6/8 Loss: 1.7076\n",
      "Epoch 2 Step 7/8 Loss: 3.1776\n",
      "Epoch 2 Step 8/8 Loss: 3.1567\n",
      "Epoch 2 Step 1/8 Loss: 2.4360\n",
      "Epoch 2 Step 2/8 Loss: 2.4539\n",
      "Epoch 2 Step 3/8 Loss: 2.4993\n",
      "Epoch 2 Step 4/8 Loss: 2.3393\n",
      "Epoch 2 Step 5/8 Loss: 4.3689\n",
      "Epoch 2 Step 6/8 Loss: 3.5152\n",
      "Epoch 2 Step 7/8 Loss: 2.4999\n",
      "Epoch 2 Step 8/8 Loss: 2.0272\n",
      "Epoch 2 Step 1/8 Loss: 0.9764\n",
      "Epoch 2 Step 2/8 Loss: 3.8413\n",
      "Epoch 2 Step 3/8 Loss: 1.1843\n",
      "Epoch 2 Step 4/8 Loss: 3.1987\n",
      "Epoch 2 Step 5/8 Loss: 2.8781\n",
      "Epoch 2 Step 6/8 Loss: 4.0897\n",
      "Epoch 2 Step 7/8 Loss: 3.5342\n",
      "Epoch 2 Step 8/8 Loss: 3.7223\n",
      "Epoch 2 Step 1/8 Loss: 4.5126\n",
      "Epoch 2 Step 2/8 Loss: 2.7003\n",
      "Epoch 2 Step 3/8 Loss: 1.5490\n",
      "Epoch 2 Step 4/8 Loss: 3.1895\n",
      "Epoch 2 Step 5/8 Loss: 3.7024\n",
      "Epoch 2 Step 6/8 Loss: 1.9601\n",
      "Epoch 2 Step 7/8 Loss: 2.8608\n",
      "Epoch 2 Step 8/8 Loss: 3.1634\n",
      "Epoch 2 Step 1/8 Loss: 1.0263\n",
      "Epoch 2 Step 2/8 Loss: 3.6679\n",
      "Epoch 2 Step 3/8 Loss: 1.3211\n",
      "Epoch 2 Step 4/8 Loss: 2.3270\n",
      "Epoch 2 Step 5/8 Loss: 2.6469\n",
      "Epoch 2 Step 6/8 Loss: 2.3333\n",
      "Epoch 2 Step 7/8 Loss: 3.9381\n",
      "Epoch 2 Step 8/8 Loss: 3.0098\n",
      "Epoch 2 Step 1/8 Loss: 2.2305\n",
      "Epoch 2 Step 2/8 Loss: 3.8776\n",
      "Epoch 2 Step 3/8 Loss: 2.8660\n",
      "Epoch 2 Step 4/8 Loss: 2.7870\n",
      "Epoch 2 Step 5/8 Loss: 3.9085\n",
      "Epoch 2 Step 6/8 Loss: 3.0556\n",
      "Epoch 2 Step 7/8 Loss: 0.7688\n",
      "Epoch 2 Step 8/8 Loss: 1.7419\n",
      "Epoch 2 Step 1/8 Loss: 4.6716\n",
      "Epoch 2 Step 2/8 Loss: 2.5514\n",
      "Epoch 2 Step 3/8 Loss: 3.5094\n",
      "Epoch 2 Step 4/8 Loss: 1.6315\n",
      "Epoch 2 Step 5/8 Loss: 1.8843\n",
      "Epoch 2 Step 6/8 Loss: 2.0403\n",
      "Epoch 2 Step 7/8 Loss: 3.3092\n",
      "Epoch 2 Step 8/8 Loss: 2.5200\n",
      "Epoch 2 Step 1/8 Loss: 3.6304\n",
      "Epoch 2 Step 2/8 Loss: 3.1136\n",
      "Epoch 2 Step 3/8 Loss: 3.2499\n",
      "Epoch 2 Step 4/8 Loss: 3.5816\n",
      "Epoch 2 Step 5/8 Loss: 2.8222\n",
      "Epoch 2 Step 6/8 Loss: 2.4250\n",
      "Epoch 2 Step 7/8 Loss: 1.6021\n",
      "Epoch 2 Step 8/8 Loss: 2.1435\n",
      "Epoch 2 Step 1/8 Loss: 1.6200\n",
      "Epoch 2 Step 2/8 Loss: 1.9381\n",
      "Epoch 2 Step 3/8 Loss: 2.6464\n",
      "Epoch 2 Step 4/8 Loss: 1.9302\n",
      "Epoch 2 Step 5/8 Loss: 1.2547\n",
      "Epoch 2 Step 6/8 Loss: 2.6294\n",
      "Epoch 2 Step 7/8 Loss: 3.9037\n",
      "Epoch 2 Step 8/8 Loss: 3.0802\n",
      "Epoch 2 Step 1/8 Loss: 1.9121\n",
      "Epoch 2 Step 2/8 Loss: 2.6519\n",
      "Epoch 2 Step 3/8 Loss: 1.0939\n",
      "Epoch 2 Step 4/8 Loss: 2.0508\n",
      "Epoch 2 Step 5/8 Loss: 3.7076\n",
      "Epoch 2 Step 6/8 Loss: 2.4954\n",
      "Epoch 2 Step 7/8 Loss: 2.7176\n",
      "Epoch 2 Step 8/8 Loss: 3.7372\n",
      "Epoch 2 Step 1/8 Loss: 2.7668\n",
      "Epoch 2 Step 2/8 Loss: 2.2567\n",
      "Epoch 2 Step 3/8 Loss: 2.5326\n",
      "Epoch 2 Step 4/8 Loss: 1.9648\n",
      "Epoch 2 Step 5/8 Loss: 1.5846\n",
      "Epoch 2 Step 6/8 Loss: 1.8982\n",
      "Epoch 2 Step 7/8 Loss: 4.1762\n",
      "Epoch 2 Step 8/8 Loss: 2.8426\n",
      "Epoch 2 Step 1/8 Loss: 2.9248\n",
      "Epoch 2 Step 2/8 Loss: 4.1314\n",
      "Epoch 2 Step 3/8 Loss: 2.8947\n",
      "Epoch 2 Step 4/8 Loss: 4.2260\n",
      "Epoch 2 Step 5/8 Loss: 2.4790\n",
      "Epoch 2 Step 6/8 Loss: 3.3798\n",
      "Epoch 2 Step 7/8 Loss: 3.1444\n",
      "Epoch 2 Step 8/8 Loss: 1.3825\n",
      "Epoch 2 Step 1/8 Loss: 2.8407\n",
      "Epoch 2 Step 2/8 Loss: 2.0846\n",
      "Epoch 2 Step 3/8 Loss: 1.7590\n",
      "Epoch 2 Step 4/8 Loss: 1.0441\n",
      "Epoch 2 Step 5/8 Loss: 2.3972\n",
      "Epoch 2 Step 6/8 Loss: 2.6461\n",
      "Epoch 2 Step 7/8 Loss: 3.3716\n",
      "Epoch 2 Step 8/8 Loss: 3.0304\n",
      "Epoch 2 Step 1/8 Loss: 3.0383\n",
      "Epoch 2 Step 2/8 Loss: 2.6870\n",
      "Epoch 2 Step 3/8 Loss: 1.3688\n",
      "Epoch 2 Step 4/8 Loss: 1.0454\n",
      "Epoch 2 Step 5/8 Loss: 2.0704\n",
      "Epoch 2 Step 6/8 Loss: 3.7498\n",
      "Epoch 2 Step 7/8 Loss: 4.5368\n",
      "Epoch 2 Step 8/8 Loss: 2.9506\n",
      "Epoch 2 Step 1/8 Loss: 1.8443\n",
      "Epoch 2 Step 2/8 Loss: 2.5745\n",
      "Epoch 2 Step 3/8 Loss: 2.8879\n",
      "Epoch 2 Step 4/8 Loss: 2.1453\n",
      "Epoch 2 Step 5/8 Loss: 2.0329\n",
      "Epoch 2 Step 6/8 Loss: 3.6166\n",
      "Epoch 2 Step 7/8 Loss: 1.7947\n",
      "Epoch 2 Step 8/8 Loss: 1.8595\n",
      "Epoch 2 Step 1/8 Loss: 2.3541\n",
      "Epoch 2 Step 2/8 Loss: 1.0631\n",
      "Epoch 2 Step 3/8 Loss: 3.4477\n",
      "Epoch 2 Step 4/8 Loss: 2.5106\n",
      "Epoch 2 Step 5/8 Loss: 1.9658\n",
      "Epoch 2 Step 6/8 Loss: 3.1435\n",
      "Epoch 2 Step 7/8 Loss: 3.5148\n",
      "Epoch 2 Step 8/8 Loss: 2.1502\n",
      "Epoch 2 Step 1/8 Loss: 3.3818\n",
      "Epoch 2 Step 2/8 Loss: 2.0719\n",
      "Epoch 2 Step 3/8 Loss: 3.4915\n",
      "Epoch 2 Step 4/8 Loss: 2.6170\n",
      "Epoch 2 Step 5/8 Loss: 3.2569\n",
      "Epoch 2 Step 6/8 Loss: 4.1382\n",
      "Epoch 2 Step 7/8 Loss: 2.5810\n",
      "Epoch 2 Step 8/8 Loss: 1.8043\n",
      "Epoch 2 Step 1/8 Loss: 3.5454\n",
      "Epoch 2 Step 2/8 Loss: 3.0085\n",
      "Epoch 2 Step 3/8 Loss: 1.2967\n",
      "Epoch 2 Step 4/8 Loss: 3.5221\n",
      "Epoch 2 Step 5/8 Loss: 1.7903\n",
      "Epoch 2 Step 6/8 Loss: 1.5616\n",
      "Epoch 2 Step 7/8 Loss: 2.5410\n",
      "Epoch 2 Step 8/8 Loss: 1.6288\n",
      "Epoch 2 Step 1/8 Loss: 4.0207\n",
      "Epoch 2 Step 2/8 Loss: 1.5195\n",
      "Epoch 2 Step 3/8 Loss: 2.0614\n",
      "Epoch 2 Step 4/8 Loss: 2.5326\n",
      "Epoch 2 Step 5/8 Loss: 2.0979\n",
      "Epoch 2 Step 6/8 Loss: 3.1486\n",
      "Epoch 2 Step 7/8 Loss: 2.8143\n",
      "Epoch 2 Step 8/8 Loss: 3.6178\n",
      "Epoch 2 Step 1/8 Loss: 3.0941\n",
      "Epoch 2 Step 2/8 Loss: 1.7591\n",
      "Epoch 2 Step 3/8 Loss: 2.9600\n",
      "Epoch 2 Step 4/8 Loss: 3.5390\n",
      "Epoch 2 Step 5/8 Loss: 2.5659\n",
      "Epoch 2 Step 6/8 Loss: 2.3100\n",
      "Epoch 2 Step 7/8 Loss: 2.5143\n",
      "Epoch 2 Step 8/8 Loss: 1.4103\n",
      "Epoch 2 Step 1/8 Loss: 2.2823\n",
      "Epoch 2 Step 2/8 Loss: 1.9755\n",
      "Epoch 2 Step 3/8 Loss: 1.8122\n",
      "Epoch 2 Step 4/8 Loss: 2.2618\n",
      "Epoch 2 Step 5/8 Loss: 2.6383\n",
      "Epoch 2 Step 6/8 Loss: 2.0576\n",
      "Epoch 2 Step 7/8 Loss: 2.5815\n",
      "Epoch 2 Step 8/8 Loss: 1.5324\n",
      "Epoch 2 Step 1/8 Loss: 1.1205\n",
      "Epoch 2 Step 2/8 Loss: 2.7296\n",
      "Epoch 2 Step 3/8 Loss: 2.7764\n",
      "Epoch 2 Step 4/8 Loss: 2.8189\n",
      "Epoch 2 Step 5/8 Loss: 1.8979\n",
      "Epoch 2 Step 6/8 Loss: 2.8127\n",
      "Epoch 2 Step 7/8 Loss: 1.6720\n",
      "Epoch 2 Step 8/8 Loss: 2.1570\n",
      "Epoch 2 Step 1/8 Loss: 1.8789\n",
      "Epoch 2 Step 2/8 Loss: 1.1737\n",
      "Epoch 2 Step 3/8 Loss: 2.3788\n",
      "Epoch 2 Step 4/8 Loss: 1.6665\n",
      "Epoch 2 Step 5/8 Loss: 2.8368\n",
      "Epoch 2 Step 6/8 Loss: 2.6730\n",
      "Epoch 2 Step 7/8 Loss: 3.5970\n",
      "Epoch 2 Step 8/8 Loss: 3.5242\n",
      "Epoch 2 Step 1/8 Loss: 3.6245\n",
      "Epoch 2 Step 2/8 Loss: 2.4721\n",
      "Epoch 2 Step 3/8 Loss: 2.8598\n",
      "Epoch 2 Step 4/8 Loss: 4.1474\n",
      "Epoch 2 Step 5/8 Loss: 1.0657\n",
      "Epoch 2 Step 6/8 Loss: 2.7480\n",
      "Epoch 2 Step 7/8 Loss: 2.2645\n",
      "Epoch 2 Step 8/8 Loss: 1.1056\n",
      "Epoch 2 Step 1/8 Loss: 2.6974\n",
      "Epoch 2 Step 2/8 Loss: 1.1475\n",
      "Epoch 2 Step 3/8 Loss: 2.8291\n",
      "Epoch 2 Step 4/8 Loss: 4.2241\n",
      "Epoch 2 Step 5/8 Loss: 2.3156\n",
      "Epoch 2 Step 6/8 Loss: 3.7851\n",
      "Epoch 2 Step 7/8 Loss: 1.8270\n",
      "Epoch 2 Step 8/8 Loss: 3.1316\n",
      "Epoch 2 Step 1/8 Loss: 3.0432\n",
      "Epoch 2 Step 2/8 Loss: 2.1797\n",
      "Epoch 2 Step 3/8 Loss: 1.5105\n",
      "Epoch 2 Step 4/8 Loss: 2.5154\n",
      "Epoch 2 Step 5/8 Loss: 4.1827\n",
      "Epoch 2 Step 6/8 Loss: 2.9770\n",
      "Epoch 2 Step 7/8 Loss: 2.5444\n",
      "Epoch 2 Step 8/8 Loss: 3.5326\n",
      "Epoch 2 Step 1/8 Loss: 2.4837\n",
      "Epoch 2 Step 2/8 Loss: 2.5534\n",
      "Epoch 2 Step 3/8 Loss: 2.8841\n",
      "Epoch 2 Step 4/8 Loss: 2.1421\n",
      "Epoch 2 Step 5/8 Loss: 2.8283\n",
      "Epoch 2 Step 6/8 Loss: 2.9649\n",
      "Epoch 2 Step 7/8 Loss: 3.2380\n",
      "Epoch 2 Step 8/8 Loss: 1.2439\n",
      "Epoch 2 Step 1/8 Loss: 3.8858\n",
      "Epoch 2 Step 2/8 Loss: 2.8615\n",
      "Epoch 2 Step 3/8 Loss: 2.4934\n",
      "Epoch 2 Step 4/8 Loss: 2.9718\n",
      "Epoch 2 Step 5/8 Loss: 2.3444\n",
      "Epoch 2 Step 6/8 Loss: 3.4582\n",
      "Epoch 2 Step 7/8 Loss: 2.1918\n",
      "Epoch 2 Step 8/8 Loss: 2.0170\n",
      "Epoch 2 Step 1/8 Loss: 2.0621\n",
      "Epoch 2 Step 2/8 Loss: 4.3490\n",
      "Epoch 2 Step 3/8 Loss: 1.8717\n",
      "Epoch 2 Step 4/8 Loss: 3.4585\n",
      "Epoch 2 Step 5/8 Loss: 2.0384\n",
      "Epoch 2 Step 6/8 Loss: 1.4858\n",
      "Epoch 2 Step 7/8 Loss: 2.9715\n",
      "Epoch 2 Step 8/8 Loss: 2.9073\n",
      "Epoch 2 Step 1/8 Loss: 2.8577\n",
      "Epoch 2 Step 2/8 Loss: 2.4466\n",
      "Epoch 2 Step 3/8 Loss: 2.4308\n",
      "Epoch 2 Step 4/8 Loss: 2.9683\n",
      "Epoch 2 Step 5/8 Loss: 4.2454\n",
      "Epoch 2 Step 6/8 Loss: 2.8509\n",
      "Epoch 2 Step 7/8 Loss: 3.5715\n",
      "Epoch 2 Step 8/8 Loss: 1.7520\n",
      "Epoch 2 completed. Average Loss: nan\n",
      "Epoch 3 Step 1/8 Loss: 1.3601\n",
      "Epoch 3 Step 2/8 Loss: 4.0426\n",
      "Epoch 3 Step 3/8 Loss: 3.3057\n",
      "Epoch 3 Step 4/8 Loss: 2.3398\n",
      "Epoch 3 Step 5/8 Loss: 2.6342\n",
      "Epoch 3 Step 6/8 Loss: 2.8055\n",
      "Epoch 3 Step 7/8 Loss: 2.6319\n",
      "Epoch 3 Step 8/8 Loss: 2.8696\n",
      "Epoch 3 Step 1/8 Loss: 2.5605\n",
      "Epoch 3 Step 2/8 Loss: 1.2104\n",
      "Epoch 3 Step 3/8 Loss: 2.2663\n",
      "Epoch 3 Step 4/8 Loss: 2.8696\n",
      "Epoch 3 Step 5/8 Loss: 2.4919\n",
      "Epoch 3 Step 6/8 Loss: 2.6069\n",
      "Epoch 3 Step 7/8 Loss: 1.2113\n",
      "Epoch 3 Step 8/8 Loss: 2.3680\n",
      "Epoch 3 Step 1/8 Loss: 3.6746\n",
      "Epoch 3 Step 2/8 Loss: 2.4362\n",
      "Epoch 3 Step 3/8 Loss: 1.3888\n",
      "Epoch 3 Step 4/8 Loss: 4.5332\n",
      "Epoch 3 Step 5/8 Loss: 2.9608\n",
      "Epoch 3 Step 6/8 Loss: 2.8967\n",
      "Epoch 3 Step 7/8 Loss: 3.3633\n",
      "Epoch 3 Step 8/8 Loss: 2.9555\n",
      "Epoch 3 Step 1/8 Loss: 2.7987\n",
      "Epoch 3 Step 2/8 Loss: 4.0007\n",
      "Epoch 3 Step 3/8 Loss: 4.0419\n",
      "Epoch 3 Step 4/8 Loss: 4.2597\n",
      "Epoch 3 Step 5/8 Loss: 1.5999\n",
      "Epoch 3 Step 6/8 Loss: 2.3841\n",
      "Epoch 3 Step 7/8 Loss: 1.3352\n",
      "Epoch 3 Step 8/8 Loss: 3.5137\n",
      "Epoch 3 Step 1/8 Loss: 1.5433\n",
      "Epoch 3 Step 2/8 Loss: 1.9791\n",
      "Epoch 3 Step 3/8 Loss: 2.9475\n",
      "Epoch 3 Step 4/8 Loss: 3.4517\n",
      "Epoch 3 Step 5/8 Loss: 1.8396\n",
      "Epoch 3 Step 6/8 Loss: 3.5429\n",
      "Epoch 3 Step 7/8 Loss: 1.6261\n",
      "Epoch 3 Step 8/8 Loss: 2.7728\n",
      "Epoch 3 Step 1/8 Loss: 1.5760\n",
      "Epoch 3 Step 2/8 Loss: 2.5199\n",
      "Epoch 3 Step 3/8 Loss: 2.9240\n",
      "Epoch 3 Step 4/8 Loss: 3.0163\n",
      "Epoch 3 Step 5/8 Loss: 3.2017\n",
      "Epoch 3 Step 6/8 Loss: 3.1502\n",
      "Epoch 3 Step 7/8 Loss: 3.3517\n",
      "Epoch 3 Step 8/8 Loss: 1.3969\n",
      "Epoch 3 Step 1/8 Loss: 3.2893\n",
      "Epoch 3 Step 2/8 Loss: 3.4646\n",
      "Epoch 3 Step 3/8 Loss: 3.5499\n",
      "Epoch 3 Step 4/8 Loss: 2.1561\n",
      "Epoch 3 Step 5/8 Loss: 3.2058\n",
      "Epoch 3 Step 6/8 Loss: 1.3021\n",
      "Epoch 3 Step 7/8 Loss: 3.6622\n",
      "Epoch 3 Step 8/8 Loss: 2.5411\n",
      "Epoch 3 Step 1/8 Loss: 1.2437\n",
      "Epoch 3 Step 2/8 Loss: 3.8205\n",
      "Epoch 3 Step 3/8 Loss: 4.3389\n",
      "Epoch 3 Step 4/8 Loss: 2.6123\n",
      "Epoch 3 Step 5/8 Loss: 2.9429\n",
      "Epoch 3 Step 6/8 Loss: 3.8266\n",
      "Epoch 3 Step 7/8 Loss: 2.1228\n",
      "Epoch 3 Step 8/8 Loss: 2.5736\n",
      "Epoch 3 Step 1/8 Loss: 2.0271\n",
      "Epoch 3 Step 2/8 Loss: 1.5427\n",
      "Epoch 3 Step 3/8 Loss: 3.7110\n",
      "Epoch 3 Step 4/8 Loss: 3.3949\n",
      "Epoch 3 Step 5/8 Loss: 2.0531\n",
      "Epoch 3 Step 6/8 Loss: 2.5089\n",
      "Epoch 3 Step 7/8 Loss: 2.3195\n",
      "Epoch 3 Step 8/8 Loss: 2.3355\n",
      "Epoch 3 Step 1/8 Loss: 3.2382\n",
      "Epoch 3 Step 2/8 Loss: 1.5663\n",
      "Epoch 3 Step 3/8 Loss: 2.0317\n",
      "Epoch 3 Step 4/8 Loss: 3.2481\n",
      "Epoch 3 Step 5/8 Loss: 1.2550\n",
      "Epoch 3 Step 6/8 Loss: 4.7729\n",
      "Epoch 3 Step 7/8 Loss: 3.1261\n",
      "Epoch 3 Step 8/8 Loss: 2.3200\n",
      "Epoch 3 Step 1/8 Loss: 1.7989\n",
      "Epoch 3 Step 2/8 Loss: 4.2642\n",
      "Epoch 3 Step 3/8 Loss: 1.0126\n",
      "Epoch 3 Step 4/8 Loss: 3.5352\n",
      "Epoch 3 Step 5/8 Loss: 4.3786\n",
      "Epoch 3 Step 6/8 Loss: 2.5632\n",
      "Epoch 3 Step 7/8 Loss: 2.8707\n",
      "Epoch 3 Step 8/8 Loss: 1.9946\n",
      "Epoch 3 Step 1/8 Loss: 3.1885\n",
      "Epoch 3 Step 2/8 Loss: 3.7547\n",
      "Epoch 3 Step 3/8 Loss: 3.0009\n",
      "Epoch 3 Step 4/8 Loss: 2.3086\n",
      "Epoch 3 Step 5/8 Loss: 2.0348\n",
      "Epoch 3 Step 6/8 Loss: 2.0218\n",
      "Epoch 3 Step 7/8 Loss: 1.3997\n",
      "Epoch 3 Step 8/8 Loss: 2.1792\n",
      "Epoch 3 Step 1/8 Loss: 3.3914\n",
      "Epoch 3 Step 2/8 Loss: 2.2146\n",
      "Epoch 3 Step 3/8 Loss: 2.2210\n",
      "Epoch 3 Step 4/8 Loss: 1.5841\n",
      "Epoch 3 Step 5/8 Loss: 3.1704\n",
      "Epoch 3 Step 6/8 Loss: 2.2061\n",
      "Epoch 3 Step 7/8 Loss: 4.1785\n",
      "Epoch 3 Step 8/8 Loss: 2.8951\n",
      "Epoch 3 Step 1/8 Loss: 4.2061\n",
      "Epoch 3 Step 2/8 Loss: 1.3027\n",
      "Epoch 3 Step 3/8 Loss: 3.2539\n",
      "Epoch 3 Step 4/8 Loss: 3.2470\n",
      "Epoch 3 Step 5/8 Loss: 1.1592\n",
      "Epoch 3 Step 6/8 Loss: 2.7067\n",
      "Epoch 3 Step 7/8 Loss: 2.0119\n",
      "Epoch 3 Step 8/8 Loss: 3.6329\n",
      "Epoch 3 Step 1/8 Loss: 1.9475\n",
      "Epoch 3 Step 2/8 Loss: 1.2129\n",
      "Epoch 3 Step 3/8 Loss: 4.6578\n",
      "Epoch 3 Step 4/8 Loss: 2.4200\n",
      "Epoch 3 Step 5/8 Loss: 2.6805\n",
      "Epoch 3 Step 6/8 Loss: 2.3385\n",
      "Epoch 3 Step 7/8 Loss: 2.3215\n",
      "Epoch 3 Step 8/8 Loss: 2.1918\n",
      "Epoch 3 Step 1/8 Loss: 1.9099\n",
      "Epoch 3 Step 2/8 Loss: 1.4213\n",
      "Epoch 3 Step 3/8 Loss: 2.6901\n",
      "Epoch 3 Step 4/8 Loss: 3.5380\n",
      "Epoch 3 Step 5/8 Loss: 2.3394\n",
      "Epoch 3 Step 6/8 Loss: 2.3638\n",
      "Epoch 3 Step 7/8 Loss: 2.0268\n",
      "Epoch 3 Step 8/8 Loss: 2.7983\n",
      "Epoch 3 Step 1/8 Loss: 2.6659\n",
      "Epoch 3 Step 2/8 Loss: 1.4908\n",
      "Epoch 3 Step 3/8 Loss: 4.6606\n",
      "Epoch 3 Step 4/8 Loss: 4.9543\n",
      "Epoch 3 Step 5/8 Loss: 2.1161\n",
      "Epoch 3 Step 6/8 Loss: 2.3767\n",
      "Epoch 3 Step 7/8 Loss: 2.3380\n",
      "Epoch 3 Step 8/8 Loss: 4.4462\n",
      "Epoch 3 Step 1/8 Loss: 2.4882\n",
      "Epoch 3 Step 2/8 Loss: 2.2039\n",
      "Epoch 3 Step 3/8 Loss: 2.4779\n",
      "Epoch 3 Step 4/8 Loss: 3.2533\n",
      "Epoch 3 Step 5/8 Loss: 1.8778\n",
      "Epoch 3 Step 6/8 Loss: 2.4152\n",
      "Epoch 3 Step 7/8 Loss: 2.9798\n",
      "Epoch 3 Step 8/8 Loss: 2.0529\n",
      "Epoch 3 Step 1/8 Loss: 3.1972\n",
      "Epoch 3 Step 2/8 Loss: 2.5711\n",
      "Epoch 3 Step 3/8 Loss: 2.5584\n",
      "Epoch 3 Step 4/8 Loss: 4.1598\n",
      "Epoch 3 Step 5/8 Loss: 2.0496\n",
      "Epoch 3 Step 6/8 Loss: 1.0036\n",
      "Epoch 3 Step 7/8 Loss: 1.3870\n",
      "Epoch 3 Step 8/8 Loss: 2.4212\n",
      "Epoch 3 Step 1/8 Loss: 2.7837\n",
      "Epoch 3 Step 2/8 Loss: 2.6671\n",
      "Epoch 3 Step 3/8 Loss: 4.3204\n",
      "Epoch 3 Step 4/8 Loss: 1.7270\n",
      "Epoch 3 Step 5/8 Loss: 3.4551\n",
      "Epoch 3 Step 6/8 Loss: 1.2250\n",
      "Epoch 3 Step 7/8 Loss: 3.1877\n",
      "Epoch 3 Step 8/8 Loss: 2.9934\n",
      "Epoch 3 Step 1/8 Loss: 4.6385\n",
      "Epoch 3 Step 2/8 Loss: 2.2740\n",
      "Epoch 3 Step 3/8 Loss: 1.5601\n",
      "Epoch 3 Step 4/8 Loss: 2.0986\n",
      "Epoch 3 Step 5/8 Loss: 3.6143\n",
      "Epoch 3 Step 6/8 Loss: 1.2260\n",
      "Epoch 3 Step 7/8 Loss: 3.8601\n",
      "Epoch 3 Step 8/8 Loss: 1.9389\n",
      "Epoch 3 Step 1/8 Loss: 2.1186\n",
      "Epoch 3 Step 2/8 Loss: 0.9448\n",
      "Epoch 3 Step 3/8 Loss: 3.9638\n",
      "Epoch 3 Step 4/8 Loss: 3.3094\n",
      "Epoch 3 Step 5/8 Loss: 4.0863\n",
      "Epoch 3 Step 6/8 Loss: 3.9090\n",
      "Epoch 3 Step 7/8 Loss: 2.2311\n",
      "Epoch 3 Step 8/8 Loss: 1.7889\n",
      "Epoch 3 Step 1/8 Loss: 4.1469\n",
      "Epoch 3 Step 2/8 Loss: 3.1134\n",
      "Epoch 3 Step 3/8 Loss: 2.5549\n",
      "Epoch 3 Step 4/8 Loss: 2.3790\n",
      "Epoch 3 Step 5/8 Loss: 2.6657\n",
      "Epoch 3 Step 6/8 Loss: 1.0260\n",
      "Epoch 3 Step 7/8 Loss: 1.1994\n",
      "Epoch 3 Step 8/8 Loss: 3.1798\n",
      "Epoch 3 Step 1/8 Loss: 2.0119\n",
      "Epoch 3 Step 2/8 Loss: 1.8922\n",
      "Epoch 3 Step 3/8 Loss: 1.8754\n",
      "Epoch 3 Step 4/8 Loss: 1.2635\n",
      "Epoch 3 Step 5/8 Loss: 1.1677\n",
      "Epoch 3 Step 6/8 Loss: 3.7869\n",
      "Epoch 3 Step 7/8 Loss: 4.4181\n",
      "Epoch 3 Step 8/8 Loss: 2.7631\n",
      "Epoch 3 Step 1/8 Loss: 2.6124\n",
      "Epoch 3 Step 2/8 Loss: 3.9031\n",
      "Epoch 3 Step 3/8 Loss: 2.4129\n",
      "Epoch 3 Step 4/8 Loss: 3.0957\n",
      "Epoch 3 Step 5/8 Loss: 2.6918\n",
      "Epoch 3 Step 6/8 Loss: 1.6950\n",
      "Epoch 3 Step 7/8 Loss: 3.4520\n",
      "Epoch 3 Step 8/8 Loss: 2.8683\n",
      "Epoch 3 Step 1/8 Loss: 1.2172\n",
      "Epoch 3 Step 2/8 Loss: 2.4537\n",
      "Epoch 3 Step 3/8 Loss: 2.6626\n",
      "Epoch 3 Step 4/8 Loss: 2.9150\n",
      "Epoch 3 Step 5/8 Loss: 2.8864\n",
      "Epoch 3 Step 6/8 Loss: 1.8023\n",
      "Epoch 3 Step 7/8 Loss: 2.5613\n",
      "Epoch 3 Step 8/8 Loss: 2.8768\n",
      "Epoch 3 Step 1/8 Loss: 1.1274\n",
      "Epoch 3 Step 2/8 Loss: 3.9247\n",
      "Epoch 3 Step 3/8 Loss: 2.5858\n",
      "Epoch 3 Step 4/8 Loss: 2.2338\n",
      "Epoch 3 Step 5/8 Loss: 2.0541\n",
      "Epoch 3 Step 6/8 Loss: 4.0228\n",
      "Epoch 3 Step 7/8 Loss: 2.4359\n",
      "Epoch 3 Step 8/8 Loss: 2.7894\n",
      "Epoch 3 Step 1/8 Loss: 3.0770\n",
      "Epoch 3 Step 2/8 Loss: 2.7189\n",
      "Epoch 3 Step 3/8 Loss: 2.0980\n",
      "Epoch 3 Step 4/8 Loss: 3.6462\n",
      "Epoch 3 Step 5/8 Loss: 4.2553\n",
      "Epoch 3 Step 6/8 Loss: 3.2007\n",
      "Epoch 3 Step 7/8 Loss: 1.7117\n",
      "Epoch 3 Step 8/8 Loss: 2.0432\n",
      "Epoch 3 Step 1/8 Loss: 1.9906\n",
      "Epoch 3 Step 2/8 Loss: 2.7783\n",
      "Epoch 3 Step 3/8 Loss: 1.2156\n",
      "Epoch 3 Step 4/8 Loss: 3.6775\n",
      "Epoch 3 Step 5/8 Loss: 2.5454\n",
      "Epoch 3 Step 6/8 Loss: 3.5174\n",
      "Epoch 3 Step 7/8 Loss: 2.3401\n",
      "Epoch 3 Step 8/8 Loss: 4.2835\n",
      "Epoch 3 Step 1/8 Loss: 2.3310\n",
      "Epoch 3 Step 2/8 Loss: 2.8195\n",
      "Epoch 3 Step 3/8 Loss: 2.9394\n",
      "Epoch 3 Step 4/8 Loss: 2.1008\n",
      "Epoch 3 Step 5/8 Loss: 1.0615\n",
      "Epoch 3 Step 6/8 Loss: 2.7625\n",
      "Epoch 3 Step 7/8 Loss: 1.3433\n",
      "Epoch 3 Step 8/8 Loss: 2.3509\n",
      "Epoch 3 Step 1/8 Loss: 2.2400\n",
      "Epoch 3 Step 2/8 Loss: 2.9398\n",
      "Epoch 3 Step 3/8 Loss: 3.0992\n",
      "Epoch 3 Step 4/8 Loss: 3.3860\n",
      "Epoch 3 Step 5/8 Loss: 3.1221\n",
      "Epoch 3 Step 6/8 Loss: 2.9851\n",
      "Epoch 3 Step 7/8 Loss: 2.8974\n",
      "Epoch 3 Step 8/8 Loss: 1.7686\n",
      "Epoch 3 Step 1/8 Loss: 2.7738\n",
      "Epoch 3 Step 2/8 Loss: 2.5105\n",
      "Epoch 3 Step 3/8 Loss: 1.1374\n",
      "Epoch 3 Step 4/8 Loss: 2.1683\n",
      "Epoch 3 Step 5/8 Loss: 2.4087\n",
      "Epoch 3 Step 6/8 Loss: 3.6376\n",
      "Epoch 3 Step 7/8 Loss: 3.4968\n",
      "Epoch 3 Step 8/8 Loss: 2.9185\n",
      "Epoch 3 Step 1/8 Loss: 2.5034\n",
      "Epoch 3 Step 2/8 Loss: 2.8152\n",
      "Epoch 3 Step 3/8 Loss: 3.1964\n",
      "Epoch 3 Step 4/8 Loss: 2.7709\n",
      "Epoch 3 Step 5/8 Loss: 2.6284\n",
      "Epoch 3 Step 6/8 Loss: 2.0200\n",
      "Epoch 3 Step 7/8 Loss: 4.7041\n",
      "Epoch 3 Step 8/8 Loss: 2.2684\n",
      "Epoch 3 Step 1/8 Loss: 4.2302\n",
      "Epoch 3 Step 2/8 Loss: 2.0147\n",
      "Epoch 3 Step 3/8 Loss: 2.7985\n",
      "Epoch 3 Step 4/8 Loss: 1.6166\n",
      "Epoch 3 Step 5/8 Loss: 2.8349\n",
      "Epoch 3 Step 6/8 Loss: 2.2815\n",
      "Epoch 3 Step 7/8 Loss: 3.3494\n",
      "Epoch 3 Step 8/8 Loss: 2.9162\n",
      "Epoch 3 Step 1/8 Loss: 3.4820\n",
      "Epoch 3 Step 2/8 Loss: 1.3261\n",
      "Epoch 3 Step 3/8 Loss: 2.8133\n",
      "Epoch 3 Step 4/8 Loss: 2.4869\n",
      "Epoch 3 Step 5/8 Loss: 1.7217\n",
      "Epoch 3 Step 6/8 Loss: 3.1395\n",
      "Epoch 3 Step 7/8 Loss: 1.0520\n",
      "Epoch 3 Step 8/8 Loss: 4.2915\n",
      "Epoch 3 Step 1/8 Loss: 1.3193\n",
      "Epoch 3 Step 2/8 Loss: 1.9245\n",
      "Epoch 3 Step 3/8 Loss: 3.2695\n",
      "Epoch 3 Step 4/8 Loss: 2.3709\n",
      "Epoch 3 Step 5/8 Loss: 3.2233\n",
      "Epoch 3 Step 6/8 Loss: 1.8015\n",
      "Epoch 3 Step 7/8 Loss: 2.8545\n",
      "Epoch 3 Step 8/8 Loss: 4.0746\n",
      "Epoch 3 Step 1/8 Loss: 1.9836\n",
      "Epoch 3 Step 2/8 Loss: 3.4013\n",
      "Epoch 3 Step 3/8 Loss: 3.3745\n",
      "Epoch 3 Step 4/8 Loss: 1.1441\n",
      "Epoch 3 Step 5/8 Loss: 4.1887\n",
      "Epoch 3 Step 6/8 Loss: 1.6159\n",
      "Epoch 3 Step 7/8 Loss: 2.7889\n",
      "Epoch 3 Step 8/8 Loss: 2.0232\n",
      "Epoch 3 Step 1/8 Loss: 2.2916\n",
      "Epoch 3 Step 2/8 Loss: 2.3094\n",
      "Epoch 3 Step 3/8 Loss: 2.7165\n",
      "Epoch 3 Step 4/8 Loss: 1.6505\n",
      "Epoch 3 Step 5/8 Loss: 3.0475\n",
      "Epoch 3 Step 6/8 Loss: 3.8429\n",
      "Epoch 3 Step 7/8 Loss: 2.1875\n",
      "Epoch 3 Step 8/8 Loss: 2.7521\n",
      "Epoch 3 Step 1/8 Loss: 1.1472\n",
      "Epoch 3 Step 2/8 Loss: 1.7857\n",
      "Epoch 3 Step 3/8 Loss: 1.5732\n",
      "Epoch 3 Step 4/8 Loss: 2.0057\n",
      "Epoch 3 Step 5/8 Loss: 2.4564\n",
      "Epoch 3 Step 6/8 Loss: 2.7281\n",
      "Epoch 3 Step 7/8 Loss: 1.4760\n",
      "Epoch 3 Step 8/8 Loss: 4.5821\n",
      "Epoch 3 Step 1/8 Loss: 3.5617\n",
      "Epoch 3 Step 2/8 Loss: 1.4773\n",
      "Epoch 3 Step 3/8 Loss: 3.9649\n",
      "Epoch 3 Step 4/8 Loss: 1.7060\n",
      "Epoch 3 Step 5/8 Loss: 2.7863\n",
      "Epoch 3 Step 6/8 Loss: 1.2599\n",
      "Epoch 3 Step 7/8 Loss: 2.2719\n",
      "Epoch 3 Step 8/8 Loss: 2.3649\n",
      "Epoch 3 Step 1/8 Loss: 3.4738\n",
      "Epoch 3 Step 2/8 Loss: 3.5864\n",
      "Epoch 3 Step 3/8 Loss: 1.1257\n",
      "Epoch 3 Step 4/8 Loss: 1.0618\n",
      "Epoch 3 Step 5/8 Loss: 2.4576\n",
      "Epoch 3 Step 6/8 Loss: 2.4675\n",
      "Epoch 3 Step 7/8 Loss: 3.3185\n",
      "Epoch 3 Step 8/8 Loss: 3.9643\n",
      "Epoch 3 Step 1/8 Loss: 4.8583\n",
      "Epoch 3 Step 2/8 Loss: 2.0065\n",
      "Epoch 3 Step 3/8 Loss: 3.7163\n",
      "Epoch 3 Step 4/8 Loss: 1.4873\n",
      "Epoch 3 Step 5/8 Loss: 3.0997\n",
      "Epoch 3 Step 6/8 Loss: 2.7891\n",
      "Epoch 3 Step 7/8 Loss: 2.2150\n",
      "Epoch 3 Step 8/8 Loss: 3.5730\n",
      "Epoch 3 Step 1/8 Loss: 4.0639\n",
      "Epoch 3 Step 2/8 Loss: 2.1955\n",
      "Epoch 3 Step 3/8 Loss: 2.2262\n",
      "Epoch 3 Step 4/8 Loss: 3.8419\n",
      "Epoch 3 Step 5/8 Loss: 3.1945\n",
      "Epoch 3 Step 6/8 Loss: 2.6120\n",
      "Epoch 3 Step 7/8 Loss: 2.7557\n",
      "Epoch 3 Step 8/8 Loss: 3.4543\n",
      "Epoch 3 Step 1/8 Loss: 2.2447\n",
      "Epoch 3 Step 2/8 Loss: 2.4570\n",
      "Epoch 3 Step 3/8 Loss: 3.2110\n",
      "Epoch 3 Step 4/8 Loss: 3.0258\n",
      "Epoch 3 Step 5/8 Loss: 3.2394\n",
      "Epoch 3 Step 6/8 Loss: 2.0547\n",
      "Epoch 3 Step 7/8 Loss: 1.1658\n",
      "Epoch 3 Step 8/8 Loss: 1.9719\n",
      "Epoch 3 Step 1/8 Loss: 3.5478\n",
      "Epoch 3 Step 2/8 Loss: 2.9995\n",
      "Epoch 3 Step 3/8 Loss: 4.4359\n",
      "Epoch 3 Step 4/8 Loss: 2.9758\n",
      "Epoch 3 Step 5/8 Loss: 2.6332\n",
      "Epoch 3 Step 6/8 Loss: 3.2183\n",
      "Epoch 3 Step 7/8 Loss: 3.6853\n",
      "Epoch 3 Step 8/8 Loss: 2.5410\n",
      "Epoch 3 Step 1/8 Loss: 1.9818\n",
      "Epoch 3 Step 2/8 Loss: 1.5367\n",
      "Epoch 3 Step 3/8 Loss: 2.4434\n",
      "Epoch 3 Step 4/8 Loss: 3.4002\n",
      "Epoch 3 Step 5/8 Loss: 2.5778\n",
      "Epoch 3 Step 6/8 Loss: 3.2066\n",
      "Epoch 3 Step 7/8 Loss: 3.8128\n",
      "Epoch 3 Step 8/8 Loss: 2.3293\n",
      "Epoch 3 Step 1/8 Loss: 2.0616\n",
      "Epoch 3 Step 2/8 Loss: 3.0013\n",
      "Epoch 3 Step 3/8 Loss: 2.7706\n",
      "Epoch 3 Step 4/8 Loss: 2.2219\n",
      "Epoch 3 Step 5/8 Loss: 2.1540\n",
      "Epoch 3 Step 6/8 Loss: 2.2998\n",
      "Epoch 3 Step 7/8 Loss: 1.7792\n",
      "Epoch 3 Step 8/8 Loss: 4.2094\n",
      "Epoch 3 Step 1/8 Loss: 1.8874\n",
      "Epoch 3 Step 2/8 Loss: 2.3006\n",
      "Epoch 3 Step 3/8 Loss: 3.9032\n",
      "Epoch 3 Step 4/8 Loss: 4.8934\n",
      "Epoch 3 Step 5/8 Loss: 1.0764\n",
      "Epoch 3 Step 6/8 Loss: 3.3531\n",
      "Epoch 3 Step 7/8 Loss: 2.9402\n",
      "Epoch 3 Step 8/8 Loss: 1.7773\n",
      "Epoch 3 Step 1/8 Loss: 3.8240\n",
      "Epoch 3 Step 2/8 Loss: 3.4790\n",
      "Epoch 3 Step 3/8 Loss: 1.4827\n",
      "Epoch 3 Step 4/8 Loss: 3.0516\n",
      "Epoch 3 Step 5/8 Loss: 2.2293\n",
      "Epoch 3 Step 6/8 Loss: 2.3139\n",
      "Epoch 3 Step 7/8 Loss: 2.0312\n",
      "Epoch 3 Step 8/8 Loss: 2.4791\n",
      "Epoch 3 Step 1/8 Loss: 1.5165\n",
      "Epoch 3 Step 2/8 Loss: 3.1601\n",
      "Epoch 3 Step 3/8 Loss: 1.8168\n",
      "Epoch 3 Step 4/8 Loss: 2.4863\n",
      "Epoch 3 Step 5/8 Loss: 1.5551\n",
      "Epoch 3 Step 6/8 Loss: 1.1477\n",
      "Epoch 3 Step 7/8 Loss: 2.9690\n",
      "Epoch 3 Step 8/8 Loss: 4.1384\n",
      "Epoch 3 Step 1/8 Loss: 3.0864\n",
      "Epoch 3 Step 2/8 Loss: 1.5838\n",
      "Epoch 3 Step 3/8 Loss: 3.9206\n",
      "Epoch 3 Step 4/8 Loss: 3.2248\n",
      "Epoch 3 Step 5/8 Loss: 3.8346\n",
      "Epoch 3 Step 6/8 Loss: 1.7877\n",
      "Epoch 3 Step 7/8 Loss: 4.6951\n",
      "Epoch 3 Step 8/8 Loss: 2.6690\n",
      "Epoch 3 Step 1/8 Loss: 3.9899\n",
      "Epoch 3 Step 2/8 Loss: 1.8993\n",
      "Epoch 3 Step 3/8 Loss: 2.7554\n",
      "Epoch 3 Step 4/8 Loss: 2.3293\n",
      "Epoch 3 Step 5/8 Loss: 1.7630\n",
      "Epoch 3 Step 6/8 Loss: 2.1928\n",
      "Epoch 3 Step 7/8 Loss: 1.8519\n",
      "Epoch 3 Step 8/8 Loss: 2.1832\n",
      "Epoch 3 Step 1/8 Loss: 2.9490\n",
      "Epoch 3 Step 2/8 Loss: 1.9832\n",
      "Epoch 3 Step 3/8 Loss: 1.9441\n",
      "Epoch 3 Step 4/8 Loss: 2.1152\n",
      "Epoch 3 Step 5/8 Loss: 3.6151\n",
      "Epoch 3 Step 6/8 Loss: 3.6455\n",
      "Epoch 3 Step 7/8 Loss: 3.9737\n",
      "Epoch 3 Step 8/8 Loss: 1.9927\n",
      "Epoch 3 Step 1/8 Loss: 2.5279\n",
      "Epoch 3 Step 2/8 Loss: 3.1451\n",
      "Epoch 3 Step 3/8 Loss: 3.3894\n",
      "Epoch 3 Step 4/8 Loss: 1.9226\n",
      "Epoch 3 Step 5/8 Loss: 1.2681\n",
      "Epoch 3 Step 6/8 Loss: 2.5738\n",
      "Epoch 3 Step 7/8 Loss: 2.6045\n",
      "Epoch 3 Step 8/8 Loss: 2.3508\n",
      "Epoch 3 Step 1/8 Loss: 1.5631\n",
      "Epoch 3 Step 2/8 Loss: 4.9200\n",
      "Epoch 3 Step 3/8 Loss: 1.7480\n",
      "Epoch 3 Step 4/8 Loss: 3.7095\n",
      "Epoch 3 Step 5/8 Loss: 2.8583\n",
      "Epoch 3 Step 6/8 Loss: 3.8960\n",
      "Epoch 3 Step 7/8 Loss: 2.7635\n",
      "Epoch 3 Step 8/8 Loss: 3.8288\n",
      "Epoch 3 Step 1/8 Loss: 1.1670\n",
      "Epoch 3 Step 2/8 Loss: 2.7232\n",
      "Epoch 3 Step 3/8 Loss: 2.1489\n",
      "Epoch 3 Step 4/8 Loss: 2.8542\n",
      "Epoch 3 Step 5/8 Loss: 5.2951\n",
      "Epoch 3 Step 6/8 Loss: 2.0274\n",
      "Epoch 3 Step 7/8 Loss: 2.1345\n",
      "Epoch 3 Step 8/8 Loss: 2.4951\n",
      "Epoch 3 Step 1/8 Loss: 3.1421\n",
      "Epoch 3 Step 2/8 Loss: 2.4192\n",
      "Epoch 3 Step 3/8 Loss: 2.0367\n",
      "Epoch 3 Step 4/8 Loss: 2.5874\n",
      "Epoch 3 Step 5/8 Loss: 3.3435\n",
      "Epoch 3 Step 6/8 Loss: 2.1702\n",
      "Epoch 3 Step 7/8 Loss: 2.9626\n",
      "Epoch 3 Step 8/8 Loss: 2.2644\n",
      "Epoch 3 Step 1/8 Loss: 2.2351\n",
      "Epoch 3 Step 2/8 Loss: 5.1839\n",
      "Epoch 3 Step 3/8 Loss: 2.2635\n",
      "Epoch 3 Step 4/8 Loss: 2.3097\n",
      "Epoch 3 Step 5/8 Loss: 1.5062\n",
      "Epoch 3 Step 6/8 Loss: 5.2057\n",
      "Epoch 3 Step 7/8 Loss: 2.7957\n",
      "Epoch 3 Step 8/8 Loss: 1.3232\n",
      "Epoch 3 Step 1/8 Loss: 3.0352\n",
      "Epoch 3 Step 2/8 Loss: 3.3618\n",
      "Epoch 3 Step 3/8 Loss: 1.7478\n",
      "Epoch 3 Step 4/8 Loss: 6.6899\n",
      "Epoch 3 Step 5/8 Loss: 3.0298\n",
      "Epoch 3 Step 6/8 Loss: 2.2833\n",
      "Epoch 3 Step 7/8 Loss: 1.0811\n",
      "Epoch 3 Step 8/8 Loss: 1.5368\n",
      "Epoch 3 Step 1/8 Loss: 5.0283\n",
      "Epoch 3 Step 2/8 Loss: 2.2515\n",
      "Epoch 3 Step 3/8 Loss: 2.2785\n",
      "Epoch 3 Step 4/8 Loss: 1.8163\n",
      "Epoch 3 Step 5/8 Loss: 3.6338\n",
      "Epoch 3 Step 6/8 Loss: 2.3455\n",
      "Epoch 3 Step 7/8 Loss: 2.0772\n",
      "Epoch 3 Step 8/8 Loss: 2.6310\n",
      "Epoch 3 Step 1/8 Loss: 1.1696\n",
      "Epoch 3 Step 2/8 Loss: 3.5267\n",
      "Epoch 3 Step 3/8 Loss: 2.6122\n",
      "Epoch 3 Step 4/8 Loss: 3.2285\n",
      "Epoch 3 Step 5/8 Loss: 3.1971\n",
      "Epoch 3 Step 6/8 Loss: 3.8921\n",
      "Epoch 3 Step 7/8 Loss: 1.0009\n",
      "Epoch 3 Step 8/8 Loss: 2.0122\n",
      "Epoch 3 Step 1/8 Loss: 4.5980\n",
      "Epoch 3 Step 2/8 Loss: 1.4690\n",
      "Epoch 3 Step 3/8 Loss: 3.6884\n",
      "Epoch 3 Step 4/8 Loss: 2.7336\n",
      "Epoch 3 Step 5/8 Loss: 1.9960\n",
      "Epoch 3 Step 6/8 Loss: 2.6280\n",
      "Epoch 3 Step 7/8 Loss: 1.2549\n",
      "Epoch 3 Step 8/8 Loss: 1.1101\n",
      "Epoch 3 Step 1/8 Loss: 3.6571\n",
      "Epoch 3 Step 2/8 Loss: 1.8538\n",
      "Epoch 3 Step 3/8 Loss: 3.7376\n",
      "Epoch 3 Step 4/8 Loss: 1.4217\n",
      "Epoch 3 Step 5/8 Loss: 1.0827\n",
      "Epoch 3 Step 6/8 Loss: 1.9936\n",
      "Epoch 3 Step 7/8 Loss: 1.7799\n",
      "Epoch 3 Step 8/8 Loss: 4.7798\n",
      "Epoch 3 Step 1/8 Loss: 2.1981\n",
      "Epoch 3 Step 2/8 Loss: 2.8038\n",
      "Epoch 3 Step 3/8 Loss: 3.0582\n",
      "Epoch 3 Step 4/8 Loss: 2.1924\n",
      "Epoch 3 Step 5/8 Loss: 2.2529\n",
      "Epoch 3 Step 6/8 Loss: 1.8205\n",
      "Epoch 3 Step 7/8 Loss: 3.4976\n",
      "Epoch 3 Step 8/8 Loss: 3.1819\n",
      "Epoch 3 Step 1/8 Loss: 3.7642\n",
      "Epoch 3 Step 2/8 Loss: 1.4672\n",
      "Epoch 3 Step 3/8 Loss: 2.5501\n",
      "Epoch 3 Step 4/8 Loss: 2.3319\n",
      "Epoch 3 Step 5/8 Loss: 2.5901\n",
      "Epoch 3 Step 6/8 Loss: 1.5692\n",
      "Epoch 3 Step 7/8 Loss: 3.6230\n",
      "Epoch 3 Step 8/8 Loss: 3.4046\n",
      "Epoch 3 Step 1/8 Loss: 2.6872\n",
      "Epoch 3 Step 2/8 Loss: 3.0007\n",
      "Epoch 3 Step 3/8 Loss: 1.5544\n",
      "Epoch 3 Step 4/8 Loss: 2.3961\n",
      "Epoch 3 Step 5/8 Loss: 2.0231\n",
      "Epoch 3 Step 6/8 Loss: 2.1972\n",
      "Epoch 3 Step 7/8 Loss: 3.5839\n",
      "Epoch 3 Step 8/8 Loss: 1.2396\n",
      "Epoch 3 Step 1/8 Loss: 2.5549\n",
      "Epoch 3 Step 2/8 Loss: 2.1799\n",
      "Epoch 3 Step 3/8 Loss: 2.1891\n",
      "Epoch 3 Step 4/8 Loss: 4.0212\n",
      "Epoch 3 Step 5/8 Loss: 1.1111\n",
      "Epoch 3 Step 6/8 Loss: 2.5067\n",
      "Epoch 3 Step 7/8 Loss: 2.5219\n",
      "Epoch 3 Step 8/8 Loss: 2.0472\n",
      "Epoch 3 Step 1/8 Loss: 2.2877\n",
      "Epoch 3 Step 2/8 Loss: 1.3237\n",
      "Epoch 3 Step 3/8 Loss: 2.9917\n",
      "Epoch 3 Step 4/8 Loss: 2.2115\n",
      "Epoch 3 Step 5/8 Loss: 1.3463\n",
      "Epoch 3 Step 6/8 Loss: 3.0249\n",
      "Epoch 3 Step 7/8 Loss: 2.0102\n",
      "Epoch 3 Step 8/8 Loss: 3.0946\n",
      "Epoch 3 Step 1/8 Loss: 4.8080\n",
      "Epoch 3 Step 2/8 Loss: 2.0192\n",
      "Epoch 3 Step 3/8 Loss: 1.2022\n",
      "Epoch 3 Step 4/8 Loss: 1.5160\n",
      "Epoch 3 Step 5/8 Loss: 3.0542\n",
      "Epoch 3 Step 6/8 Loss: 3.5533\n",
      "Epoch 3 Step 7/8 Loss: 3.6643\n",
      "Epoch 3 Step 8/8 Loss: 3.3105\n",
      "Epoch 3 Step 1/8 Loss: 2.1665\n",
      "Epoch 3 Step 2/8 Loss: 1.9388\n",
      "Epoch 3 Step 3/8 Loss: 2.2681\n",
      "Epoch 3 Step 4/8 Loss: 2.1291\n",
      "Epoch 3 Step 5/8 Loss: 1.9893\n",
      "Epoch 3 Step 6/8 Loss: 2.2606\n",
      "Epoch 3 Step 7/8 Loss: 3.5380\n",
      "Epoch 3 Step 8/8 Loss: 3.9318\n",
      "Epoch 3 Step 1/8 Loss: 2.0407\n",
      "Epoch 3 Step 2/8 Loss: 3.1309\n",
      "Epoch 3 Step 3/8 Loss: 3.4646\n",
      "Epoch 3 Step 4/8 Loss: 1.9657\n",
      "Epoch 3 Step 5/8 Loss: 1.0159\n",
      "Epoch 3 Step 6/8 Loss: 1.8247\n",
      "Epoch 3 Step 7/8 Loss: 4.3803\n",
      "Epoch 3 Step 8/8 Loss: 2.6884\n",
      "Epoch 3 Step 1/8 Loss: 4.2316\n",
      "Epoch 3 Step 2/8 Loss: 3.9431\n",
      "Epoch 3 Step 3/8 Loss: 2.8141\n",
      "Epoch 3 Step 4/8 Loss: 1.8322\n",
      "Epoch 3 Step 5/8 Loss: 4.0387\n",
      "Epoch 3 Step 6/8 Loss: 3.1000\n",
      "Epoch 3 Step 7/8 Loss: 1.0707\n",
      "Epoch 3 Step 8/8 Loss: 1.0128\n",
      "Epoch 3 Step 1/8 Loss: 1.5143\n",
      "Epoch 3 Step 2/8 Loss: 3.2444\n",
      "Epoch 3 Step 3/8 Loss: 1.2041\n",
      "Epoch 3 Step 4/8 Loss: 2.7480\n",
      "Epoch 3 Step 5/8 Loss: 3.8145\n",
      "Epoch 3 Step 6/8 Loss: 1.7033\n",
      "Epoch 3 Step 7/8 Loss: 2.6542\n",
      "Epoch 3 Step 8/8 Loss: 2.5056\n",
      "Epoch 3 Step 1/8 Loss: 2.4434\n",
      "Epoch 3 Step 2/8 Loss: 4.1590\n",
      "Epoch 3 Step 3/8 Loss: 1.2726\n",
      "Epoch 3 Step 4/8 Loss: 2.4605\n",
      "Epoch 3 Step 5/8 Loss: 3.4277\n",
      "Epoch 3 Step 6/8 Loss: 2.1175\n",
      "Epoch 3 Step 7/8 Loss: 3.1477\n",
      "Epoch 3 Step 8/8 Loss: 2.4218\n",
      "Epoch 3 Step 1/8 Loss: 2.2299\n",
      "Epoch 3 Step 2/8 Loss: 3.3382\n",
      "Epoch 3 Step 3/8 Loss: 1.7019\n",
      "Epoch 3 Step 4/8 Loss: 2.2266\n",
      "Epoch 3 Step 5/8 Loss: 1.9548\n",
      "Epoch 3 Step 6/8 Loss: 0.9019\n",
      "Epoch 3 Step 7/8 Loss: 2.3011\n",
      "Epoch 3 Step 8/8 Loss: 2.5184\n",
      "Epoch 3 Step 1/8 Loss: 1.1138\n",
      "Epoch 3 Step 2/8 Loss: 1.8537\n",
      "Epoch 3 Step 3/8 Loss: 2.4426\n",
      "Epoch 3 Step 4/8 Loss: 4.0138\n",
      "Epoch 3 Step 5/8 Loss: 4.2754\n",
      "Epoch 3 Step 6/8 Loss: 2.0291\n",
      "Epoch 3 Step 7/8 Loss: 1.8827\n",
      "Epoch 3 Step 8/8 Loss: 0.9251\n",
      "Epoch 3 Step 1/8 Loss: 2.0082\n",
      "Epoch 3 Step 2/8 Loss: 2.4952\n",
      "Epoch 3 Step 3/8 Loss: 2.0135\n",
      "Epoch 3 Step 4/8 Loss: 2.1349\n",
      "Epoch 3 Step 5/8 Loss: 2.4963\n",
      "Epoch 3 Step 6/8 Loss: 2.3743\n",
      "Epoch 3 Step 7/8 Loss: 3.1398\n",
      "Epoch 3 Step 8/8 Loss: 1.9931\n",
      "Epoch 3 Step 1/8 Loss: 3.4287\n",
      "Epoch 3 Step 2/8 Loss: 3.1125\n",
      "Epoch 3 Step 3/8 Loss: 1.8045\n",
      "Epoch 3 Step 4/8 Loss: 5.0808\n",
      "Epoch 3 Step 5/8 Loss: 2.1375\n",
      "Epoch 3 Step 6/8 Loss: 1.3117\n",
      "Epoch 3 Step 7/8 Loss: 3.6994\n",
      "Epoch 3 Step 8/8 Loss: 2.7443\n",
      "Epoch 3 Step 1/8 Loss: 3.4653\n",
      "Epoch 3 Step 2/8 Loss: 2.1263\n",
      "Epoch 3 Step 3/8 Loss: 2.5848\n",
      "Epoch 3 Step 4/8 Loss: 4.8401\n",
      "Epoch 3 Step 5/8 Loss: 1.7369\n",
      "Epoch 3 Step 6/8 Loss: 3.5844\n",
      "Epoch 3 Step 7/8 Loss: 1.2164\n",
      "Epoch 3 Step 8/8 Loss: 3.5668\n",
      "Epoch 3 Step 1/8 Loss: 2.7915\n",
      "Epoch 3 Step 2/8 Loss: 2.3972\n",
      "Epoch 3 Step 3/8 Loss: 1.8139\n",
      "Epoch 3 Step 4/8 Loss: 2.1530\n",
      "Epoch 3 Step 5/8 Loss: 2.4266\n",
      "Epoch 3 Step 6/8 Loss: 1.3644\n",
      "Epoch 3 Step 7/8 Loss: 2.1935\n",
      "Epoch 3 Step 8/8 Loss: 2.5850\n",
      "Epoch 3 Step 1/8 Loss: 2.3040\n",
      "Epoch 3 Step 2/8 Loss: 3.2154\n",
      "Epoch 3 Step 3/8 Loss: 2.3343\n",
      "Epoch 3 Step 4/8 Loss: 0.9263\n",
      "Epoch 3 Step 5/8 Loss: 1.7953\n",
      "Epoch 3 Step 6/8 Loss: 3.6399\n",
      "Epoch 3 Step 7/8 Loss: 2.1909\n",
      "Epoch 3 Step 8/8 Loss: 2.8315\n",
      "Epoch 3 Step 1/8 Loss: 4.8433\n",
      "Epoch 3 Step 2/8 Loss: 3.1182\n",
      "Epoch 3 Step 3/8 Loss: 1.9758\n",
      "Epoch 3 Step 4/8 Loss: 1.0478\n",
      "Epoch 3 Step 5/8 Loss: 1.1937\n",
      "Epoch 3 Step 6/8 Loss: 3.9790\n",
      "Epoch 3 Step 7/8 Loss: 3.3387\n",
      "Epoch 3 Step 8/8 Loss: 2.1099\n",
      "Epoch 3 Step 1/8 Loss: 0.8493\n",
      "Epoch 3 Step 2/8 Loss: 3.1956\n",
      "Epoch 3 Step 3/8 Loss: 3.0713\n",
      "Epoch 3 Step 4/8 Loss: 2.0950\n",
      "Epoch 3 Step 5/8 Loss: 1.9553\n",
      "Epoch 3 Step 6/8 Loss: 3.7370\n",
      "Epoch 3 Step 7/8 Loss: 1.0372\n",
      "Epoch 3 Step 8/8 Loss: 3.7118\n",
      "Epoch 3 Step 1/8 Loss: 3.0940\n",
      "Epoch 3 Step 2/8 Loss: 1.1637\n",
      "Epoch 3 Step 3/8 Loss: 1.8665\n",
      "Epoch 3 Step 4/8 Loss: 3.0491\n",
      "Epoch 3 Step 5/8 Loss: 1.3731\n",
      "Epoch 3 Step 6/8 Loss: 3.2614\n",
      "Epoch 3 Step 7/8 Loss: 2.7976\n",
      "Epoch 3 Step 8/8 Loss: 1.5424\n",
      "Epoch 3 Step 1/8 Loss: 2.8450\n",
      "Epoch 3 Step 2/8 Loss: 3.4345\n",
      "Epoch 3 Step 3/8 Loss: 2.3090\n",
      "Epoch 3 Step 4/8 Loss: 2.8199\n",
      "Epoch 3 Step 5/8 Loss: 2.3639\n",
      "Epoch 3 Step 6/8 Loss: 2.6562\n",
      "Epoch 3 Step 7/8 Loss: 4.6006\n",
      "Epoch 3 Step 8/8 Loss: 3.0157\n",
      "Epoch 3 Step 1/8 Loss: 4.0543\n",
      "Epoch 3 Step 2/8 Loss: 2.9041\n",
      "Epoch 3 Step 3/8 Loss: 2.0992\n",
      "Epoch 3 Step 4/8 Loss: 1.6580\n",
      "Epoch 3 Step 5/8 Loss: 1.4498\n",
      "Epoch 3 Step 6/8 Loss: 2.6047\n",
      "Epoch 3 Step 7/8 Loss: 2.3185\n",
      "Epoch 3 Step 8/8 Loss: 3.5623\n",
      "Epoch 3 Step 1/8 Loss: 2.8962\n",
      "Epoch 3 Step 2/8 Loss: 3.7455\n",
      "Epoch 3 Step 3/8 Loss: 4.0616\n",
      "Epoch 3 Step 4/8 Loss: 2.2246\n",
      "Epoch 3 Step 5/8 Loss: 1.2234\n",
      "Epoch 3 Step 6/8 Loss: 2.2958\n",
      "Epoch 3 Step 7/8 Loss: 3.2839\n",
      "Epoch 3 Step 8/8 Loss: 4.5870\n",
      "Epoch 3 Step 1/8 Loss: 4.3770\n",
      "Epoch 3 Step 2/8 Loss: 2.7837\n",
      "Epoch 3 Step 3/8 Loss: 3.2339\n",
      "Epoch 3 Step 4/8 Loss: 2.8425\n",
      "Epoch 3 Step 5/8 Loss: 1.4166\n",
      "Epoch 3 Step 6/8 Loss: 2.0997\n",
      "Epoch 3 Step 7/8 Loss: 2.2644\n",
      "Epoch 3 Step 8/8 Loss: 1.3907\n",
      "Epoch 3 Step 1/8 Loss: 3.0992\n",
      "Epoch 3 Step 2/8 Loss: 2.9413\n",
      "Epoch 3 Step 3/8 Loss: 3.3384\n",
      "Epoch 3 Step 4/8 Loss: 3.9154\n",
      "Epoch 3 Step 5/8 Loss: 2.9629\n",
      "Epoch 3 Step 6/8 Loss: 3.4600\n",
      "Epoch 3 Step 7/8 Loss: 2.1464\n",
      "Epoch 3 Step 8/8 Loss: 1.3099\n",
      "Epoch 3 Step 1/8 Loss: 1.7778\n",
      "Epoch 3 Step 2/8 Loss: 2.3921\n",
      "Epoch 3 Step 3/8 Loss: 2.6230\n",
      "Epoch 3 Step 4/8 Loss: 3.0928\n",
      "Epoch 3 Step 5/8 Loss: 2.7176\n",
      "Epoch 3 Step 6/8 Loss: 1.5770\n",
      "Epoch 3 Step 7/8 Loss: 1.6576\n",
      "Epoch 3 Step 8/8 Loss: 3.1965\n",
      "Epoch 3 Step 1/8 Loss: 1.9430\n",
      "Epoch 3 Step 2/8 Loss: 3.2649\n",
      "Epoch 3 Step 3/8 Loss: 1.5257\n",
      "Epoch 3 Step 4/8 Loss: 4.8188\n",
      "Epoch 3 Step 5/8 Loss: 3.0764\n",
      "Epoch 3 Step 6/8 Loss: 2.6225\n",
      "Epoch 3 Step 7/8 Loss: 2.3306\n",
      "Epoch 3 Step 8/8 Loss: 3.7088\n",
      "Epoch 3 Step 1/8 Loss: 2.6500\n",
      "Epoch 3 Step 2/8 Loss: 2.3863\n",
      "Epoch 3 Step 3/8 Loss: 2.5944\n",
      "Epoch 3 Step 4/8 Loss: 1.1524\n",
      "Epoch 3 Step 5/8 Loss: 2.5167\n",
      "Epoch 3 Step 6/8 Loss: 2.1945\n",
      "Epoch 3 Step 7/8 Loss: 3.9105\n",
      "Epoch 3 Step 8/8 Loss: 2.3562\n",
      "Epoch 3 Step 1/8 Loss: 3.6285\n",
      "Epoch 3 Step 2/8 Loss: 2.5213\n",
      "Epoch 3 Step 3/8 Loss: 4.0072\n",
      "Epoch 3 Step 4/8 Loss: 3.1440\n",
      "Epoch 3 Step 5/8 Loss: 1.4007\n",
      "Epoch 3 Step 6/8 Loss: 2.4048\n",
      "Epoch 3 Step 7/8 Loss: 1.6332\n",
      "Epoch 3 Step 8/8 Loss: 2.8923\n",
      "Epoch 3 Step 1/8 Loss: 2.9152\n",
      "Epoch 3 Step 2/8 Loss: 4.6608\n",
      "Epoch 3 Step 3/8 Loss: 1.8930\n",
      "Epoch 3 Step 4/8 Loss: 3.1407\n",
      "Epoch 3 Step 5/8 Loss: 3.3560\n",
      "Epoch 3 Step 6/8 Loss: 1.9691\n",
      "Epoch 3 Step 7/8 Loss: 1.1469\n",
      "Epoch 3 Step 8/8 Loss: 3.3986\n",
      "Epoch 3 Step 1/8 Loss: 2.1482\n",
      "Epoch 3 Step 2/8 Loss: 1.1810\n",
      "Epoch 3 Step 3/8 Loss: 1.4915\n",
      "Epoch 3 Step 4/8 Loss: 5.2887\n",
      "Epoch 3 Step 5/8 Loss: 2.7008\n",
      "Epoch 3 Step 6/8 Loss: 2.2045\n",
      "Epoch 3 Step 7/8 Loss: 2.1394\n",
      "Epoch 3 Step 8/8 Loss: 4.6286\n",
      "Epoch 3 Step 1/8 Loss: 4.2766\n",
      "Epoch 3 Step 2/8 Loss: 2.7499\n",
      "Epoch 3 Step 3/8 Loss: 3.5643\n",
      "Epoch 3 Step 4/8 Loss: 1.4439\n",
      "Epoch 3 Step 5/8 Loss: 2.9813\n",
      "Epoch 3 Step 6/8 Loss: 2.3673\n",
      "Epoch 3 Step 7/8 Loss: 3.8082\n",
      "Epoch 3 Step 8/8 Loss: 2.2805\n",
      "Epoch 3 Step 1/8 Loss: 2.7139\n",
      "Epoch 3 Step 2/8 Loss: 2.7432\n",
      "Epoch 3 Step 3/8 Loss: 2.0243\n",
      "Epoch 3 Step 4/8 Loss: 2.3680\n",
      "Epoch 3 Step 5/8 Loss: 4.2296\n",
      "Epoch 3 Step 6/8 Loss: 2.5250\n",
      "Epoch 3 Step 7/8 Loss: 2.6369\n",
      "Epoch 3 Step 8/8 Loss: 2.1476\n",
      "Epoch 3 Step 1/8 Loss: 2.6701\n",
      "Epoch 3 Step 2/8 Loss: 2.9480\n",
      "Epoch 3 Step 3/8 Loss: 2.2911\n",
      "Epoch 3 Step 4/8 Loss: 2.0318\n",
      "Epoch 3 Step 5/8 Loss: 3.0086\n",
      "Epoch 3 Step 6/8 Loss: 2.6546\n",
      "Epoch 3 Step 7/8 Loss: 3.1885\n",
      "Epoch 3 Step 8/8 Loss: 1.3662\n",
      "Epoch 3 Step 1/8 Loss: 3.8010\n",
      "Epoch 3 Step 2/8 Loss: 2.0580\n",
      "Epoch 3 Step 3/8 Loss: 3.7367\n",
      "Epoch 3 Step 4/8 Loss: 1.1206\n",
      "Epoch 3 Step 5/8 Loss: 2.6969\n",
      "Epoch 3 Step 6/8 Loss: 2.4207\n",
      "Epoch 3 Step 7/8 Loss: 3.7298\n",
      "Epoch 3 Step 8/8 Loss: 2.2950\n",
      "Epoch 3 Step 1/8 Loss: 2.6039\n",
      "Epoch 3 Step 2/8 Loss: 1.8518\n",
      "Epoch 3 Step 3/8 Loss: 2.7105\n",
      "Epoch 3 Step 4/8 Loss: 4.6033\n",
      "Epoch 3 Step 5/8 Loss: 3.8465\n",
      "Epoch 3 Step 6/8 Loss: 1.8874\n",
      "Epoch 3 Step 7/8 Loss: 2.6026\n",
      "Epoch 3 Step 8/8 Loss: 2.4718\n",
      "Epoch 3 completed. Average Loss: nan\n",
      "Epoch 4 Step 1/8 Loss: 2.8911\n",
      "Epoch 4 Step 2/8 Loss: 2.9829\n",
      "Epoch 4 Step 3/8 Loss: 2.2084\n",
      "Epoch 4 Step 4/8 Loss: 2.3555\n",
      "Epoch 4 Step 5/8 Loss: 1.5723\n",
      "Epoch 4 Step 6/8 Loss: 2.7139\n",
      "Epoch 4 Step 7/8 Loss: 2.5594\n",
      "Epoch 4 Step 8/8 Loss: 1.9158\n",
      "Epoch 4 Step 1/8 Loss: 3.2962\n",
      "Epoch 4 Step 2/8 Loss: 2.3308\n",
      "Epoch 4 Step 3/8 Loss: 2.3858\n",
      "Epoch 4 Step 4/8 Loss: 2.8079\n",
      "Epoch 4 Step 5/8 Loss: 3.0553\n",
      "Epoch 4 Step 6/8 Loss: 1.3071\n",
      "Epoch 4 Step 7/8 Loss: 2.3697\n",
      "Epoch 4 Step 8/8 Loss: 2.5949\n",
      "Epoch 4 Step 1/8 Loss: 1.1567\n",
      "Epoch 4 Step 2/8 Loss: 3.1246\n",
      "Epoch 4 Step 3/8 Loss: 4.6286\n",
      "Epoch 4 Step 4/8 Loss: 3.2650\n",
      "Epoch 4 Step 5/8 Loss: 1.5900\n",
      "Epoch 4 Step 6/8 Loss: 2.8480\n",
      "Epoch 4 Step 7/8 Loss: 2.6229\n",
      "Epoch 4 Step 8/8 Loss: 2.1585\n",
      "Epoch 4 Step 1/8 Loss: 2.2844\n",
      "Epoch 4 Step 2/8 Loss: 2.9174\n",
      "Epoch 4 Step 3/8 Loss: 3.2595\n",
      "Epoch 4 Step 4/8 Loss: 3.9355\n",
      "Epoch 4 Step 5/8 Loss: 2.6873\n",
      "Epoch 4 Step 6/8 Loss: 2.2030\n",
      "Epoch 4 Step 7/8 Loss: 4.4137\n",
      "Epoch 4 Step 8/8 Loss: 1.6258\n",
      "Epoch 4 Step 1/8 Loss: 2.6826\n",
      "Epoch 4 Step 2/8 Loss: 3.6350\n",
      "Epoch 4 Step 3/8 Loss: 1.1797\n",
      "Epoch 4 Step 4/8 Loss: 2.1366\n",
      "Epoch 4 Step 5/8 Loss: 2.4785\n",
      "Epoch 4 Step 6/8 Loss: 1.7597\n",
      "Epoch 4 Step 7/8 Loss: 3.1954\n",
      "Epoch 4 Step 8/8 Loss: 2.8273\n",
      "Epoch 4 Step 1/8 Loss: 2.2107\n",
      "Epoch 4 Step 2/8 Loss: 2.4151\n",
      "Epoch 4 Step 3/8 Loss: 2.8827\n",
      "Epoch 4 Step 4/8 Loss: 1.9889\n",
      "Epoch 4 Step 5/8 Loss: 1.5645\n",
      "Epoch 4 Step 6/8 Loss: 2.9765\n",
      "Epoch 4 Step 7/8 Loss: 2.7005\n",
      "Epoch 4 Step 8/8 Loss: 1.8531\n",
      "Epoch 4 Step 1/8 Loss: 2.8355\n",
      "Epoch 4 Step 2/8 Loss: 2.1018\n",
      "Epoch 4 Step 3/8 Loss: 3.8627\n",
      "Epoch 4 Step 4/8 Loss: 1.2497\n",
      "Epoch 4 Step 5/8 Loss: 2.0303\n",
      "Epoch 4 Step 6/8 Loss: 1.9488\n",
      "Epoch 4 Step 7/8 Loss: 3.5420\n",
      "Epoch 4 Step 8/8 Loss: 2.1458\n",
      "Epoch 4 Step 1/8 Loss: 3.6047\n",
      "Epoch 4 Step 2/8 Loss: 1.3189\n",
      "Epoch 4 Step 3/8 Loss: 2.2308\n",
      "Epoch 4 Step 4/8 Loss: 2.9875\n",
      "Epoch 4 Step 5/8 Loss: 3.2895\n",
      "Epoch 4 Step 6/8 Loss: 2.7598\n",
      "Epoch 4 Step 7/8 Loss: 2.0425\n",
      "Epoch 4 Step 8/8 Loss: 2.0543\n",
      "Epoch 4 Step 1/8 Loss: 2.4518\n",
      "Epoch 4 Step 2/8 Loss: 3.3308\n",
      "Epoch 4 Step 3/8 Loss: 2.7365\n",
      "Epoch 4 Step 4/8 Loss: 2.7023\n",
      "Epoch 4 Step 5/8 Loss: 1.2052\n",
      "Epoch 4 Step 6/8 Loss: 2.7983\n",
      "Epoch 4 Step 7/8 Loss: 2.1018\n",
      "Epoch 4 Step 8/8 Loss: 2.2626\n",
      "Epoch 4 Step 1/8 Loss: 2.0589\n",
      "Epoch 4 Step 2/8 Loss: 2.6491\n",
      "Epoch 4 Step 3/8 Loss: 1.6998\n",
      "Epoch 4 Step 4/8 Loss: 2.3517\n",
      "Epoch 4 Step 5/8 Loss: 2.6869\n",
      "Epoch 4 Step 6/8 Loss: 1.4744\n",
      "Epoch 4 Step 7/8 Loss: 2.7006\n",
      "Epoch 4 Step 8/8 Loss: 2.0056\n",
      "Epoch 4 Step 1/8 Loss: 1.0487\n",
      "Epoch 4 Step 2/8 Loss: 3.1161\n",
      "Epoch 4 Step 3/8 Loss: 4.9653\n",
      "Epoch 4 Step 4/8 Loss: 2.0996\n",
      "Epoch 4 Step 5/8 Loss: 2.5524\n",
      "Epoch 4 Step 6/8 Loss: 2.5023\n",
      "Epoch 4 Step 7/8 Loss: 2.2452\n",
      "Epoch 4 Step 8/8 Loss: 2.9401\n",
      "Epoch 4 Step 1/8 Loss: 1.4366\n",
      "Epoch 4 Step 2/8 Loss: 3.4102\n",
      "Epoch 4 Step 3/8 Loss: 3.6732\n",
      "Epoch 4 Step 4/8 Loss: 2.4564\n",
      "Epoch 4 Step 5/8 Loss: 1.2793\n",
      "Epoch 4 Step 6/8 Loss: 2.5271\n",
      "Epoch 4 Step 7/8 Loss: 4.6769\n",
      "Epoch 4 Step 8/8 Loss: 2.7392\n",
      "Epoch 4 Step 1/8 Loss: 3.7351\n",
      "Epoch 4 Step 2/8 Loss: 2.6435\n",
      "Epoch 4 Step 3/8 Loss: 1.2075\n",
      "Epoch 4 Step 4/8 Loss: 1.2108\n",
      "Epoch 4 Step 5/8 Loss: 3.2621\n",
      "Epoch 4 Step 6/8 Loss: 3.8444\n",
      "Epoch 4 Step 7/8 Loss: 1.9840\n",
      "Epoch 4 Step 8/8 Loss: 2.2521\n",
      "Epoch 4 Step 1/8 Loss: 3.2322\n",
      "Epoch 4 Step 2/8 Loss: 2.6654\n",
      "Epoch 4 Step 3/8 Loss: 2.1549\n",
      "Epoch 4 Step 4/8 Loss: 2.1040\n",
      "Epoch 4 Step 5/8 Loss: 4.3807\n",
      "Epoch 4 Step 6/8 Loss: 2.5179\n",
      "Epoch 4 Step 7/8 Loss: 2.3954\n",
      "Epoch 4 Step 8/8 Loss: 3.5007\n",
      "Epoch 4 Step 1/8 Loss: 1.9082\n",
      "Epoch 4 Step 2/8 Loss: 2.6532\n",
      "Epoch 4 Step 3/8 Loss: 2.3527\n",
      "Epoch 4 Step 4/8 Loss: 2.8945\n",
      "Epoch 4 Step 5/8 Loss: 1.7099\n",
      "Epoch 4 Step 6/8 Loss: 2.8558\n",
      "Epoch 4 Step 7/8 Loss: 2.6929\n",
      "Epoch 4 Step 8/8 Loss: 2.9031\n",
      "Epoch 4 Step 1/8 Loss: 2.3747\n",
      "Epoch 4 Step 2/8 Loss: 1.9565\n",
      "Epoch 4 Step 3/8 Loss: 2.6117\n",
      "Epoch 4 Step 4/8 Loss: 2.2597\n",
      "Epoch 4 Step 5/8 Loss: 3.1960\n",
      "Epoch 4 Step 6/8 Loss: 2.4884\n",
      "Epoch 4 Step 7/8 Loss: 3.3897\n",
      "Epoch 4 Step 8/8 Loss: 3.0862\n",
      "Epoch 4 Step 1/8 Loss: 2.1123\n",
      "Epoch 4 Step 2/8 Loss: 2.0028\n",
      "Epoch 4 Step 3/8 Loss: 3.2790\n",
      "Epoch 4 Step 4/8 Loss: 2.2207\n",
      "Epoch 4 Step 5/8 Loss: 2.4174\n",
      "Epoch 4 Step 6/8 Loss: 2.2267\n",
      "Epoch 4 Step 7/8 Loss: 4.3834\n",
      "Epoch 4 Step 8/8 Loss: 4.0296\n",
      "Epoch 4 Step 1/8 Loss: 2.6544\n",
      "Epoch 4 Step 2/8 Loss: 1.4482\n",
      "Epoch 4 Step 3/8 Loss: 2.4453\n",
      "Epoch 4 Step 4/8 Loss: 2.4080\n",
      "Epoch 4 Step 5/8 Loss: 4.1584\n",
      "Epoch 4 Step 6/8 Loss: 4.5295\n",
      "Epoch 4 Step 7/8 Loss: 3.0471\n",
      "Epoch 4 Step 8/8 Loss: 2.3531\n",
      "Epoch 4 Step 1/8 Loss: 4.4842\n",
      "Epoch 4 Step 2/8 Loss: 1.0990\n",
      "Epoch 4 Step 3/8 Loss: 3.2491\n",
      "Epoch 4 Step 4/8 Loss: 2.3851\n",
      "Epoch 4 Step 5/8 Loss: 1.6870\n",
      "Epoch 4 Step 6/8 Loss: 3.7331\n",
      "Epoch 4 Step 7/8 Loss: 3.4120\n",
      "Epoch 4 Step 8/8 Loss: 2.1507\n",
      "Epoch 4 Step 1/8 Loss: 3.3530\n",
      "Epoch 4 Step 2/8 Loss: 3.2385\n",
      "Epoch 4 Step 3/8 Loss: 0.9931\n",
      "Epoch 4 Step 4/8 Loss: 2.6412\n",
      "Epoch 4 Step 5/8 Loss: 3.0087\n",
      "Epoch 4 Step 6/8 Loss: 3.7143\n",
      "Epoch 4 Step 7/8 Loss: 4.4011\n",
      "Epoch 4 Step 8/8 Loss: 1.3841\n",
      "Epoch 4 Step 1/8 Loss: 2.1164\n",
      "Epoch 4 Step 2/8 Loss: 1.9310\n",
      "Epoch 4 Step 3/8 Loss: 3.3429\n",
      "Epoch 4 Step 4/8 Loss: 5.0124\n",
      "Epoch 4 Step 5/8 Loss: 2.4705\n",
      "Epoch 4 Step 6/8 Loss: 1.3940\n",
      "Epoch 4 Step 7/8 Loss: 2.7772\n",
      "Epoch 4 Step 8/8 Loss: 2.5303\n",
      "Epoch 4 Step 1/8 Loss: 2.2115\n",
      "Epoch 4 Step 2/8 Loss: 2.4792\n",
      "Epoch 4 Step 3/8 Loss: 2.6423\n",
      "Epoch 4 Step 4/8 Loss: 3.0580\n",
      "Epoch 4 Step 5/8 Loss: 2.6607\n",
      "Epoch 4 Step 6/8 Loss: 4.0833\n",
      "Epoch 4 Step 7/8 Loss: 2.3299\n",
      "Epoch 4 Step 8/8 Loss: 2.3491\n",
      "Epoch 4 Step 1/8 Loss: 3.0458\n",
      "Epoch 4 Step 2/8 Loss: 2.3088\n",
      "Epoch 4 Step 3/8 Loss: 2.7365\n",
      "Epoch 4 Step 4/8 Loss: 2.1424\n",
      "Epoch 4 Step 5/8 Loss: 2.2236\n",
      "Epoch 4 Step 6/8 Loss: 1.2631\n",
      "Epoch 4 Step 7/8 Loss: 2.4881\n",
      "Epoch 4 Step 8/8 Loss: 2.9185\n",
      "Epoch 4 Step 1/8 Loss: 0.8346\n",
      "Epoch 4 Step 2/8 Loss: 3.9893\n",
      "Epoch 4 Step 3/8 Loss: 3.5623\n",
      "Epoch 4 Step 4/8 Loss: 1.8788\n",
      "Epoch 4 Step 5/8 Loss: 2.0079\n",
      "Epoch 4 Step 6/8 Loss: 2.7913\n",
      "Epoch 4 Step 7/8 Loss: 3.3460\n",
      "Epoch 4 Step 8/8 Loss: 2.3865\n",
      "Epoch 4 Step 1/8 Loss: 2.1050\n",
      "Epoch 4 Step 2/8 Loss: 3.3357\n",
      "Epoch 4 Step 3/8 Loss: 2.3864\n",
      "Epoch 4 Step 4/8 Loss: 2.7640\n",
      "Epoch 4 Step 5/8 Loss: 1.8706\n",
      "Epoch 4 Step 6/8 Loss: 1.2369\n",
      "Epoch 4 Step 7/8 Loss: 3.3546\n",
      "Epoch 4 Step 8/8 Loss: 1.7051\n",
      "Epoch 4 Step 1/8 Loss: 1.3269\n",
      "Epoch 4 Step 2/8 Loss: 3.5766\n",
      "Epoch 4 Step 3/8 Loss: 2.3813\n",
      "Epoch 4 Step 4/8 Loss: 2.0724\n",
      "Epoch 4 Step 5/8 Loss: 3.2321\n",
      "Epoch 4 Step 6/8 Loss: 2.7869\n",
      "Epoch 4 Step 7/8 Loss: 2.6632\n",
      "Epoch 4 Step 8/8 Loss: 2.2312\n",
      "Epoch 4 Step 1/8 Loss: 4.1095\n",
      "Epoch 4 Step 2/8 Loss: 3.1850\n",
      "Epoch 4 Step 3/8 Loss: 2.4437\n",
      "Epoch 4 Step 4/8 Loss: 3.3495\n",
      "Epoch 4 Step 5/8 Loss: 4.2584\n",
      "Epoch 4 Step 6/8 Loss: 1.8687\n",
      "Epoch 4 Step 7/8 Loss: 2.2150\n",
      "Epoch 4 Step 8/8 Loss: 3.7338\n",
      "Epoch 4 Step 1/8 Loss: 1.6417\n",
      "Epoch 4 Step 2/8 Loss: 3.5378\n",
      "Epoch 4 Step 3/8 Loss: 3.1514\n",
      "Epoch 4 Step 4/8 Loss: 2.4243\n",
      "Epoch 4 Step 5/8 Loss: 2.1495\n",
      "Epoch 4 Step 6/8 Loss: 3.6169\n",
      "Epoch 4 Step 7/8 Loss: 2.1286\n",
      "Epoch 4 Step 8/8 Loss: 2.4692\n",
      "Epoch 4 Step 1/8 Loss: 3.7496\n",
      "Epoch 4 Step 2/8 Loss: 1.8781\n",
      "Epoch 4 Step 3/8 Loss: 3.2876\n",
      "Epoch 4 Step 4/8 Loss: 2.5639\n",
      "Epoch 4 Step 5/8 Loss: 2.0416\n",
      "Epoch 4 Step 6/8 Loss: 3.1077\n",
      "Epoch 4 Step 7/8 Loss: 3.3566\n",
      "Epoch 4 Step 8/8 Loss: 4.4245\n",
      "Epoch 4 Step 1/8 Loss: 3.5052\n",
      "Epoch 4 Step 2/8 Loss: 2.1675\n",
      "Epoch 4 Step 3/8 Loss: 2.8341\n",
      "Epoch 4 Step 4/8 Loss: 1.3891\n",
      "Epoch 4 Step 5/8 Loss: 3.0950\n",
      "Epoch 4 Step 6/8 Loss: 1.1336\n",
      "Epoch 4 Step 7/8 Loss: 2.1176\n",
      "Epoch 4 Step 8/8 Loss: 2.9932\n",
      "Epoch 4 Step 1/8 Loss: 2.4157\n",
      "Epoch 4 Step 2/8 Loss: 2.8816\n",
      "Epoch 4 Step 3/8 Loss: 3.5290\n",
      "Epoch 4 Step 4/8 Loss: 2.5832\n",
      "Epoch 4 Step 5/8 Loss: 3.0362\n",
      "Epoch 4 Step 6/8 Loss: 2.3979\n",
      "Epoch 4 Step 7/8 Loss: 1.7389\n",
      "Epoch 4 Step 8/8 Loss: 3.0437\n",
      "Epoch 4 Step 1/8 Loss: 0.9954\n",
      "Epoch 4 Step 2/8 Loss: 3.6110\n",
      "Epoch 4 Step 3/8 Loss: 3.2302\n",
      "Epoch 4 Step 4/8 Loss: 4.6371\n",
      "Epoch 4 Step 5/8 Loss: 3.0774\n",
      "Epoch 4 Step 6/8 Loss: 2.3075\n",
      "Epoch 4 Step 7/8 Loss: 2.5063\n",
      "Epoch 4 Step 8/8 Loss: 2.0202\n",
      "Epoch 4 Step 1/8 Loss: 3.1553\n",
      "Epoch 4 Step 2/8 Loss: 2.1156\n",
      "Epoch 4 Step 3/8 Loss: 3.7620\n",
      "Epoch 4 Step 4/8 Loss: 3.7102\n",
      "Epoch 4 Step 5/8 Loss: 2.6745\n",
      "Epoch 4 Step 6/8 Loss: 2.6078\n",
      "Epoch 4 Step 7/8 Loss: 4.0690\n",
      "Epoch 4 Step 8/8 Loss: 3.4680\n",
      "Epoch 4 Step 1/8 Loss: 2.6707\n",
      "Epoch 4 Step 2/8 Loss: 2.6281\n",
      "Epoch 4 Step 3/8 Loss: 3.0658\n",
      "Epoch 4 Step 4/8 Loss: 2.5605\n",
      "Epoch 4 Step 5/8 Loss: 1.8929\n",
      "Epoch 4 Step 6/8 Loss: 3.7666\n",
      "Epoch 4 Step 7/8 Loss: 2.6010\n",
      "Epoch 4 Step 8/8 Loss: 2.1375\n",
      "Epoch 4 Step 1/8 Loss: 3.9820\n",
      "Epoch 4 Step 2/8 Loss: 2.4901\n",
      "Epoch 4 Step 3/8 Loss: 1.7170\n",
      "Epoch 4 Step 4/8 Loss: 3.5624\n",
      "Epoch 4 Step 5/8 Loss: 2.3952\n",
      "Epoch 4 Step 6/8 Loss: 3.8455\n",
      "Epoch 4 Step 7/8 Loss: 2.4168\n",
      "Epoch 4 Step 8/8 Loss: 2.2401\n",
      "Epoch 4 Step 1/8 Loss: 2.2532\n",
      "Epoch 4 Step 2/8 Loss: 1.3497\n",
      "Epoch 4 Step 3/8 Loss: 1.8323\n",
      "Epoch 4 Step 4/8 Loss: 2.2694\n",
      "Epoch 4 Step 5/8 Loss: 2.5554\n",
      "Epoch 4 Step 6/8 Loss: 1.9798\n",
      "Epoch 4 Step 7/8 Loss: 1.1745\n",
      "Epoch 4 Step 8/8 Loss: 5.3661\n",
      "Epoch 4 Step 1/8 Loss: 2.1682\n",
      "Epoch 4 Step 2/8 Loss: 2.3100\n",
      "Epoch 4 Step 3/8 Loss: 3.3805\n",
      "Epoch 4 Step 4/8 Loss: 1.7512\n",
      "Epoch 4 Step 5/8 Loss: 3.9012\n",
      "Epoch 4 Step 6/8 Loss: 2.4601\n",
      "Epoch 4 Step 7/8 Loss: 3.6854\n",
      "Epoch 4 Step 8/8 Loss: 3.4211\n",
      "Epoch 4 Step 1/8 Loss: 3.3455\n",
      "Epoch 4 Step 2/8 Loss: 1.8816\n",
      "Epoch 4 Step 3/8 Loss: 2.0826\n",
      "Epoch 4 Step 4/8 Loss: 1.2831\n",
      "Epoch 4 Step 5/8 Loss: 3.0194\n",
      "Epoch 4 Step 6/8 Loss: 2.9973\n",
      "Epoch 4 Step 7/8 Loss: 3.1992\n",
      "Epoch 4 Step 8/8 Loss: 1.0388\n",
      "Epoch 4 Step 1/8 Loss: 3.9525\n",
      "Epoch 4 Step 2/8 Loss: 3.2843\n",
      "Epoch 4 Step 3/8 Loss: 1.0311\n",
      "Epoch 4 Step 4/8 Loss: 1.8181\n",
      "Epoch 4 Step 5/8 Loss: 4.4715\n",
      "Epoch 4 Step 6/8 Loss: 3.0579\n",
      "Epoch 4 Step 7/8 Loss: 1.2126\n",
      "Epoch 4 Step 8/8 Loss: 2.0813\n",
      "Epoch 4 Step 1/8 Loss: 4.0473\n",
      "Epoch 4 Step 2/8 Loss: 2.1368\n",
      "Epoch 4 Step 3/8 Loss: 2.9747\n",
      "Epoch 4 Step 4/8 Loss: 1.8619\n",
      "Epoch 4 Step 5/8 Loss: 3.0621\n",
      "Epoch 4 Step 6/8 Loss: 1.1854\n",
      "Epoch 4 Step 7/8 Loss: 2.8321\n",
      "Epoch 4 Step 8/8 Loss: 3.9034\n",
      "Epoch 4 Step 1/8 Loss: 2.7739\n",
      "Epoch 4 Step 2/8 Loss: 1.3072\n",
      "Epoch 4 Step 3/8 Loss: 3.4306\n",
      "Epoch 4 Step 4/8 Loss: 2.0467\n",
      "Epoch 4 Step 5/8 Loss: 2.0427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 173\u001b[0m\n\u001b[1;32m    169\u001b[0m     combined_loss \u001b[38;5;241m=\u001b[39m ppo_loss \u001b[38;5;241m+\u001b[39m kl_lambda \u001b[38;5;241m*\u001b[39m kl_div\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Backward pass with scaled loss\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m    175\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Increased Sample Training Data (8 examples)\n",
    "# -------------------------------\n",
    "train_examples = [\n",
    "    {\"content\": \"def print_hello_world():\\n    print('Hello, world!')\"},\n",
    "    {\"content\": \"def add(a, b):\\n    return a + b\"},\n",
    "    {\"content\": \"def subtract(a, b):\\n    return a - b\"},\n",
    "    {\"content\": \"def multiply(a, b):\\n    return a * b\"},\n",
    "    {\"content\": (\n",
    "        \"def factorial(n):\\n\"\n",
    "        \"    if n == 0:\\n\"\n",
    "        \"        return 1\\n\"\n",
    "        \"    else:\\n\"\n",
    "        \"        return n * factorial(n - 1)\"\n",
    "    )},\n",
    "    {\"content\": (\n",
    "        \"def fibonacci(n):\\n\"\n",
    "        \"    a, b = 0, 1\\n\"\n",
    "        \"    result = []\\n\"\n",
    "        \"    for _ in range(n):\\n\"\n",
    "        \"        result.append(a)\\n\"\n",
    "        \"        a, b = b, a + b\\n\"\n",
    "        \"    return result\"\n",
    "    )},\n",
    "    {\"content\": \"def square(x):\\n    return x * x\"},\n",
    "    {\"content\": \"def cube(x):\\n    return x * x * x\"},\n",
    "]\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "\n",
    "# -------------------------------\n",
    "# Model & Tokenizer Setup\n",
    "# -------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Set pad token if not present (using EOS token as pad token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenization Function\n",
    "# -------------------------------\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",  # pad all examples to max_length\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# -------------------------------\n",
    "# Data Collator and DataLoader\n",
    "# -------------------------------\n",
    "# DataCollatorForLanguageModeling automatically creates a \"labels\" field equal to input_ids\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "batch_size = 1  # Increased batch size\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model & Set Training Mode\n",
    "# -------------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizer Setup with Adafactor\n",
    "# -------------------------------\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    lr=1e-5,               # Learning rate can be tuned\n",
    "    relative_step=False,   # Set to True to use relative step sizes\n",
    "    scale_parameter=False  # Adjust scaling based on model size\n",
    ")\n",
    "\n",
    "epsilon = 0.01\n",
    "\n",
    "# Setup AMP GradScaler for 16-bit training (float16)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a reference model (deep copy) and set it to eval mode.\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model = ref_model.half()\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "kl_lambda = 0.5  # Weight for the KL divergence term\n",
    "\n",
    "# -------------------------------\n",
    "# Manual Training Loop with AMP (float16)\n",
    "# -------------------------------\n",
    "num_epochs = 1000\n",
    "num_grpo = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    old_model = None\n",
    "        \n",
    "    old_model = copy.deepcopy(model)\n",
    "    old_model = old_model.half()\n",
    "    old_model.eval()\n",
    "    for param in old_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for i in range(num_grpo):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # Move batch tensors to device\n",
    "            batch = {k: v.to(device).repeat_interleave(3, dim=0) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with AMP autocast (float16)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**batch)  # Data collator already provides \"labels\"\n",
    "                loss = outputs.loss\n",
    "    \n",
    "                # just test\n",
    "                advantages = loss # (ri - mean(R))/std(R) to do\n",
    "    \n",
    "                old_outputs = old_model(**batch)\n",
    "    \n",
    "                # Get logits from both models\n",
    "                model_logits = outputs.logits  # shape: (batch, seq_len, vocab_size)\n",
    "                old_model_logits = outputs.logits\n",
    "                ref_outputs = ref_model(**batch)\n",
    "                ref_logits = ref_outputs.logits  # same shape as model_logits\n",
    "                \n",
    "                probability_ratio = model_logits / old_model_logits\n",
    "    \n",
    "                # Calculate the unclipped objective\n",
    "                unclipped_objective = probability_ratio * advantages\n",
    "                \n",
    "                # Calculate the clipped objective\n",
    "                clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                clipped_objective = clipped_ratio * advantages\n",
    "                \n",
    "                # Take the minimum of the unclipped and clipped objectives\n",
    "                ppo_loss = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "            \n",
    "                # Compute log-probabilities and probabilities\n",
    "                model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                ref_log_probs = F.softmax(ref_logits, dim=-1)\n",
    "            \n",
    "                # Compute KL divergence (using batchmean reduction)\n",
    "                kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "    \n",
    "                # Combine the primary loss and the KL divergence loss\n",
    "                combined_loss = ppo_loss + kl_lambda * kl_div\n",
    "    \n",
    "            \n",
    "            # Backward pass with scaled loss\n",
    "            scaler.scale(combined_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "            print(f\"Epoch {epoch+1} Step {step+1}/{len(dataloader)} Loss: {combined_loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "output_dir = \"./starcoder2-3b-finetuned_adafactor_fp16\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95ad1ec-2d6b-46aa-a728-5b3938d668f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = None\n",
    "model = None\n",
    "old_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5496f9-f24c-4c95-8d91-5192ad5782be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear cached memory that is no longer used\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally, run garbage collection to free up Python memory as well\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053c459-c257-40d2-a044-b7e6f81d271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Sample Training Data\n",
    "# -------------------------------\n",
    "# Each sample contains a \"content\" key with a code snippet.\n",
    "train_examples = [\n",
    "    {\"content\": \"def print_hello_world():\\n    print('Hello, world!')\"},\n",
    "    {\"content\": \"def add(a, b):\\n    return a + b\"},\n",
    "    {\"content\": (\n",
    "        \"def factorial(n):\\n\"\n",
    "        \"    if n == 0:\\n\"\n",
    "        \"        return 1\\n\"\n",
    "        \"    else:\\n\"\n",
    "        \"        return n * factorial(n - 1)\"\n",
    "    )},\n",
    "]\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "\n",
    "# -------------------------------\n",
    "# Model & Tokenizer Setup\n",
    "# -------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Set pad token to eos if not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenization\n",
    "# -------------------------------\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Create a data collator for causal language modeling (mlm=False)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create a DataLoader for the training data\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model & Prepare for Training\n",
    "# -------------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Move batch tensors to the proper device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        for k in batch.keys():\n",
    "            print(\"key:\", k)\n",
    "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True, check=True)\n",
    "        print(result.stdout)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        if (step + 1) % 10 == 0 or (step + 1) == len(dataloader):\n",
    "            print(f\"Epoch {epoch+1} Step {step+1}/{len(dataloader)} Loss: {loss.item():.4f}\")\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "output_dir = \"./starcoder2-3b-finetuned_manual\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b657e381-b16c-49f6-aecf-c77d135e2596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffaa02238c54dec8ea1ca6505355d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: input_ids\n",
      "key: attention_mask\n",
      "key: labels\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey:\u001b[39m\u001b[38;5;124m\"\u001b[39m, k)\n\u001b[0;32m--> 105\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia-smi\u001b[39m\u001b[38;5;124m\"\u001b[39m], capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m    107\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Increased Sample Training Data (8 examples)\n",
    "# -------------------------------\n",
    "train_examples = [\n",
    "    {\"content\": \"def print_hello_world():\\n    print('Hello, world!')\"},\n",
    "    {\"content\": \"def add(a, b):\\n    return a + b\"},\n",
    "    {\"content\": \"def subtract(a, b):\\n    return a - b\"},\n",
    "    {\"content\": \"def multiply(a, b):\\n    return a * b\"},\n",
    "    {\"content\": (\n",
    "        \"def factorial(n):\\n\"\n",
    "        \"    if n == 0:\\n\"\n",
    "        \"        return 1\\n\"\n",
    "        \"    else:\\n\"\n",
    "        \"        return n * factorial(n - 1)\"\n",
    "    )},\n",
    "    {\"content\": (\n",
    "        \"def fibonacci(n):\\n\"\n",
    "        \"    a, b = 0, 1\\n\"\n",
    "        \"    result = []\\n\"\n",
    "        \"    for _ in range(n):\\n\"\n",
    "        \"        result.append(a)\\n\"\n",
    "        \"        a, b = b, a + b\\n\"\n",
    "        \"    return result\"\n",
    "    )},\n",
    "    {\"content\": \"def square(x):\\n    return x * x\"},\n",
    "    {\"content\": \"def cube(x):\\n    return x * x * x\"},\n",
    "]\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "\n",
    "# -------------------------------\n",
    "# Model & Tokenizer Setup\n",
    "# -------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Set pad token if not present (using EOS token as pad token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenization Function\n",
    "# -------------------------------\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",  # Pad all examples to max_length\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# -------------------------------\n",
    "# Data Collator and DataLoader\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "batch_size = 2  # Increased batch size\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model & Set Training Mode\n",
    "# -------------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizer Setup with Adafactor\n",
    "# -------------------------------\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,               # Learning rate can be tuned\n",
    "    relative_step=False,   # Set to True to use relative step sizes\n",
    "    scale_parameter=False  # Adjust scaling based on model size\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Manual Training Loop\n",
    "# -------------------------------\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Move batch tensors to the proper device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        for k in batch.keys():\n",
    "            print(\"key:\", k)\n",
    "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True, check=True)\n",
    "        print(result.stdout)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Step {step+1}/{len(dataloader)} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "output_dir = \"./starcoder2-3b-finetuned_adafactor\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6cab1-64eb-454e-8167-d9d697191ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Select the checkpoint and device\n",
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model (using bfloat16 for efficiency)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Define a simple prompt to generate a \"hello world\" function\n",
    "prompt = \"def hello_world():\\n    print('Hello, world!')\\n\\n# End of function\\n\"\n",
    "\n",
    "# Tokenize the prompt and move input tensors to the device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Generate output (limiting to a few additional tokens)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85911117-10bc-45b4-9607-6a0d3edf5320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
