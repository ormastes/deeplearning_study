{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchtext-0.18.0a0+9bed85d-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchaudio-2.6.0a0+d883142-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch_optimizer\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
      "Collecting lion_pytorch\n",
      "  Downloading lion_pytorch-0.2.3-py3-none-any.whl.metadata (616 bytes)\n",
      "Requirement already satisfied: clang_repl_kernel in /usr/local/lib/python3.12/dist-packages (1.5.18)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (2.7.0a0+ecf3bae40a.nv25.2)\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from clang_repl_kernel) (6.29.5)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.12/dist-packages (from clang_repl_kernel) (8.6.3)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from clang_repl_kernel) (5.14.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (8.32.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (1.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang_repl_kernel) (6.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client->clang_repl_kernel) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang_repl_kernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang_repl_kernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang_repl_kernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang_repl_kernel) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang_repl_kernel) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang_repl_kernel) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->clang_repl_kernel) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client->clang_repl_kernel) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->clang_repl_kernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->clang_repl_kernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->clang_repl_kernel) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->clang_repl_kernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->clang_repl_kernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->clang_repl_kernel) (0.2.3)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "Downloading lion_pytorch-0.2.3-py3-none-any.whl (6.6 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, pytorch-ranger, lion_pytorch, torch_optimizer, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-3.4.1 dill-0.3.8 lion_pytorch-0.2.3 multiprocess-0.70.16 pytorch-ranger-0.1.1 torch_optimizer-0.3.0 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchtext-0.18.0a0+9bed85d-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchaudio-2.6.0a0+d883142-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: clang-repl-kernel in /usr/local/lib/python3.12/dist-packages (1.5.18)\n",
      "Collecting clang-repl-kernel\n",
      "  Downloading clang_repl_kernel-1.6.24-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from clang-repl-kernel) (6.29.5)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.12/dist-packages (from clang-repl-kernel) (8.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clang-repl-kernel) (4.67.1)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from clang-repl-kernel) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (8.32.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->clang-repl-kernel) (6.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client->clang-repl-kernel) (2.9.0.post0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang-repl-kernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang-repl-kernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang-repl-kernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang-repl-kernel) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang-repl-kernel) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->clang-repl-kernel) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->clang-repl-kernel) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client->clang-repl-kernel) (1.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->clang-repl-kernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->clang-repl-kernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->clang-repl-kernel) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->clang-repl-kernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->clang-repl-kernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->clang-repl-kernel) (0.2.3)\n",
      "Downloading clang_repl_kernel-1.6.24-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: clang-repl-kernel\n",
      "  Attempting uninstall: clang-repl-kernel\n",
      "    Found existing installation: clang-repl-kernel 1.5.18\n",
      "    Uninstalling clang-repl-kernel-1.5.18:\n",
      "      Successfully uninstalled clang-repl-kernel-1.5.18\n",
      "Successfully installed clang-repl-kernel-1.6.24\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets torch_optimizer lion_pytorch clang_repl_kernel --break-system-packages\n",
    "!pip install --upgrade clang-repl-kernel  --break-system-packages\n",
    "#!apt update\n",
    "#!apt-get install libc++1 libc++abi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2123aaf5-c62e-4f87-8543-30a8afe51044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading clang_repl binary from Lin64.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin64.zip: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 350M/350M [00:02<00:00, 118MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: /usr/local/lib/python3.12/dist-packages/clang_repl_kernel/clang/Lin64/Lin64.zip\n",
      "Extracting /usr/local/lib/python3.12/dist-packages/clang_repl_kernel/clang/Lin64/Lin64.zip to /usr/local/lib/python3.12/dist-packages/clang_repl_kernel/clang/Lin64\n",
      "Reading zip file... Please wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3245/3245 [00:04<00:00, 675.35file/s]\n"
     ]
    }
   ],
   "source": [
    "from ClangReplInterface import ClangReplInterface\n",
    "clang_repl = ClangReplInterface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145ce4a5-2703-4960-97b9-b6ca27a9a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"runs/starcoder2_reasoning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce104cb8-682a-49fe-9922-1ccf0aef571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf \"saved_models/reasoning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6e617b-71bd-4c7f-b6a3-d8aa3e83471e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0a0+ecf3bae40a.nv25.2 available.\n",
      "INFO:datasets:Polars version 1.14.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "import pickle\n",
    "from Config import SimpleConfig\n",
    "from ClangReplInterface import ClangReplInterface\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "ref_checkpoint_path = \"./saved_models/sample/checkpoint.pt\"\n",
    "last_checkpoint_path = \"./saved_models/reasoning/checkpoint.pt\"\n",
    "checkpoint_dir_pre = \"./saved_models/reasoning/epoch_\"\n",
    "\n",
    "test_target_object_file = \"./saved_models/ReasoningTestTarget_prompt.json\"\n",
    "\n",
    "config = SimpleConfig()\n",
    "\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2413006f38ad4b8e8348d0514dc85c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9a6f5b27bb41938ab2ef89ae48826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 23:40:56,038] A new study created in memory with name: no-name-0203ce69-423b-4cc5-9a77-8cc40d9eb2af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Trial hyperparameters -> lr: 1.5069929525878771e-05, kl_lambda: 0.02398748051937347, epsilon: 0.19806932805426544, num_grpo: 2, warming_up_step: 1, gradient_accumulation_step: 5, temperature: 0.5959953460059638\n",
      "No duplicate parameters found in the optimizer.\n",
      "Epoch 1/1 - Validation\n",
      "Epoch 1/1 - Training Gradient Group 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 202852278468608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:45<06:49, 45.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [2.24, 2.24, 1.24, 1.24]\n",
      "Final Loss: 23785449193472.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:30<06:02, 45.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.6100000143051147 Rewards: [0.02, 1.24, 1.24, 1.24]\n",
      "Final Loss: 20325767053312.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:10<04:58, 42.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [02:55<04:23, 43.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: 1028864671744.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [03:41<03:42, 44.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5951750874519348 Rewards: [2.24, 2.24, 1.18, 1.24]\n",
      "Final Loss: 2910985912320.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [04:26<02:58, 44.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 7703497801728.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:11<02:14, 44.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 2.24, 1.24, 1.24]\n",
      "Final Loss: 89051935801344.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [05:56<01:29, 44.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 1.24, 2.24, 1.24]\n",
      "Final Loss: 72969103605760.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [06:42<00:45, 45.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.015000025741755962 Rewards: [2.21, 2.24, 2.24, 2.24]\n",
      "Final Loss: 8259976036352.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:27<00:00, 44.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Training Gradient Group 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: 13836792889344.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:45<06:49, 45.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: 91909607391232.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:30<06:02, 45.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:11<05:04, 43.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [2.24, 1.24, 2.24, 1.24]\n",
      "Final Loss: 329623137157120.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [02:57<04:25, 44.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 27736416452608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [03:42<03:43, 44.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [2.24, 1.24, 1.24, 2.24]\n",
      "Final Loss: 260330248732672.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [04:27<02:59, 44.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:13<02:14, 44.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 51962292731904.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [05:58<01:30, 45.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [06:44<00:45, 45.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:29<00:00, 44.99s/it]\n",
      "[W 2025-03-25 23:57:12,799] Trial 0 failed with parameters: {'lr': 1.5069929525878771e-05, 'kl_lambda': 0.02398748051937347, 'epsilon': 0.19806932805426544, 'num_grpo': 2, 'warming_up_step': 1, 'gradient_accumulation_step': 5, 'temperature': 0.5959953460059638} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-25 23:57:12,799] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Trial hyperparameters -> lr: 3.1229187954362966e-05, kl_lambda: 0.004548647259414737, epsilon: 0.07558535952458566, num_grpo: 1, warming_up_step: 1, gradient_accumulation_step: 1, temperature: 0.9456642087451629\n",
      "No duplicate parameters found in the optimizer.\n",
      "Epoch 1/1 - Validation\n",
      "Epoch 1/1 - Training Gradient Group 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [2.24, 2.24, 1.24, 1.24]\n",
      "Final Loss: 9193023078400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:45<06:47, 45.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5831809639930725 Rewards: [1.24, 1.22, 2.24, 2.24]\n",
      "Final Loss: 713214887198720.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:30<06:03, 45.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.568821907043457 Rewards: [1.24, 2.21, 2.24, 1.24]\n",
      "Final Loss: 13987487940608.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:16<05:17, 45.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 1.0 Rewards: [0.24000000000000005, 2.24, 2.24, 2.24]\n",
      "Final Loss: 6893169803264.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [03:00<04:29, 44.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 1.24, 2.24, 2.24]\n",
      "Final Loss: 7018318397440.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [03:46<03:45, 45.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 2420591034368.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [04:31<03:01, 45.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 1.24, 2.24, 1.24]\n",
      "Final Loss: 15473030725632.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:17<02:16, 45.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.004999955650418997 Rewards: [2.23, 2.24, 2.24, 2.24]\n",
      "Final Loss: 4793518522368.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [06:03<01:31, 45.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [06:44<00:44, 44.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 2.24, 1.24, 1.24]\n",
      "Final Loss: 20647619067904.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:30<00:00, 45.07s/it]\n",
      "[W 2025-03-26 00:06:00,316] Trial 1 failed with parameters: {'lr': 3.1229187954362966e-05, 'kl_lambda': 0.004548647259414737, 'epsilon': 0.07558535952458566, 'num_grpo': 1, 'warming_up_step': 1, 'gradient_accumulation_step': 1, 'temperature': 0.9456642087451629} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-26 00:06:00,317] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Trial hyperparameters -> lr: 2.931656302845327e-06, kl_lambda: 0.01696313262741188, epsilon: 0.018546200050657477, num_grpo: 3, warming_up_step: 1, gradient_accumulation_step: 4, temperature: 0.9642851965617386\n",
      "No duplicate parameters found in the optimizer.\n",
      "Epoch 1/1 - Validation\n",
      "Epoch 1/1 - Training Gradient Group 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 2.24, 2.24, 1.24]\n",
      "Final Loss: 9050722926592.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:45<06:48, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:30<06:02, 45.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [2.24, 1.24, 2.24, 1.24]\n",
      "Final Loss: 10402296496128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:16<05:18, 45.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 2520496734208.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [03:01<04:31, 45.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [03:46<03:46, 45.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 1.24, 2.24, 2.24]\n",
      "Final Loss: 289807213264896.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [04:32<03:01, 45.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [2.24, 2.24, 1.24, 1.24]\n",
      "Final Loss: 6346067935232.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:18<02:16, 45.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [05:59<01:28, 44.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 1.24, 1.24, 1.24]\n",
      "Final Loss: 3625095266304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [06:46<00:45, 45.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.4890381991863251 Rewards: [2.24, 2.18, 1.24, 2.23]\n",
      "Final Loss: 234717597138944.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:33<00:00, 45.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Training Gradient Group 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 1.24, 1.24, 2.24]\n",
      "Final Loss: 2618189414400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:46<06:55, 46.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 1.24, 2.24, 1.24]\n",
      "Final Loss: 46692007149568.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:32<06:12, 46.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 1.0 Rewards: [2.24, 2.24, 2.24, 0.24000000000000005]\n",
      "Final Loss: 45957458690048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:18<05:22, 46.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: 478008922079232.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [03:05<04:37, 46.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [2.24, 1.24, 1.24, 1.24]\n",
      "Final Loss: 6524042974593024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [03:51<03:52, 46.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.015000025741755962 Rewards: [2.21, 2.24, 2.24, 2.24]\n",
      "Final Loss: 2128036364288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [04:38<03:06, 46.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [2.24, 2.24, 2.24, 2.24]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:25<02:19, 46.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 283861032370176.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [06:12<01:33, 46.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 1.24, 2.24]\n",
      "Final Loss: 171745541619712.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [06:59<00:46, 46.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 0.24000000000000005, 1.24, 1.24]\n",
      "Final Loss: 1683157680128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:40<00:00, 46.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Training Gradient Group 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 2.24, 1.24]\n",
      "Final Loss: 2736642565603328.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:46<07:00, 46.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.9574271440505981 Rewards: [1.24, 2.24, 0.24000000000000005, 2.24]\n",
      "Final Loss: 58677943861248.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:32<06:08, 46.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 2.24, 1.24]\n",
      "Final Loss: 114750620958720.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:19<05:25, 46.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.5 Rewards: [1.24, 1.24, 2.24, 1.24]\n",
      "Final Loss: 8106147840000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [03:06<04:39, 46.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [03:48<03:45, 45.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [04:39<03:07, 46.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.8207060694694519 Rewards: [1.24, 2.22, 0.21000000000000005, 1.24]\n",
      "Final Loss: 6745084657664.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:28<02:23, 47.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [06:19<01:37, 48.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [07:09<00:49, 49.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:59<00:00, 48.00s/it]\n",
      "[W 2025-03-26 00:30:33,685] Trial 2 failed with parameters: {'lr': 2.931656302845327e-06, 'kl_lambda': 0.01696313262741188, 'epsilon': 0.018546200050657477, 'num_grpo': 3, 'warming_up_step': 1, 'gradient_accumulation_step': 4, 'temperature': 0.9642851965617386} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-26 00:30:33,686] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Trial hyperparameters -> lr: 2.858527077139412e-06, kl_lambda: 0.05757740368847213, epsilon: 0.10866377220354449, num_grpo: 1, warming_up_step: 1, gradient_accumulation_step: 4, temperature: 0.5323536191000062\n",
      "No duplicate parameters found in the optimizer.\n",
      "Epoch 1/1 - Validation\n",
      "Epoch 1/1 - Training Gradient Group 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                        | 1/10 [00:50<07:38, 50.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████▊                                                                                                           | 2/10 [01:41<06:45, 50.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████▏                                                                                             | 3/10 [02:32<05:55, 50.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████▌                                                                                | 4/10 [03:22<05:04, 50.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 5/10 [04:14<04:14, 50.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 6/10 [05:04<03:23, 50.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 7/10 [05:55<02:32, 50.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 8/10 [06:40<01:38, 49.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 9/10 [07:31<00:49, 49.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_rewards: 0.0 Rewards: [0, 0, 0, 0]\n",
      "Final Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [08:22<00:00, 50.24s/it]\n",
      "[W 2025-03-26 00:40:20,270] Trial 3 failed with parameters: {'lr': 2.858527077139412e-06, 'kl_lambda': 0.05757740368847213, 'epsilon': 0.10866377220354449, 'num_grpo': 1, 'warming_up_step': 1, 'gradient_accumulation_step': 4, 'temperature': 0.5323536191000062} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-26 00:40:20,271] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Trial hyperparameters -> lr: 1.1269427459439703e-06, kl_lambda: 0.07066949695746146, epsilon: 0.15373160925195395, num_grpo: 1, warming_up_step: 1, gradient_accumulation_step: 3, temperature: 1.1301817108632517\n",
      "No duplicate parameters found in the optimizer.\n",
      "Epoch 1/1 - Validation\n",
      "Epoch 1/1 - Training Gradient Group 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# num_iterations=1, num_steps=500, batch_size=4, num_generations=4, max_completion_length=128, kl=0.1,\n",
    "# learning_rate=5e-6, mu=3, epsilon=0.2,\n",
    "#\n",
    "# lr: 7.205691481165551e-05 kl_lambda: 0.2654706177039008 epsilon: 0.019437902361559744 num_grpo: 1\n",
    "# lr: 1.1111588431283189e-06 kl_lambda: 0.15842765249477542 epsilon: 0.11144786260484413 num_grpo: 3\n",
    "MAX_REWARD=4.4 \n",
    "\n",
    "log_content = False\n",
    "log_step = False\n",
    "log_memory = False\n",
    "\n",
    "full_layer_train=True\n",
    "\n",
    "is_finding_opt = True\n",
    "if not is_finding_opt:\n",
    "    num_epochs = 200\n",
    "    lr = 1.1111588431283189e-06\n",
    "    kl_lambda = 0.15842765249477542\n",
    "    epsilon = 0.11144786260484413\n",
    "    num_grpo = 1\n",
    "    save_epochs = 10\n",
    "    warming_up_step= 10\n",
    "    gradient_accumulation_step = 1\n",
    "    temperature = 1.0\n",
    "    \n",
    "group_size = 4\n",
    "batch_size = 1 # 7\n",
    "\n",
    "def pad_to_match(tensor_a, tensor_b, padding_value=0):\n",
    "    # Determine the current sequence lengths\n",
    "    seq_len_a = tensor_a.size(1)\n",
    "    seq_len_b = tensor_b.size(1)\n",
    "\n",
    "    if seq_len_a > seq_len_b:\n",
    "        max_seq_len = max(seq_len_a, seq_len_b)\n",
    "    \n",
    "        # Define padding function\n",
    "        def pad_tensor(tensor, target_length):\n",
    "            pad_length = target_length - tensor.size(1)\n",
    "            if pad_length > 0:\n",
    "                padding = (0, 0) * (tensor.dim() - 2) + (0, pad_length)\n",
    "                tensor = F.pad(tensor, padding, value=padding_value)\n",
    "            return tensor\n",
    "    \n",
    "        # Pad both tensors to the maximum sequence length\n",
    "        tensor_a_padded = pad_tensor(tensor_a, max_seq_len)\n",
    "        tensor_b_padded = pad_tensor(tensor_b, max_seq_len)\n",
    "    else:\n",
    "        tensor_b_padded = tensor_b[:, :seq_len_a]\n",
    "        tensor_a_padded = tensor_a\n",
    "\n",
    "    return tensor_a_padded, tensor_b_padded\n",
    "\n",
    "\n",
    "def reward_atag(front, end, response):\n",
    "    tag_len = len(front + end)\n",
    "    start = response.find(front + end)\n",
    "    end = response.find(front + '/' + end)\n",
    "    reward = 0\n",
    "    if start != -1: reward += 0.1\n",
    "    if end != -1: reward += 0.1\n",
    "    \n",
    "    if start + tag_len < end:\n",
    "        if len(response[start + tag_len:end].strip()) > 1:\n",
    "            reward += 0.1\n",
    "    return reward\n",
    "\n",
    "\n",
    "def remove_comments(code: str):\n",
    "    pattern = re.compile(r'//.*?$|/\\*.*?\\*/', re.DOTALL | re.MULTILINE)\n",
    "    return re.sub(pattern, '', code)\n",
    "\n",
    "\n",
    "def find_all_tag_indexes(text, tag):\n",
    "    \"\"\"Return a list of starting indexes where the tag occurs in the text.\"\"\"\n",
    "    indexes = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        idx = text.find(tag, start)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        indexes.append(idx)\n",
    "        start = idx + len(tag)\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_tag_start_end(idx, starts, ends, tag, full_text):\n",
    "    start = starts[idx]+len(tag)\n",
    "    end = ends[idx]\n",
    "    return full_text[start: end].strip()\n",
    "    \n",
    "\n",
    "def reward_correct(full_text):\n",
    "    # handle only first answer\n",
    "    reward = 0.0\n",
    "    test_target_open = find_all_tag_indexes(full_text, \"<Test Target>\")\n",
    "    test_target_close = find_all_tag_indexes(full_text, \"</Test Target>\")\n",
    "    clang_repl_open = find_all_tag_indexes(full_text, \"<Clang-repl Test>\")\n",
    "    clang_repl_close = find_all_tag_indexes(full_text, \"</Clang-repl Test>\")\n",
    "    if len(test_target_open) == 0 or len(test_target_close) == 0 or len(clang_repl_open) == 0 or len(clang_repl_close) == 0:\n",
    "        return reward, '<Test Target> or <Clang-repl Test> not found'\n",
    "    if len(test_target_open) != len(test_target_close) or len(clang_repl_open) != len(clang_repl_close):\n",
    "        return reward , '<Test Target> or <Clang-repl Test> pair not match'\n",
    "    if not all(x < y for x, y in zip(test_target_open, test_target_close)):\n",
    "        return reward, '<Test Target> not closed properly'\n",
    "    if not all(x < y for x, y in zip(clang_repl_open, clang_repl_close)):\n",
    "        return reward, '<Clang-repl Test> not closed properly' \n",
    "    target_text = get_tag_start_end(-1, test_target_open, test_target_close, \"<Test Target>\", full_text)\n",
    "    target_text = remove_comments(target_text)\n",
    "    target_text = \">>> \"+target_text.replace('\\n', '')\n",
    "\n",
    "    for idx in range(len(clang_repl_open)):\n",
    "        clang_repl_test = get_tag_start_end(idx, clang_repl_open, clang_repl_close, \"<Clang-repl Test>\", full_text)\n",
    "        clang_repl = ClangReplInterface()\n",
    "        test_case_with_target = target_text+'\\n'+clang_repl_test\n",
    "        #print(test_case_with_target)\n",
    "        result, response = clang_repl.run_verify(test_case_with_target)\n",
    "        reward = 0.0\n",
    "        if result == 'ok':\n",
    "            reward = 2.0\n",
    "        elif result == 'fail':\n",
    "            reward = 1.0\n",
    "        elif result == 'error':\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            assert False\n",
    "        return reward, response\n",
    "    else:\n",
    "        return reward, ''\n",
    "\n",
    "def _reward(full_text):\n",
    "    # https://blog.gopenai.com/coding-grpo-from-scratch-a-guide-to-distributed-implementation-with-qwen2-5-1-5b-instruct-59b34227edacabs\n",
    "    format_rewards = []\n",
    "    rewards = []\n",
    "    responses = []\n",
    "    for response_full in full_text:\n",
    "        need_more_test_idx = response_full.find('<Need More Test')\n",
    "        response = response_full if need_more_test_idx == -1 else response_full[:need_more_test_idx]\n",
    "        score = 0.0\n",
    "        score += reward_atag(\"<\", \"Test Object>\", response) # 0.3\n",
    "        score += reward_atag(\"<\", \"Input Data>\", response) # 0.3\n",
    "        score += reward_atag(\"<\", \"Expected Output>\", response) # 0.3\n",
    "        score += reward_atag(\"<\", \"Clang-repl Test>\", response) # 0.3\n",
    "        score += reward_atag(\"[\", \"REASON]\", response) * 2 # 0.3*2\n",
    "        score += reward_atag(\"[\", \"ANSWER]\", response) * 2 # 0.3*2\n",
    "        score = score / 10 # 0.24 *10 = 2.4\n",
    "        format_rewards.append(score)\n",
    "        correct_reward, response = reward_correct(response)\n",
    "        rewards.append(score + correct_reward)\n",
    "        responses.append(response)\n",
    "\n",
    "    return rewards, responses\n",
    "\n",
    "# Custom exception for timeouts.\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def _handle_timeout(signum, frame):\n",
    "    raise TimeoutException(\"Function call timed out.\")\n",
    "    \n",
    "def reward(full_text, writer=None, global_step=None):\n",
    "    # Set the signal handler and alarm.\n",
    "    signal.signal(signal.SIGALRM, _handle_timeout)\n",
    "    signal.alarm(5)  # Set timeout to 5 seconds.\n",
    "    try:\n",
    "        results, responses = _reward(full_text)\n",
    "    except TimeoutException:\n",
    "        # Log the timeout event to TensorBoard under a specific tag if a writer is provided.\n",
    "        if writer is not None:\n",
    "            writer.add_text(\"TimeoutEvents/RewardFunction\", f\"Timeout occurred at global step {global_step}\", global_step=global_step)\n",
    "        results = [0]*len(full_text)\n",
    "        responses = [\"reward() timeout\"]*len(full_text)\n",
    "    finally:\n",
    "        # Cancel the alarm.\n",
    "        signal.alarm(0)\n",
    "    return results, responses\n",
    "\n",
    "def object_hiper_param(trial):\n",
    "    # Shortened training for demonstration:\n",
    "    num_epochs = 1  # 2   # or 2–3, to save time during hyperparameter search\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True) # 5e-6\n",
    "    kl_lambda = trial.suggest_float(\"kl_lambda\", 0.001, 0.08) # 0.04 from\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.2) # 0.1\n",
    "    num_grpo = trial.suggest_int(\"num_grpo\", 1, 3, step=1) # 1\n",
    "    warming_up_step= trial.suggest_int(\"warming_up_step\", 1, 1, step=1)\n",
    "    gradient_accumulation_step = trial.suggest_int(\"gradient_accumulation_step\", 1, 5, step=1)\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.5, 1.2) # 1.0\n",
    "\n",
    "    return num_epochs, lr, kl_lambda, epsilon, num_grpo, warming_up_step, gradient_accumulation_step, temperature\n",
    "\n",
    "def list_cuda_tensors():\n",
    "    cuda_tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                cuda_tensors.append(obj)\n",
    "        except Exception:\n",
    "            pass  # Some objects might not have the attributes we need.\n",
    "    return cuda_tensors\n",
    "\n",
    "previous_tensor_info = {}\n",
    "def print_memory(tag):\n",
    "    global previous_tensor_info\n",
    "    if not log_memory:\n",
    "        return\n",
    "    # Make sure you have a GPU device available.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Print current allocated and reserved memory in MB:\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "    print(tag)\n",
    "    print(f\"Memory allocated: {allocated:.2f} MB\")\n",
    "    print(f\"Memory reserved: {reserved:.2f} MB\")\n",
    "\n",
    "    \n",
    "    if False:\n",
    "        print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        \n",
    "    if True:\n",
    "        # List all CUDA tensors and collect their details in a dictionary.\n",
    "        cuda_tensors = list_cuda_tensors()\n",
    "        current_tensor_info = {id(tensor): (tensor.shape, tensor.device) for tensor in cuda_tensors}\n",
    "        \n",
    "        # Determine new and deleted tensor IDs.\n",
    "        current_ids = set(current_tensor_info.keys())\n",
    "        previous_ids = set(previous_tensor_info.keys())\n",
    "\n",
    "        print(\"Total Tensors:\", len(current_ids), \", Changes:\", len(current_ids)-len(previous_ids))\n",
    "\n",
    "        if False:\n",
    "            # Determine new tensors since the last call.\n",
    "            new_tensor_ids = current_tensor_ids - previous_tensor_ids\n",
    "            deleted_tensor_ids = previous_ids - current_ids\n",
    "            if new_tensor_ids:\n",
    "                print(\"New CUDA tensors created since the last call:\")\n",
    "                for tid in new_tensor_ids:\n",
    "                    shape, dev = current_tensor_info[tid]\n",
    "                    print(f\"Tensor id: {tid} | Shape: {shape} | Device: {dev}\")\n",
    "    \n",
    "            if deleted_tensor_ids:\n",
    "                print(\"Deleted CUDA tensors since the last call:\")\n",
    "                for tid in deleted_tensor_ids:\n",
    "                    shape, dev = previous_tensor_info[tid]\n",
    "                    print(f\"Tensor id: {tid} | Shape: {shape} | Device: {dev}\")\n",
    "        previous_tensor_info = current_tensor_info.copy()\n",
    "\n",
    "def print_step(tag, main_step=False):\n",
    "    if log_memory:\n",
    "        print_memory(tag)\n",
    "    else:\n",
    "        if log_step or main_step:\n",
    "            print(tag)\n",
    "\n",
    "def check_optimizer_duplicates(optimizer):\n",
    "    seen_ids = set()\n",
    "    duplicates = []\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            pid = id(param)\n",
    "            if pid in seen_ids:\n",
    "                duplicates.append(param)\n",
    "            else:\n",
    "                seen_ids.add(pid)\n",
    "    return duplicates\n",
    "\n",
    "def samping(model, tokenizer, device, epoch, writer, sample_prompt, expected):\n",
    "    # Include attention_mask in the tokenization\n",
    "    sample_prompt = f\"### Instruction\\n\\n{sample_prompt}\\n\\n### Response\"\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Pass the attention_mask and explicitly set pad_token_id to eos_token_id for reliable generation\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    sample_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    sample_text = sample_text.strip()\n",
    "    if (log_content):print(f\"Sample Output (Epoch {epoch + 1}): {sample_text}\")\n",
    "    if (log_content):print(\"Expected:\", expected)\n",
    "    writer.add_text(\"Sample Output\", f\"Epoch {epoch + 1}: {sample_text}\", epoch)\n",
    "\n",
    "\n",
    "\n",
    "def selective_log_softmax(logits, input_ids, tokenizer):\n",
    "    # Ensure input_ids are on the same device as logits\n",
    "    if input_ids.device != logits.device:\n",
    "        input_ids = input_ids.to(logits.device)\n",
    "\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    if input_ids.size(1) > log_probs.size(1):\n",
    "        input_ids = input_ids[:, :log_probs.size(1)]\n",
    "\n",
    "    # Gather log probabilities corresponding to input_ids\n",
    "    selected_log_probs = log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    if (log_content):\n",
    "        input_text = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        print(\"Input Texts:\")\n",
    "        for text in input_text:\n",
    "            print(text)\n",
    "        logits_ids = logits.argmax(dim=-1)\n",
    "        logit_text = tokenizer.batch_decode(logits_ids, skip_special_tokens=True)\n",
    "        print(\"\\nLogit Texts:\")\n",
    "        for text in logit_text:\n",
    "            print(text)\n",
    "\n",
    "    return selected_log_probs\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    layer_index = 1\n",
    "    original_first_block = self.model.layers[layer_index]\n",
    "\n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "\n",
    "    self.model.layers.insert(layer_index, new_block)\n",
    "\n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "\n",
    "def get_layer_params(self, layer_index: int = 0):\n",
    "    if full_layer_train:\n",
    "        return self.model.parameters()\n",
    "    else:\n",
    "        first_params = list(self.model.layers[0].parameters())\n",
    "        sec_params = list(self.model.layers[0].parameters())\n",
    "        # last_params = list(self.model.layers[-1].parameters())\n",
    "        return first_params + sec_params  # + last_params\n",
    "\n",
    "\n",
    "def cut_tensors_by_min(a: torch.Tensor, b: torch.Tensor, dim: int):\n",
    "    \"\"\"\n",
    "    Cut tensors `a` and `b` along a specified dimension to the smaller length \n",
    "    between them along that dimension.\n",
    "\n",
    "    Args:\n",
    "        a (torch.Tensor): A 2D or 3D tensor.\n",
    "        b (torch.Tensor): A 2D or 3D tensor.\n",
    "        dim (int): The dimension along which to slice.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Sliced versions of `a` and `b`.\n",
    "    \"\"\"\n",
    "    assert a.dim() > dim and b.dim() > dim, \"Specified dim exceeds tensor rank\"\n",
    "\n",
    "    min_length = min(a.size(dim), b.size(dim))\n",
    "    a_cut = torch.narrow(a, dim, 0, min_length)\n",
    "    b_cut = torch.narrow(b, dim, 0, min_length)\n",
    "    return a_cut, b_cut\n",
    "\n",
    "def generate_ids(model, batch, tokenizer, temperature):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Determine prompt length for each example in the batch based on the first occurrence of EOS.\n",
    "    prompt_lengths = []\n",
    "    for i in range(input_ids.size(0)):\n",
    "        seq = input_ids[i]\n",
    "        # Find indices where the token equals the eos_token_id.\n",
    "        eos_positions = (seq == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        # If there's at least one occurrence, use its index + 1 (if you want to include the EOS in the prompt).\n",
    "        # Otherwise, fallback to the full sequence length.\n",
    "        if eos_positions.numel() > 0:\n",
    "            first_eos = eos_positions[0].item() + 1\n",
    "        else:\n",
    "            first_eos = seq.size(0)\n",
    "        prompt_lengths.append(first_eos)\n",
    "\n",
    "    print_memory(\"Prompt lengths per batch element: \" + str(prompt_lengths))\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,  # assuming max_length is defined globally\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        early_stop=False\n",
    "    )\n",
    "    generated_ids = output.sequences.detach()\n",
    "    print_memory(\"generated_ids.shape[-1]: \" + str(generated_ids.shape[-1]))\n",
    "    return generated_ids, prompt_lengths\n",
    "\n",
    "\n",
    "def compute_logits(model, generated_ids, prompt_lengths, tokenizer):\n",
    "    generated_ids_mask = (generated_ids != tokenizer.pad_token_id).long().to(generated_ids.device)\n",
    "    logits = model(input_ids=generated_ids, attention_mask=generated_ids_mask, early_stop=False).logits\n",
    "    \n",
    "    completion_ids_list = []\n",
    "    completion_logits_list = []\n",
    "    batch_size = generated_ids.size(0)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        p_len = prompt_lengths[i]\n",
    "        \n",
    "        comp_ids = generated_ids[i, p_len:].detach()\n",
    "        comp_logits = logits[i, p_len-1:, :]\n",
    "        \n",
    "        completion_ids_list.append(comp_ids)\n",
    "        completion_logits_list.append(comp_logits)\n",
    "    \n",
    "    # Determine the minimum length across the batch for completion tokens.\n",
    "    min_length_ids = min(ids.size(0) for ids in completion_ids_list)\n",
    "    min_length_logits = min(lgts.size(0) for lgts in completion_logits_list)\n",
    "    min_length = min(min_length_ids, min_length_logits)\n",
    "    \n",
    "    # Truncate all tensors to the minimum length and stack them.\n",
    "    completion_ids_truncated = torch.stack([ids[:min_length] for ids in completion_ids_list])\n",
    "    completion_logits_truncated = torch.stack([lgts[:min_length] for lgts in completion_logits_list])\n",
    "    \n",
    "    return completion_logits_truncated, completion_ids_truncated\n",
    "    \n",
    "def process_generated_ids(generated_ids, eos_token_id):\n",
    "    processed_ids = []\n",
    "    for seq in generated_ids:\n",
    "        if eos_token_id in seq:\n",
    "            # Truncate the sequence at the first occurrence of the EOS token\n",
    "            first_eos_index = seq.index(eos_token_id)\n",
    "            processed_ids.append(seq[:first_eos_index])\n",
    "        else:\n",
    "            processed_ids.append(seq)\n",
    "    return processed_ids\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "        train_examples.append({\"content\": content + \"<|endoftext|>\"})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "def load_sample_dataset(pk_file):\n",
    "    with open(config.dataset_file, \"rb\") as f:\n",
    "        global_samples = pickle.load(f)\n",
    "        sample_dataset = []\n",
    "        for sample in global_samples:\n",
    "            sample_dataset.append({\"content\": sample + \"<|endoftext|>\"})\n",
    "        return sample_dataset\n",
    "\n",
    "def get_test_target_content(full_text):\n",
    "    test_target_open = find_all_tag_indexes(full_text, \"<Test Target>\")\n",
    "    test_target_close = find_all_tag_indexes(full_text, \"</Test Target>\")\n",
    "    target_text = get_tag_start_end(-1, test_target_open, test_target_close, \"<Test Target>\", full_text)\n",
    "    return target_text\n",
    "\n",
    "def load_reasoning_dataset(test_target_object_file):\n",
    "    with open(test_target_object_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        train = []\n",
    "        val = []\n",
    "        categories = set()\n",
    "        data_dic = {}\n",
    "        for item in data:\n",
    "            categories.add(item['category'])\n",
    "        for cat in categories:\n",
    "            data_dic[cat] = []\n",
    "        for item in data:\n",
    "            data_dic[item['category']].append(item['content'])\n",
    "        for cat in categories:\n",
    "            for idx, item in enumerate(data_dic[cat]):\n",
    "                content = f\"### Instruction\\n\\nn<Test Target>\\n{get_test_target_content(item)}\\n</Test Target>\\nWrtie a Clang-repl Test\\n### Response\\n\"\n",
    "                if idx >=10:\n",
    "                    val.append({\"content\":content})\n",
    "                else:\n",
    "                    train.append({\"content\":content})\n",
    "\n",
    "        return train, val\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "train_data_prompt = load_qa_dataset(qa_json_path)\n",
    "train_data_sample = load_sample_dataset(config.dataset_file)\n",
    "train_data = train_data_prompt + (train_data_sample * 10)\n",
    "\n",
    "test_target_object_file = \"manual_data_set/ReasoningTestTarget.json\"\n",
    "reasoning_dataset, val_reasoning_dataset = load_reasoning_dataset(test_target_object_file)\n",
    "reasoning_dataset = reasoning_dataset[:10]\n",
    "val_reasoning_dataset = val_reasoning_dataset[:5]\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(reasoning_dataset)\n",
    "val_train_dataset = Dataset.from_list(val_reasoning_dataset)\n",
    "# ------------------------------------------------\n",
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "# Load tokenizer from saved directory if exists; otherwise, load from pretrained.\n",
    "tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "if os.path.exists(tokenizer_save_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_dir)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if log_step: print(\"Loaded tokenizer from saved checkpoint.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        #padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "if log_step: print(\"eos: \", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "val_tokenized_dataset = val_train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized_dataset = val_tokenized_dataset.remove_columns([\"content\"])\n",
    "val_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def run(model, old_model, ref_model, dataloader, optimizer, device, tokenizer,\n",
    "        group_size, epsilon, kl_lambda, scaler, writer, global_step, log_group,\n",
    "        warm_up_step, gradient_accumulation_step, is_validation=False, temperature=1.0):\n",
    "    running_loss = 0.0\n",
    "    print_memory(\"_.1. run() enter\")\n",
    "    # For accumulation mode, ensure gradients are zeroed at the start.\n",
    "    if not is_validation and global_step >= warm_up_step:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, total=len(dataloader)), start=1):\n",
    "        print_step(f\"Processing batch {step}/{len(dataloader)}: Start Loop\")\n",
    "        # Move batch to device and expand the tensors.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        input_ids = batch['input_ids'].repeat_interleave(group_size, dim=0)\n",
    "        attention_mask = batch['attention_mask'].repeat_interleave(group_size, dim=0)\n",
    "        batch['input_ids'] = input_ids\n",
    "        batch['attention_mask'] = attention_mask\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            # 1. Model forward pass for generation.\n",
    "            print_step(\"1. Model train\")\n",
    "            with torch.no_grad():\n",
    "                generated_ids, prompt_lengths = generate_ids(old_model, batch, tokenizer, temperature)\n",
    "            completion_logits, completion_ids = compute_logits(model, generated_ids, prompt_lengths, tokenizer)\n",
    "            full_ids = process_generated_ids(generated_ids, tokenizer.eos_token_id)\n",
    "            \n",
    "            # 2. Run legacy models (old and reference models).\n",
    "            print_step(\"2. Legacy Models Run\")\n",
    "            with torch.no_grad():\n",
    "                old_completion_logits, old_completion_ids = compute_logits(old_model, generated_ids, prompt_lengths, tokenizer)\n",
    "                ref_completion_logits, ref_completion_ids = compute_logits(ref_model, generated_ids, prompt_lengths, tokenizer)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_log_logits = selective_log_softmax(completion_logits, completion_ids, tokenizer)\n",
    "                old_model_log_logits = selective_log_softmax(old_completion_logits, old_completion_ids, tokenizer)\n",
    "                model_log_logits, old_model_log_logits = pad_to_match(model_log_logits, old_model_log_logits)\n",
    "                probability_ratio = torch.exp(model_log_logits - old_model_log_logits)\n",
    "                # Remove legacy model intermediates (no longer needed)\n",
    "                old_completion_logits = None\n",
    "                old_completion_ids = None\n",
    "                ref_completion_ids = None\n",
    "                model_log_logits = None\n",
    "                old_model_log_logits = None\n",
    "\n",
    "            # 3. Calculate rewards.\n",
    "            print_step(\"3. Reward calc\")\n",
    "            full_text_lists = tokenizer.batch_decode(full_ids, skip_special_tokens=True)\n",
    "            rewards, responses = reward(full_text_lists, writer, global_step)\n",
    "            # Release unused tensors from generation.\n",
    "            full_ids = None\n",
    "            full_text_lists = None\n",
    "\n",
    "            # Convert rewards to tensor\n",
    "            advantages = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Reshape rewards to match batch structure (assuming same shape as in first implementation)\n",
    "            batch_size = advantages.size(0) // group_size\n",
    "            advantages = advantages.view(batch_size, group_size)\n",
    "            \n",
    "            # Calculate mean and std per batch (along dim=1) and repeat to match original size\n",
    "            mean_rewards = advantages.mean(dim=1).repeat_interleave(group_size)\n",
    "            std_rewards = advantages.std(dim=1).repeat_interleave(group_size)\n",
    "            print(\"std_rewards:\", std_rewards[0].item(), \"Rewards:\", rewards)\n",
    "\n",
    "            # Reshape back to original form\n",
    "            advantages = advantages.view(-1)\n",
    "            \n",
    "            _A_hat = (advantages - mean_rewards) / std_rewards\n",
    "            A_hat = _A_hat.unsqueeze(1)\n",
    "            \n",
    "            # Clear rewards intermediates.\n",
    "            advantages = None\n",
    "            _A_hat = None\n",
    "\n",
    "            # 4. Loss Calculation.\n",
    "            print_step(\"4. Loss Calc\")            \n",
    "            # PPO objective calculations.\n",
    "            unclipped_objective = probability_ratio * A_hat\n",
    "            clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "            clipped_objective = clipped_ratio * A_hat\n",
    "            \n",
    "            # Log objective stats.\n",
    "            surrogate_loss = torch.min(unclipped_objective, clipped_objective)  # shape: [batch, seq_length]\n",
    "            \n",
    "            # Calculate token-level log probabilities.\n",
    "            model_log_probs = F.log_softmax(completion_logits, dim=-1)   # log probs for model outputs\n",
    "            ref_log_probs = F.log_softmax(ref_completion_logits, dim=-1)   # log probs for reference outputs\n",
    "            \n",
    "            # Make sure both tensors have the same shape.\n",
    "            ref_log_probs, model_log_probs = pad_to_match(ref_log_probs, model_log_probs)\n",
    "            \n",
    "            # Compute token-level KL divergence.\n",
    "            kl_div = torch.exp(ref_log_probs - model_log_probs) - (ref_log_probs - model_log_probs) - 1\n",
    "            kl_div = kl_div.sum(dim=-1) \n",
    "            \n",
    "            # Create a mask for non-padding tokens.\n",
    "            completion_mask = (completion_ids != tokenizer.pad_token_id).float()  # shape: [batch, seq_length]\n",
    "            \n",
    "            # Assume beta is a scaling factor for the KL term (e.g., beta = kl_lambda).\n",
    "            per_token_loss = surrogate_loss - kl_lambda * kl_div\n",
    "            \n",
    "            # Final loss: sum the per-token losses over tokens, normalize by the number of non-padding tokens,\n",
    "            # then average over the batch. The negative sign turns maximization into minimization.\n",
    "            combined_loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "\n",
    "            # Save scalar values for logging before clearing.\n",
    "            combined_loss_val = combined_loss.mean().item()\n",
    "            ppo_loss_val=surrogate_loss.mean().item()\n",
    "            kl_div_val = kl_div.mean().item()\n",
    "            print(\"Final Loss:\", combined_loss_val)\n",
    "\n",
    "            # Remove now-unused intermediate tensors.\n",
    "            unclipped_objective = None\n",
    "            clipped_ratio = None\n",
    "            clipped_objective = None\n",
    "            model_log_probs = None\n",
    "            ref_log_probs = None\n",
    "            combined_loss = None\n",
    "            surrogate_loss = None\n",
    "            A_hat = None\n",
    "            probability_ratio = None\n",
    "\n",
    "        # 5. Backpropagation and parameter update (only if not in validation mode).\n",
    "        is_param_updated = False\n",
    "        if not is_validation:\n",
    "            if global_step < warm_up_step:\n",
    "                optimizer.zero_grad()  # Ensure gradients are zeroed in warm-up.\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                if (step + 1) % gradient_accumulation_step == 0 or (step + 1) == len(dataloader):\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    is_param_updated = True\n",
    "\n",
    "        running_loss += combined_loss.item()\n",
    "\n",
    "        # 6. Logging with dynamic log group.\n",
    "        print_step(\"6. Logging\")\n",
    "        writer.add_scalar(f\"{log_group}/combined_loss\", combined_loss_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/ppo_loss\", ppo_loss_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/kl_div\", kl_div_val, global_step)\n",
    "        if is_param_updated:\n",
    "            writer.add_scalar(f\"{log_group}/model_update_combined_loss\", combined_loss_val, global_step)\n",
    "\n",
    "        # Remove unused variables from the current iteration.\n",
    "        completion_logits = None\n",
    "        completion_ids = None\n",
    "        ref_completion_logits = None  # Already cleared above if not needed.\n",
    "        combined_loss = None\n",
    "        kl_div = None\n",
    "\n",
    "        print_step(\"7. End Loop\")\n",
    "        global_step += 1\n",
    "\n",
    "    print_step(\"_.1. run() exit\")\n",
    "    return running_loss, global_step\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, ref_model, dataloader, val_dataloader, optimizer, device, num_epochs, group_size,\n",
    "                       num_grpo, epsilon, kl_lambda, scaler, save_epochs, start_epoch,\n",
    "                       warming_up_step, gradient_accumulation_step, skip_validation_step=False, \n",
    "                       temperature=1.0):\n",
    "    writer = SummaryWriter(log_dir=\"runs/starcoder2_reasoning\")\n",
    "\n",
    "    # --- Generate sample output text after each epoch ---\n",
    "    model.eval()  # Set to eval mode for generation.\n",
    "    with torch.no_grad():\n",
    "        samping(model, tokenizer, device, 0, writer,\n",
    "                \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\",\n",
    "                \"```\\n>>> (prompt)\\n```\")\n",
    "        samping(model, tokenizer, device, 0, writer,\n",
    "                \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\",\n",
    "                \"Custom Clang-repl takes only one line input.\")\n",
    "    model.train()  # Switch back to training mode.\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print_step(f\"Epoch {epoch+1}/{num_epochs} - Validation\", main_step=True)\n",
    "\n",
    "        if not skip_validation_step:\n",
    "            print_step(\"_. Validation\")\n",
    "            print_memory(20)\n",
    "            _old_model = copy.deepcopy(model).half()\n",
    "            _old_model.eval()\n",
    "            for param in _old_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print_memory(21)\n",
    "            \n",
    "            # Run validation (no parameter updates).\n",
    "            _, _ = run(model, _old_model, ref_model, val_dataloader, optimizer, device, tokenizer,\n",
    "                                 group_size, epsilon, kl_lambda, scaler, writer, global_step,\n",
    "                                 log_group=\"val_Loss\", warm_up_step=warming_up_step,\n",
    "                                 gradient_accumulation_step=gradient_accumulation_step, is_validation=True,\n",
    "                                 temperature=temperature)\n",
    "            _old_model = None\n",
    "\n",
    "        # Loop over gradient groups for training.\n",
    "        for grpo_idx in range(num_grpo):\n",
    "            print_step(f\"Epoch {epoch+1}/{num_epochs} - Training Gradient Group {grpo_idx+1}/{num_grpo}\", main_step=True)\n",
    "            print_memory(20)\n",
    "            old_model = copy.deepcopy(model).half()\n",
    "            old_model.eval()\n",
    "            for param in old_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print_memory(21)\n",
    "\n",
    "            loss, global_step = run(model, old_model, ref_model, dataloader, optimizer, device, tokenizer,\n",
    "                                      group_size, epsilon, kl_lambda, scaler, writer, global_step,\n",
    "                                      log_group=\"Loss\", warm_up_step=warming_up_step,\n",
    "                                      gradient_accumulation_step=gradient_accumulation_step, is_validation=False,\n",
    "                                    temperature=temperature)\n",
    "            running_loss += loss\n",
    "            old_model = None\n",
    "\n",
    "        print_step(\"7. End Epoch\")\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        if log_step: print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Epoch/Average_Loss\", avg_loss, epoch + 1)\n",
    "\n",
    "        # Save latest checkpoint.\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(last_checkpoint_path), exist_ok=True)\n",
    "        torch.save(checkpoint, last_checkpoint_path)\n",
    "\n",
    "        # Optionally save checkpoint on specific epochs.\n",
    "        if save_epochs is not None and epoch % save_epochs == 0:\n",
    "            checkpoint_dir = checkpoint_dir_pre + str(epoch + 1)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            if log_step: print(f\"Checkpoint saved at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "            # Save tokenizer once if not already saved.\n",
    "            tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "            if not os.path.exists(tokenizer_save_dir):\n",
    "                os.makedirs(tokenizer_save_dir, exist_ok=True)\n",
    "                tokenizer.save_pretrained(tokenizer_save_dir)\n",
    "                if log_step: print(\"Tokenizer saved.\")\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "    return avg_loss\n",
    "\n",
    "def train(\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        kl_lambda,\n",
    "        epsilon,\n",
    "        num_grpo,\n",
    "        group_size,\n",
    "        warming_up_step,\n",
    "        gradient_accumulation_step,\n",
    "        save_epochs=None,\n",
    "        skip_validation_step=False,\n",
    "        is_finding_opt=False,\n",
    "        temperature=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check if a latest checkpoint exists to load model and optimizer states\n",
    "    if os.path.exists(last_checkpoint_path) and not is_finding_opt:\n",
    "        print_memory(1)\n",
    "        checkpoint = torch.load(last_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "        _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        config = copy.deepcopy(_model.config)\n",
    "        _model = None\n",
    "        print_memory(2)\n",
    "        config.num_hidden_layers += 2\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        print_memory(3)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        print_memory(4)\n",
    "        optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        if log_step: print(f\"Loaded checkpoint from {ref_checkpoint_path} at epoch {start_epoch}\")\n",
    "        print_memory(7)\n",
    "    else:\n",
    "        if os.path.exists(ref_checkpoint_path):\n",
    "            print_memory(1)\n",
    "            checkpoint = torch.load(ref_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "            _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            config = copy.deepcopy(_model.config)\n",
    "            _model = None\n",
    "            print_memory(2)\n",
    "            config.num_hidden_layers += 2\n",
    "            model = AutoModelForCausalLM.from_config(config)\n",
    "            print_memory(3)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.to(device)\n",
    "            print_memory(4)\n",
    "            optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "            #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = 0 #checkpoint.get('epoch', 0)\n",
    "            if log_step: print(f\"Loaded checkpoint from {ref_checkpoint_path} at epoch {start_epoch}\")\n",
    "            print_memory(7)\n",
    "\n",
    "        else:\n",
    "            assert False, \"prompt_last_checkpoint_path must exist\"\n",
    "\n",
    "    dups = check_optimizer_duplicates(optimizer)\n",
    "    if dups:\n",
    "        print(\"Warning: The optimizer contains duplicate parameters!\")\n",
    "        print(f\"Duplicate parameter count: {len(dups)}\")\n",
    "    else:\n",
    "        print(\"No duplicate parameters found in the optimizer.\")\n",
    "\n",
    "    # Clear cached memory that is no longer used\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print_memory(9)\n",
    "\n",
    "    # Reference model (for KL)\n",
    "    old_model = None\n",
    "    ref_model = copy.deepcopy(model).half().eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    print_memory(10)\n",
    "    \n",
    "    # DataLoader\n",
    "    global batch_size\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        dataloader=dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        group_size=group_size,\n",
    "        num_grpo=num_grpo,\n",
    "        epsilon=epsilon,\n",
    "        kl_lambda=kl_lambda,\n",
    "        scaler=scaler,\n",
    "        save_epochs=save_epochs,\n",
    "        start_epoch=start_epoch,\n",
    "        warming_up_step=warming_up_step,\n",
    "        gradient_accumulation_step=gradient_accumulation_step,\n",
    "        skip_validation_step=skip_validation_step,\n",
    "        temperature=temperature\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Optuna Objective Function\n",
    "# ------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines how Optuna will run each trial:\n",
    "    - sample hyperparameters\n",
    "    - set up the model & optimizer with those\n",
    "    - run a short training loop\n",
    "    - return a metric (the final avg loss) to minimize\n",
    "    \"\"\"\n",
    "    global group_size\n",
    "    num_epochs, lr, kl_lambda, epsilon, num_grpo, warming_up_step, gradient_accumulation_step, temperature = object_hiper_param(trial)\n",
    "\n",
    "    print(\n",
    "        f\"[Optuna] Trial hyperparameters -> lr: {lr}, kl_lambda: {kl_lambda}, epsilon: {epsilon}, num_grpo: {num_grpo}, warming_up_step: {warming_up_step}, gradient_accumulation_step: {gradient_accumulation_step}, temperature: {temperature}\")\n",
    "    return train(\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        kl_lambda=kl_lambda,\n",
    "        epsilon=epsilon,\n",
    "        num_grpo=num_grpo,\n",
    "        group_size=group_size,\n",
    "        warming_up_step=warming_up_step, \n",
    "        gradient_accumulation_step=gradient_accumulation_step,\n",
    "        skip_validation_step=True,\n",
    "        is_finding_opt=True,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    if is_finding_opt:\n",
    "        # Create study to minimize final loss\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "\n",
    "        print(\"Study completed!\")\n",
    "        print(\"Best trial:\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Value: {best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"#    {key}: {value}\")\n",
    "        with open(\"hiper_param.json\", \"w\") as f:\n",
    "            json.dump(best_trial.params.items(), f, indent=4)\n",
    "        # Study completed!\n",
    "        # Best trial:\n",
    "        #  Value: 715.3611988491482\n",
    "        #  Params:\n",
    "        #    lr: 0.0002746775018590349\n",
    "        #    kl_lambda: 0.10527608699361579\n",
    "        #    epsilon: 0.12442505216944565\n",
    "        #    num_grpo: 2\n",
    "    else:\n",
    "        train(\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            kl_lambda=kl_lambda,\n",
    "            epsilon=epsilon,\n",
    "            num_grpo=num_grpo,\n",
    "            group_size=group_size,\n",
    "            save_epochs=save_epochs,\n",
    "            warming_up_step=warming_up_step,\n",
    "            gradient_accumulation_step=gradient_accumulation_step,\n",
    "            skip_validation_step=False,\n",
    "            temperature=temperature\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71048004-5b1c-4335-a0cd-bbafca1d9433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
