import torch
from torch.utils.data import Dataset, DataLoader


class TestTargetDataset(Dataset):
    """
    Custom Dataset to load training samples from a plain text dataset file.
    Each sample is delimited by the marker '####SAMPLE_END####'.
    """

    def __init__(self, file_path, transform=None):
        marker = "####SAMPLE_END####"
        with open(file_path, "r", encoding="utf-8") as f:
            raw_data = f.read()
        # Split the file using the unique marker and filter out empty strings.
        self.samples = [sample.strip() for sample in raw_data.split(marker) if sample.strip()]
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample_text = self.samples[idx]
        # Optionally, apply a transformation (e.g., tokenization)
        if self.transform:
            sample_text = self.transform(sample_text)
        return sample_text


# Example usage for training:
if __name__ == "__main__":
    dataset_file = "your_dataset_file.txt"  # Path to the file generated by the converter
    dataset = ExactSampleDataset(dataset_file)

    # Example: if you want to tokenize text before training, you can define a transform.
    # from transformers import AutoTokenizer
    # tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    # def tokenize_text(text):
    #     return tokenizer(text, return_tensors="pt", padding='max_length', truncation=True, max_length=128)
    # dataset = TextDataset(dataset_file, transform=tokenize_text)

    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    for batch in dataloader:
        # Each batch is a list of processed sample texts (or tokenized outputs)
        print(batch)
        # Proceed with your training loop here...
        break
