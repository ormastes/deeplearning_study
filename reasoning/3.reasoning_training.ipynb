{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Additional modules in addition of docker environment\n",
    "# ------------------------------------------------\n",
    "\n",
    "if False:\n",
    "    !pip install datasets torch_optimizer lion_pytorch clang_repl_kernel --break-system-packages\n",
    "    !pip install --upgrade clang-repl-kernel  --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2123aaf5-c62e-4f87-8543-30a8afe51044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning execise environment prepare\n",
    "# ------------------------------------------------\n",
    "\n",
    "from ClangReplInterface import ClangReplInterface\n",
    "if False:\n",
    "    clang_repl = ClangReplInterface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145ce4a5-2703-4960-97b9-b6ca27a9a5a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!rm -rf \"runs/starcoder2_reasoning\"\n",
    "#!rm -rf \"saved_models/reasoning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6e617b-71bd-4c7f-b6a3-d8aa3e83471e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "# import and setup\n",
    "# ------------------------------------------------\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import inspect\n",
    "import gc\n",
    "from enum import Enum\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "import pickle\n",
    "from Config import SimpleConfig\n",
    "from ClangReplInterface import ClangReplInterface, ObjectPool\n",
    "import warnings\n",
    "\n",
    "# to clean up log\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "# for memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# pytorch random seed fix idiom\n",
    "def set_random_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "    \n",
    "set_random_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bc6b7e-b497-46f5-a993-ed6c15a024c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall configuration\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "config = SimpleConfig()\n",
    "max_length = 512\n",
    "MAX_REWARD=4.4 \n",
    "\n",
    "class TrainLayers(Enum):\n",
    "    FULL_LAYER = 1\n",
    "    TWO_FRONT_LAYER = 2\n",
    "    ODD_LAYER = 3\n",
    "    EVEN_LAYER = 4\n",
    "    SWITCH_PAIR_LAYER = 5\n",
    "\n",
    "train_layer = TrainLayers.FULL_LAYER\n",
    "\n",
    "ref_checkpoint_path = \"./saved_models/starcoder2-3b_exact_sample/checkpoint.pt\"\n",
    "test_target_object_file = \"manual_data_set/ReasoningTestTarget.json\"\n",
    "learning_name = f\"starcoder2-3b_reasoning_{train_layer.name}_\"\n",
    "now = datetime.now().strftime(\"%d_%H-%M\")\n",
    "learning_name_time = learning_name + now\n",
    "last_checkpoint_path = f\"./saved_models/{learning_name}/checkpoint.pt\"\n",
    "checkpoint_dir_pre = f\"./saved_models/{learning_name}/epoch_\"\n",
    "\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "# Load tokenizer from saved directory if exists; otherwise, load from pretrained.\n",
    "tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "\n",
    "\n",
    "log_content = False\n",
    "log_step = False\n",
    "log_memory = False\n",
    "log_logits = False\n",
    "log_reward = True\n",
    "log_tensor = True\n",
    "\n",
    "checking_range = False\n",
    "checking_shape = False\n",
    "\n",
    "group_size = 4\n",
    "batch_size = 1 # 7\n",
    "\n",
    "category_count_start = 1 # 9\n",
    "use_reference_model = True\n",
    "skip_validation_step=True\n",
    "is_finding_opt = True\n",
    "if not is_finding_opt:\n",
    "    num_epochs = 200\n",
    "    lr = 1.6289610787142993e-07 #1.1111588431283189e-06\n",
    "    kl_lambda = 1.0 #0.15842765249477542\n",
    "    epsilon = 0.2 #0.11144786260484413\n",
    "    num_grpo = 3\n",
    "    save_epochs = 10\n",
    "    warming_up_step= 1\n",
    "    gradient_accumulation_step = 1\n",
    "    temperature = 0.9797868214412535  #1.0\n",
    "# Best trial:\n",
    "#  Value: 10.131847697589546\n",
    "#  Params: \n",
    "#    lr: 1.6289610787142993e-06\n",
    "#    kl_lambda: 0.0490765147903471\n",
    "#    epsilon: 0.09582283082922764\n",
    "#    num_grpo: 1\n",
    "#    temperature: 0.9797868214412535    \n",
    "# for note valide setting\n",
    "# num_iterations=1, num_steps=500, batch_size=4, num_generations=4, max_completion_length=128, kl=0.1,\n",
    "# learning_rate=5e-6, mu=3, epsilon=0.2,\n",
    "#\n",
    "# lr: 7.205691481165551e-05 kl_lambda: 0.2654706177039008 epsilon: 0.019437902361559744 num_grpo: 1\n",
    "# lr: 1.1111588431283189e-06 kl_lambda: 0.15842765249477542 epsilon: 0.11144786260484413 num_grpo: 3\n",
    "\n",
    "def object_hiper_param(trial):\n",
    "    # Shortened training for demonstration:\n",
    "    num_epochs = 4  # 2   # or 2â€“3, to save time during hyperparameter search\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True) # 5e-6\n",
    "    kl_lambda = trial.suggest_float(\"kl_lambda\", 0.04, 10.0) # 0.04 from\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.3) # 0.1\n",
    "    num_grpo = trial.suggest_int(\"num_grpo\", 1, 2, step=1) # 1\n",
    "    warming_up_step= 1 #trial.suggest_int(\"warming_up_step\", 1, 1, step=1)\n",
    "    gradient_accumulation_step = 1 #trial.suggest_int(\"gradient_accumulation_step\", 1, 5, step=1)\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.5, 1.4) # 1.0\n",
    "    category_count_start = 1 #trial.suggest_int(\"category_count_start\", 1, 2, step=1)\n",
    "\n",
    "    return num_epochs, lr, kl_lambda, epsilon, num_grpo, warming_up_step, gradient_accumulation_step, temperature, category_count_start\n",
    "\n",
    "def shrink_dataset( reasoning_dataset, val_reasoning_dataset):\n",
    "    reasoning_dataset = reasoning_dataset\n",
    "    val_reasoning_dataset = val_reasoning_dataset[:5]\n",
    "    return reasoning_dataset, val_reasoning_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f3ff14-2391-43ba-819f-58eaa713d69b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "\n",
    "if os.path.exists(tokenizer_save_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_dir)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if log_step: print(\"Loaded tokenizer from saved checkpoint.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23fb1d0-1e6e-4f22-a336-236e991415f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reward utility\n",
    "# ------------------------------------------------\n",
    "\n",
    "def reward_atag(front, end, response):\n",
    "    tag_len = len(front + end)\n",
    "    start = response.find(front + end)\n",
    "    end = response.find(front + '/' + end)\n",
    "    reward = 0\n",
    "    if start != -1: reward += 0.1\n",
    "    if end != -1: reward += 0.1\n",
    "    \n",
    "    if start + tag_len < end:\n",
    "        if len(response[start + tag_len:end].strip()) > 1:\n",
    "            reward += 0.1\n",
    "    return reward\n",
    "\n",
    "\n",
    "def remove_comments(code: str):\n",
    "    pattern = re.compile(r'//.*?$|/\\*.*?\\*/', re.DOTALL | re.MULTILINE)\n",
    "    return re.sub(pattern, '', code)\n",
    "\n",
    "\n",
    "def find_all_tag_indexes(text, tag):\n",
    "    \"\"\"Return a list of starting indexes where the tag occurs in the text.\"\"\"\n",
    "    indexes = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        idx = text.find(tag, start)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        indexes.append(idx)\n",
    "        start = idx + len(tag)\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_tag_start_end(idx, starts, ends, tag, full_text):\n",
    "    start = starts[idx]+len(tag)\n",
    "    end = ends[idx]\n",
    "    return full_text[start: end].strip()\n",
    "    \n",
    "\n",
    "def reward_correct(clang_repl, full_text, writer, log_group, step):\n",
    "    # handle only first answer\n",
    "    reward = 0.0\n",
    "    test_target_open = find_all_tag_indexes(full_text, \"<Test Target>\")\n",
    "    test_target_close = find_all_tag_indexes(full_text, \"</Test Target>\")\n",
    "    clang_repl_open = find_all_tag_indexes(full_text, \"<Clang-repl Test>\")\n",
    "    clang_repl_close = find_all_tag_indexes(full_text, \"</Clang-repl Test>\")\n",
    "    if len(test_target_open) == 0 or len(test_target_close) == 0 or len(clang_repl_open) == 0 or len(clang_repl_close) == 0:\n",
    "        return reward, '<Test Target> or <Clang-repl Test> not found'\n",
    "    if len(test_target_open) != len(test_target_close) or len(clang_repl_open) != len(clang_repl_close):\n",
    "        return reward , '<Test Target> or <Clang-repl Test> pair not match'\n",
    "    if not all(x < y for x, y in zip(test_target_open, test_target_close)):\n",
    "        return reward, '<Test Target> not closed properly'\n",
    "    if not all(x < y for x, y in zip(clang_repl_open, clang_repl_close)):\n",
    "        return reward, '<Clang-repl Test> not closed properly' \n",
    "    target_text = get_tag_start_end(-1, test_target_open, test_target_close, \"<Test Target>\", full_text)\n",
    "    target_text = remove_comments(target_text)\n",
    "    target_text = \">>> \"+target_text.replace('\\n', '')\n",
    "\n",
    "    for idx in range(len(clang_repl_open)):\n",
    "        clang_repl_test = get_tag_start_end(idx, clang_repl_open, clang_repl_close, \"<Clang-repl Test>\", full_text)\n",
    "        test_case_with_target = target_text+'\\n'+clang_repl_test\n",
    "        #print(test_case_with_target)\n",
    "        result, response = clang_repl.run_verify(test_case_with_target)\n",
    "        reward = 0.0\n",
    "        if result == 'ok':\n",
    "            reward = 2.0\n",
    "        elif result == 'fail':\n",
    "            reward = 1.0\n",
    "        elif result == 'error':\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            assert False\n",
    "        if log_reward and reward < 1.9:\n",
    "            writer.add_text(f\"{log_group}/reward_correct_context\", f\"Verify: {test_case_with_target}\\nResponse: {response}\", step)\n",
    "        return reward, response\n",
    "    else:\n",
    "        return reward, ''\n",
    "\n",
    "def reward(clang_repl, response_full, writer, log_group, step):\n",
    "    # https://blog.gopenai.com/coding-grpo-from-scratch-a-guide-to-distributed-implementation-with-qwen2-5-1-5b-instruct-59b34227edacabs\n",
    "    need_more_test_idx = response_full.find('<Need More Test')\n",
    "    response = response_full if need_more_test_idx == -1 else response_full[:need_more_test_idx]\n",
    "    score = 0.0\n",
    "    score += reward_atag(\"<\", \"Test Object>\", response) # 0.3\n",
    "    score += reward_atag(\"<\", \"Input Data>\", response) # 0.3\n",
    "    score += reward_atag(\"<\", \"Expected Output>\", response) # 0.3\n",
    "    score += reward_atag(\"<\", \"Clang-repl Test>\", response) # 0.3\n",
    "    score += reward_atag(\"[\", \"REASON]\", response) * 2 # 0.3*2\n",
    "    score += reward_atag(\"[\", \"ANSWER]\", response) * 2 # 0.3*2\n",
    "    score = score / 10 # 0.24 *10 = 2.4\n",
    "    if score >= 0.24:\n",
    "        correct_reward, response = reward_correct(clang_repl, response, writer, log_group, step)\n",
    "    else:\n",
    "        correct_reward = 0.0\n",
    "        response = \"not formatted\"\n",
    "    reward = score + correct_reward\n",
    "    writer.add_scalar(f\"{log_group}/format_correct_sample\", score, step)\n",
    "    writer.add_scalar(f\"{log_group}/reward_correct_sample\", correct_reward, step)\n",
    "    return reward, response\n",
    "\n",
    "class RewardWorkPool(ObjectPool):     \n",
    "    def __init__(self, group_size):\n",
    "        super().__init__(ClangReplInterface, reward, group_size)\n",
    "\n",
    "    def reward(self, codes, writer=None, log_group=None, global_step=None):\n",
    "        global group_size\n",
    "        args = [ [code, writer, log_group, global_step*group_size + idx] for idx, code in enumerate(codes)]\n",
    "        self.start_tasks(args)\n",
    "        \n",
    "    def take_result(self):\n",
    "        return self.get_results()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0e71f6-c101-4b5f-96bd-ae13b19d7051",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dataset and dataloader\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "        train_examples.append({\"content\": content + \"<|endoftext|>\"})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "def load_sample_dataset(pk_file):\n",
    "    with open(config.dataset_file, \"rb\") as f:\n",
    "        global_samples = pickle.load(f)\n",
    "        sample_dataset = []\n",
    "        for sample in global_samples:\n",
    "            sample_dataset.append({\"content\": sample + \"<|endoftext|>\"})\n",
    "        return sample_dataset\n",
    "\n",
    "def get_test_target_content(full_text):\n",
    "    test_target_open = find_all_tag_indexes(full_text, \"<Test Target>\")\n",
    "    test_target_close = find_all_tag_indexes(full_text, \"</Test Target>\")\n",
    "    target_text = get_tag_start_end(-1, test_target_open, test_target_close, \"<Test Target>\", full_text)\n",
    "    return target_text\n",
    "\n",
    "def load_reasoning_dataset(test_target_object_file, seltected_categories):\n",
    "    with open(test_target_object_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        train = []\n",
    "        val = []\n",
    "        categories = set()\n",
    "        data_dic = {}\n",
    "        for item in data:\n",
    "            categories.add(item['category'])\n",
    "        for cat in categories:\n",
    "            data_dic[cat] = []\n",
    "        for item in data:\n",
    "            data_dic[item['category']].append(item['content'])\n",
    "\n",
    "        for cat in seltected_categories:\n",
    "            for idx, item in enumerate(data_dic[cat]):\n",
    "                content = f\"### Instruction\\n\\nn<Test Target>\\n{get_test_target_content(item)}\\n</Test Target>\\nWrtie a Clang-repl Test\\n### Response\\n\"\n",
    "                if idx >=10:\n",
    "                    val.append({\"content\":content})\n",
    "                else:\n",
    "                    train.append({\"content\":content})\n",
    "\n",
    "        return train, val\n",
    "\n",
    "\n",
    "def get_all_categories():\n",
    "    global test_target_object_file\n",
    "    with open(test_target_object_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        train = []\n",
    "        val = []\n",
    "        categories = set()\n",
    "        category_list = []\n",
    "        data_dic = {}\n",
    "        for item in data:\n",
    "            # reserve order\n",
    "            if not item['category'] in categories:\n",
    "                category_list.append(item['category'])\n",
    "                categories.add(item['category'])\n",
    "    return category_list\n",
    "\n",
    "        \n",
    "def get_dataloader(categories):\n",
    "    global test_target_object_file, tokenizer\n",
    "    reasoning_dataset, val_reasoning_dataset = load_reasoning_dataset(test_target_object_file, categories)\n",
    "    reasoning_dataset, val_reasoning_dataset = shrink_dataset( reasoning_dataset, val_reasoning_dataset)\n",
    "    \n",
    "    # Create a Hugging Face Dataset from the list\n",
    "    train_dataset = Dataset.from_list(reasoning_dataset)\n",
    "    val_train_dataset = Dataset.from_list(val_reasoning_dataset)\n",
    "\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"content\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            #padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    if log_step: print(\"eos: \", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    \n",
    "    tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    val_tokenized_dataset = val_train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized_dataset = val_tokenized_dataset.remove_columns([\"content\"])\n",
    "    val_tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    # DataLoader\n",
    "    global batch_size\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    return dataloader, val_dataloader\n",
    "\n",
    "def get_train_dataloader(categories):\n",
    "    train, _ = get_dataloader(categories)\n",
    "    return train\n",
    "\n",
    "\n",
    "def get_val_dataloader(categories):\n",
    "    _, val = get_dataloader(categories)\n",
    "    return val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcb244c-2d67-428e-8362-1741f7b8f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging or Print\n",
    "# ------------------------------------------------\n",
    "\n",
    "previous_tensor_info = {}\n",
    "def print_memory(tag):\n",
    "    global previous_tensor_info, log_memory\n",
    "    if not log_memory:\n",
    "        return\n",
    "    # Make sure you have a GPU device available.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Print current allocated and reserved memory in MB:\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "    print(tag)\n",
    "    print(f\"Memory allocated: {allocated:.2f} MB\")\n",
    "    print(f\"Memory reserved: {reserved:.2f} MB\")\n",
    "\n",
    "    \n",
    "    if False:\n",
    "        print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        \n",
    "    if True:\n",
    "        # List all CUDA tensors and collect their details in a dictionary.\n",
    "        cuda_tensors = list_cuda_tensors()\n",
    "        current_tensor_info = {id(tensor): (tensor.shape, tensor.device) for tensor in cuda_tensors}\n",
    "        \n",
    "        # Determine new and deleted tensor IDs.\n",
    "        current_ids = set(current_tensor_info.keys())\n",
    "        previous_ids = set(previous_tensor_info.keys())\n",
    "\n",
    "        print(\"Total Tensors:\", len(current_ids), \", Changes:\", len(current_ids)-len(previous_ids))\n",
    "\n",
    "        if False:\n",
    "            # Determine new tensors since the last call.\n",
    "            new_tensor_ids = current_tensor_ids - previous_tensor_ids\n",
    "            deleted_tensor_ids = previous_ids - current_ids\n",
    "            if new_tensor_ids:\n",
    "                print(\"New CUDA tensors created since the last call:\")\n",
    "                for tid in new_tensor_ids:\n",
    "                    shape, dev = current_tensor_info[tid]\n",
    "                    print(f\"Tensor id: {tid} | Shape: {shape} | Device: {dev}\")\n",
    "    \n",
    "            if deleted_tensor_ids:\n",
    "                print(\"Deleted CUDA tensors since the last call:\")\n",
    "                for tid in deleted_tensor_ids:\n",
    "                    shape, dev = previous_tensor_info[tid]\n",
    "                    print(f\"Tensor id: {tid} | Shape: {shape} | Device: {dev}\")\n",
    "        previous_tensor_info = current_tensor_info.copy()\n",
    "        \n",
    "def cur_memory_ids():\n",
    "    # List all CUDA tensors and collect their details in a dictionary.\n",
    "    cuda_tensors = list_cuda_tensors()\n",
    "    current_tensor_info = {id(tensor): (tensor.shape, tensor.device) for tensor in cuda_tensors}\n",
    "    \n",
    "    # Determine new and deleted tensor IDs.\n",
    "    current_ids = set(current_tensor_info.keys())\n",
    "    return current_ids\n",
    "\n",
    "def compare_memory_ids(previous_tensor_ids):\n",
    "    current_tensor_ids = cur_memory_ids()\n",
    "    new_tensor_ids = current_tensor_ids - previous_tensor_ids\n",
    "    deleted_tensor_ids = previous_tensor_ids - current_tensor_ids \n",
    "    print(\"New tensor:\", new_tensor_ids)\n",
    "    print(\"Delted tensor:\", deleted_tensor_ids)\n",
    "    \n",
    "def list_cuda_tensors():\n",
    "    cuda_tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                cuda_tensors.append(obj)\n",
    "        except Exception:\n",
    "            pass  # Some objects might not have the attributes we need.\n",
    "    return cuda_tensors\n",
    "    \n",
    "def print_step(tag, main_step=False):\n",
    "    global log_memory\n",
    "    if log_memory:\n",
    "        print_memory(tag)\n",
    "    else:\n",
    "        if log_step or main_step:\n",
    "            print(tag)\n",
    "\n",
    "def check_optimizer_duplicates(optimizer):\n",
    "    seen_ids = set()\n",
    "    duplicates = []\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            pid = id(param)\n",
    "            if pid in seen_ids:\n",
    "                duplicates.append(param)\n",
    "            else:\n",
    "                seen_ids.add(pid)\n",
    "    return duplicates\n",
    "\n",
    "def print_logits_ids(tag, logits, ids):\n",
    "    global log_logits\n",
    "    if log_logits:\n",
    "        logits_len = logits.shape[1]\n",
    "        ids_len = ids.shape[1]\n",
    "        if True:\n",
    "            logits_ids = torch.argmax(logits, dim=-1)\n",
    "            ids_text = [tokenizer.decode(ids[i], skip_special_tokens=True) for i in range(ids.size(0))]\n",
    "            logits_text = [tokenizer.decode(logits_ids[i], skip_special_tokens=True) for i in range(logits_ids.size(0))]\n",
    "            \n",
    "            print('##### ', tag, '( logits_len:', logits_len,', ids_len:', ids_len, ' )')\n",
    "            print('First five logits_ids:', logits_ids[0][:5].tolist(), ', First five ids:', ids[0][:5].tolist())\n",
    "            print('###### logit text:',logits_text[0][:100])\n",
    "            print('###### ids_text:',ids_text[0][:100])\n",
    "    print_tensor(logits, name=tag+'(logits)')\n",
    "    print_tensor(ids, name=tag+'(ids)')\n",
    "\n",
    "def print_tensor(tensor, name=None):\n",
    "    if log_tensor:\n",
    "        if name is None:\n",
    "            # Try to infer the variable name from the caller's local variables.\n",
    "            frame = inspect.currentframe().f_back\n",
    "            # Look for local variables that are the same object as tensor.\n",
    "            names = [var_name for var_name, var_val in frame.f_locals.items() if var_val is tensor]\n",
    "            name = names[0] if names else \"tensor\"\n",
    "        if not torch.is_floating_point(tensor):\n",
    "            mean_val = tensor.float().mean().item()\n",
    "        else:\n",
    "            mean_val = tensor.mean().item()\n",
    "        print(name, tensor.shape, '(min=',tensor.min().item(), ', avg=', mean_val, ', max=',tensor.max().item(), ')')\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def match_shape(actual, expected):\n",
    "    if len(actual) != len(expected):\n",
    "        return False\n",
    "    return all(e == a or e is None or e == -1 for a, e in zip(actual, expected))\n",
    "\n",
    "def check_shape(self, expected_shape):\n",
    "    if checking_shape:\n",
    "        if not match_shape(self.shape, expected_shape):\n",
    "            raise ValueError(f\"Shape mismatch! Got {self.shape}, expected {expected_shape}\")\n",
    "    return self\n",
    "\n",
    "def check_range(self, min_val, max_val, not_values=None):\n",
    "    if checking_range:\n",
    "        # Range check\n",
    "        in_range = (self >= min_val) & (self <= max_val)\n",
    "    \n",
    "        # Optional exclusion check\n",
    "        if not_values is not None:\n",
    "            for v in not_values:\n",
    "                in_range &= self != v\n",
    "    \n",
    "        if not torch.all(in_range):\n",
    "            raise ValueError(f\"Tensor check_range failed: values not in range [{min_val}, {max_val}] or contain excluded {not_values}\")\n",
    "    \n",
    "    return self \n",
    "\n",
    "torch.Tensor.log = print_tensor\n",
    "torch.Tensor.check_shape = check_shape\n",
    "torch.Tensor.check_range = check_range\n",
    "            \n",
    "def samping(model, tokenizer, device, epoch, writer, sample_prompt, expected):\n",
    "    # Include attention_mask in the tokenization\n",
    "    sample_prompt = f\"### Instruction\\n\\n{sample_prompt}\\n\\n### Response\"\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Pass the attention_mask and explicitly set pad_token_id to eos_token_id for reliable generation\n",
    "    full_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    sample_text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    sample_text = sample_text.strip()\n",
    "    if (log_content):print(f\"Sample Output (Epoch {epoch + 1}): {sample_text}\")\n",
    "    if (log_content):print(\"Expected:\", expected)\n",
    "    writer.add_text(\"Sample Output\", f\"Epoch {epoch + 1}: {sample_text}\", epoch)\n",
    "\n",
    "def write_time_file(folder):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"{now}.txt\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(\"This is a dummy file.\\n\")\n",
    "    \n",
    "    print(f\"Dummy file written: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66503da6-6aa2-4c25-9276-84c2ee33040c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "# ------------------------------------------------\n",
    "def write_weight_state(model, writer, step, log_group):\n",
    "    for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "        if param.requires_grad:\n",
    "            weight_mean = param.data.mean().item()\n",
    "            weight_std = param.data.std().item()\n",
    "\n",
    "            writer.add_scalar(f\"{log_group}/{idx}_{name}/mean\", weight_mean, step)\n",
    "            writer.add_scalar(f\"{log_group}/{idx}_{name}/std\", weight_std, step)\n",
    "\n",
    "def change_grad(model, layer_start, layer_end, multiple=0.01):\n",
    "    for idx, val in enumerate(model.named_parameters()):\n",
    "        name, param = val\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        if layer_start <= idx < layer_end:\n",
    "                param.grad.mul_(multiple)\n",
    "            \n",
    "class TrainLayerUpdater:\n",
    "    def __init__(self, model, train_layer):\n",
    "        self.model = model.model\n",
    "        self.train_layer = train_layer\n",
    "\n",
    "    def get_layer_params(self):\n",
    "        if self.train_layer == TrainLayers.FULL_LAYER:\n",
    "            params = list(self.model.parameters())\n",
    "        elif self.train_layer == TrainLayers.TWO_FRONT_LAYER:\n",
    "            return [p for layer in self.model.layers[:2] for p in layer.parameters()]\n",
    "        elif self.train_layer == TrainLayers.ODD_LAYER:\n",
    "            params = [p for idx, layer in enumerate(self.model.layers) if idx % 2 == 1 for p in layer.parameters()]\n",
    "        elif self.train_layer == TrainLayers.EVEN_LAYER:\n",
    "            params = [p for idx, layer in enumerate(self.model.layers) if idx % 2 == 0 for p in layer.parameters()]\n",
    "        elif self.train_layer == TrainLayers.SWITCH_PAIR_LAYER:\n",
    "            idx = self.config.current_layer_index\n",
    "            params = []\n",
    "            if idx < len(self.model.layers):\n",
    "                params.extend(list(self.model.layers[idx].parameters()))\n",
    "            if (idx + 1) < len(self.model.layers):\n",
    "                params.extend(list(self.model.layers[idx + 1].parameters()))\n",
    "            # Cycle the current_layer_index for the next update\n",
    "            self.config.current_layer_index = (idx + 2) % len(self.model.layers) \n",
    "        else:\n",
    "            raise ValueError(\"Invalid train_layer configuration\")\n",
    "\n",
    "        # add first two layer whatever\n",
    "        #params.extend([p for layer in self.model.layers[:2] for p in layer.parameters()]) # params\n",
    "        # remove first two layer\n",
    "        params = [x for x in params if x not in self.model.layers[:2]] # \n",
    "        return params\n",
    "\n",
    "    def update_optimizer_and_requires_grad(self, optimizer):\n",
    "        # Get the new set of parameters for training.\n",
    "        new_params = self.get_layer_params()\n",
    "        new_params_set = set(new_params)\n",
    "        \n",
    "        # Update requires_grad flags for all model parameters.\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = param in new_params_set\n",
    "\n",
    "        # Update the optimizer parameter group (assuming a single group).\n",
    "        optimizer.param_groups[0]['params'] = list(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f77f88a-17d5-402b-818e-8199bffe0759",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tensor utility\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "def pad_to_match(tensor_a, tensor_b, padding_value=0):\n",
    "    # Determine the current sequence lengths\n",
    "    seq_len_a = tensor_a.size(1)\n",
    "    seq_len_b = tensor_b.size(1)\n",
    "\n",
    "    if seq_len_a > seq_len_b:\n",
    "        max_seq_len = max(seq_len_a, seq_len_b)\n",
    "    \n",
    "        # Define padding function\n",
    "        def pad_tensor(tensor, target_length):\n",
    "            pad_length = target_length - tensor.size(1)\n",
    "            if pad_length > 0:\n",
    "                padding = (0, 0) * (tensor.dim() - 2) + (0, pad_length)\n",
    "                tensor = F.pad(tensor, padding, value=padding_value)\n",
    "            return tensor\n",
    "    \n",
    "        # Pad both tensors to the maximum sequence length\n",
    "        tensor_a_padded = pad_tensor(tensor_a, max_seq_len)\n",
    "        tensor_b_padded = pad_tensor(tensor_b, max_seq_len)\n",
    "    else:\n",
    "        tensor_b_padded = tensor_b[:, :seq_len_a]\n",
    "        tensor_a_padded = tensor_a\n",
    "\n",
    "    return tensor_a_padded, tensor_b_padded\n",
    "\n",
    "\n",
    "\n",
    "def selective_log_softmax(logits, input_ids, tokenizer):\n",
    "    # Ensure input_ids are on the same device as logits\n",
    "    if input_ids.device != logits.device:\n",
    "        input_ids = input_ids.to(logits.device)\n",
    "\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    if input_ids.size(1) > log_probs.size(1):\n",
    "        input_ids = input_ids[:, :log_probs.size(1)]\n",
    "\n",
    "    # Gather log probabilities corresponding to input_ids\n",
    "    selected_log_probs = log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    if (log_content):\n",
    "        input_text = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        print(\"Input Texts:\")\n",
    "        for text in input_text:\n",
    "            print(text)\n",
    "        logits_ids = logits.argmax(dim=-1)\n",
    "        logit_text = tokenizer.batch_decode(logits_ids, skip_special_tokens=True)\n",
    "        print(\"\\nLogit Texts:\")\n",
    "        for text in logit_text:\n",
    "            print(text)\n",
    "\n",
    "    return selected_log_probs\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    layer_index = 1\n",
    "    original_first_block = self.model.layers[layer_index]\n",
    "\n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "\n",
    "    self.model.layers.insert(layer_index, new_block)\n",
    "\n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def cut_tensors_by_min(a: torch.Tensor, b: torch.Tensor, dim: int):\n",
    "    assert a.dim() > dim and b.dim() > dim, \"Specified dim exceeds tensor rank\"\n",
    "\n",
    "    min_length = min(a.size(dim), b.size(dim))\n",
    "    a_cut = torch.narrow(a, dim, 0, min_length)\n",
    "    b_cut = torch.narrow(b, dim, 0, min_length)\n",
    "    return a_cut, b_cut\n",
    "\n",
    "def cut_ids_on_eos_tensor(full_ids, eos_token_id):\n",
    "    processed_ids = []\n",
    "    for seq in full_ids:\n",
    "        eos_positions = (seq == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        if eos_positions.numel() > 0:\n",
    "            first_eos_index = eos_positions[0].item()\n",
    "            processed_ids.append(seq[:first_eos_index])\n",
    "        else:\n",
    "            processed_ids.append(seq)\n",
    "    return processed_ids\n",
    "    \n",
    "def cut_ids_on_eos(generated_ids, eos_token_id):\n",
    "    processed_ids = []\n",
    "    for seq in generated_ids:\n",
    "        if eos_token_id in seq:\n",
    "            # Truncate the sequence at the first occurrence of the EOS token\n",
    "            first_eos_index = seq.index(eos_token_id)\n",
    "            processed_ids.append(seq[:first_eos_index])\n",
    "        else:\n",
    "            processed_ids.append(seq)\n",
    "    return processed_ids\n",
    "\n",
    "\n",
    "def shift_ids_with_logits(ids, shift_logits):\n",
    "    shift_ids = torch.cat([ids[:, 1:], torch.argmax(shift_logits[:, -1, :], dim=-1).unsqueeze(1)], dim=1)\n",
    "    return shift_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scheduling\n",
    "# ------------------------------------------------\n",
    "\n",
    "def train_and_evaluate(model, ref_model, lr, optimizer, device, num_epochs, group_size,\n",
    "                       num_grpo, epsilon, kl_lambda, scaler, save_epochs, start_epoch,\n",
    "                       scheduler, gradient_accumulation_step, skip_validation_step=False, \n",
    "                       temperature=1.0,\n",
    "                       train_layer_updater=None, category_count_start=1, is_finding_opt=False):\n",
    "    global learning_name_time \n",
    "    lr_str = f\"{lr:.5g}\"\n",
    "    kl_lambda_str = f\"{kl_lambda:.5g}\"\n",
    "    epsilon_str = f\"{epsilon:.5g}\"\n",
    "    temperature_str = f\"{temperature:.5g}\"\n",
    "    log_dir = f\"runs/{learning_name_time}_{lr_str}_{kl_lambda_str}_{epsilon_str}_{num_grpo}_{temperature_str}\" \n",
    "    write_time_file(log_dir)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    global batch_size\n",
    "    reward_work = RewardWorkPool(group_size*batch_size)\n",
    "\n",
    "    # --- Generate sample output text after each epoch ---\n",
    "    model.eval()  # Set to eval mode for generation.\n",
    "    with torch.no_grad():\n",
    "        samping(model, tokenizer, device, 0, writer,\n",
    "                \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\",\n",
    "                \"```\\n>>> (prompt)\\n```\")\n",
    "        samping(model, tokenizer, device, 0, writer,\n",
    "                \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\",\n",
    "                \"Custom Clang-repl takes only one line input.\")\n",
    "    model.train()  # Switch back to training mode.\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    mean_reward = 0\n",
    "    sum_mean_reward = 0\n",
    "    categories = get_all_categories()\n",
    "    category_size = len(categories)\n",
    "    cur_category_count = category_count_start\n",
    "    last_category_count = cur_category_count\n",
    "    switch_pair_layer = 0\n",
    "\n",
    "    print(\"Total categories:\", category_size, categories)\n",
    "\n",
    "    dataloader = get_train_dataloader(categories[:cur_category_count])\n",
    "    print(\"Data counts:\", len(dataloader))\n",
    "    val_dataloader = get_val_dataloader(categories[:cur_category_count])\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print_step(f\"Epoch {epoch+1}/{num_epochs} - Validation\", main_step=True)\n",
    "\n",
    "        if train_layer_updater.train_layer == TrainLayers.SWITCH_PAIR_LAYER:\n",
    "            if switch_pair_layer >=4 and mean_reward > 2.0 and cur_category_count < category_size:\n",
    "                switch_pair_layer = 0\n",
    "                if not is_finding_opt: cur_category_count += 1\n",
    "            else:\n",
    "                train_layer_updater.update_optimizer_and_requires_grad(optimizer)\n",
    "                switch_pair_layer += 1\n",
    "        else:\n",
    "            if mean_reward > 2.0 and cur_category_count < category_size:\n",
    "                if not is_finding_opt: cur_category_count += 1\n",
    "\n",
    "        print_memory(20)\n",
    "        old_model = copy.deepcopy(model).half()\n",
    "        old_model.eval()\n",
    "        for param in old_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print_memory(21)\n",
    "\n",
    "        if not skip_validation_step:\n",
    "            print(\"Validation Start ....\")\n",
    "            # Run validation (no parameter updates).\n",
    "            _, _, _ = run(model, old_model, ref_model, val_dataloader, optimizer, device, tokenizer,\n",
    "                                 group_size, epsilon, kl_lambda, scaler, writer, global_step, reward_work=reward_work,\n",
    "                                 log_group=\"validation\", scheduler=None,\n",
    "                                 gradient_accumulation_step=gradient_accumulation_step, is_validation=True,\n",
    "                                 temperature=temperature)\n",
    "            print(\"Validation End ....\")\n",
    "\n",
    "        print(\"Training Start ....\")\n",
    "        # Loop over gradient groups for training.\n",
    "        for grpo_idx in range(num_grpo):\n",
    "            print_step(f\"Epoch {epoch+1}/{num_epochs} - Training Gradient Group {grpo_idx+1}/{num_grpo}, category include ={categories[cur_category_count-1]}\", main_step=True)\n",
    "            \n",
    "            if last_category_count != cur_category_count:\n",
    "                last_category_count = cur_category_count\n",
    "                dataloader = get_train_dataloader(categories[:cur_category_count])\n",
    "                print(\"Data counts:\", len(dataloader))\n",
    "                \n",
    "            loss, mean_reward, global_step = run(model, old_model, ref_model, dataloader, optimizer, device, tokenizer,\n",
    "                                      group_size, epsilon, kl_lambda, scaler, writer, global_step, reward_work=reward_work,\n",
    "                                      log_group=\"Training\", scheduler=scheduler,\n",
    "                                      gradient_accumulation_step=gradient_accumulation_step, is_validation=False,\n",
    "                                    temperature=temperature)\n",
    "            \n",
    "            running_loss += loss\n",
    "            sum_mean_reward += mean_reward\n",
    "        old_model = None\n",
    "        print_step(\"7. End Epoch\")\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        avg_reward = sum_mean_reward / len(dataloader)\n",
    "        if log_step: print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Epoch/Average_Loss\", avg_loss, epoch + 1)\n",
    "        print(\"Training End ....\")\n",
    "\n",
    "        # Save latest checkpoint.\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(last_checkpoint_path), exist_ok=True)\n",
    "        torch.save(checkpoint, last_checkpoint_path)\n",
    "\n",
    "        # Optionally save checkpoint on specific epochs.\n",
    "        if save_epochs is not None and epoch % save_epochs == 0:\n",
    "            checkpoint_dir = checkpoint_dir_pre + str(epoch + 1)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            if log_step: print(f\"Checkpoint saved at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "            # Save tokenizer once if not already saved.\n",
    "            tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "            if not os.path.exists(tokenizer_save_dir):\n",
    "                os.makedirs(tokenizer_save_dir, exist_ok=True)\n",
    "                tokenizer.save_pretrained(tokenizer_save_dir)\n",
    "                if log_step: print(\"Tokenizer saved.\")\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "    return avg_reward\n",
    "\n",
    "def train(\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        kl_lambda,\n",
    "        epsilon,\n",
    "        num_grpo,\n",
    "        group_size,\n",
    "        warming_up_step,\n",
    "        gradient_accumulation_step,\n",
    "        save_epochs=None,\n",
    "        skip_validation_step=False,\n",
    "        is_finding_opt=False,\n",
    "        temperature=1.0,\n",
    "        category_count_start=1):\n",
    "    global use_reference_model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check if a latest checkpoint exists to load model and optimizer states\n",
    "    if os.path.exists(last_checkpoint_path) and not is_finding_opt and not use_reference_model:\n",
    "        print(\"==USING CHECK POINT MODEL==\")\n",
    "        print_memory(1)\n",
    "        checkpoint = torch.load(last_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "        _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        config = copy.deepcopy(_model.config)\n",
    "        _model = None\n",
    "        print_memory(2)\n",
    "        config.num_hidden_layers += 2\n",
    "        config.max_position_embeddings=512\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        print_memory(3)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        print_memory(4)\n",
    "        train_layer_updater = TrainLayerUpdater(model, train_layer) \n",
    "        #optimizer = Adafactor(train_layer_updater.get_layer_params(), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        optimizer = torch.optim.AdamW(train_layer_updater.get_layer_params(), lr=lr)\n",
    "        train_layer_updater.update_optimizer_and_requires_grad(optimizer)\n",
    "        #ArithmeticErroroptimizer = bnb.optim.AdamW8bit(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        if log_step: print(f\"Loaded checkpoint from last {last_checkpoint_path} at epoch {start_epoch}\")\n",
    "        print_memory(7)\n",
    "    else:\n",
    "        if os.path.exists(ref_checkpoint_path):\n",
    "            print(\"==USING REFERENCE MODEL==\")\n",
    "            print_memory(1)\n",
    "            checkpoint = torch.load(ref_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "            _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            config = copy.deepcopy(_model.config)\n",
    "            _model = None\n",
    "            print_memory(2)\n",
    "            config.num_hidden_layers += 2\n",
    "            config.max_position_embeddings=512\n",
    "            model = AutoModelForCausalLM.from_config(config)\n",
    "            print_memory(3)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.to(device)\n",
    "            print_memory(4)\n",
    "            train_layer_updater = TrainLayerUpdater(model, train_layer) \n",
    "            #optimizer = Adafactor(train_layer_updater.get_layer_params(), lr=lr, relative_step=False, scale_parameter=False)\n",
    "            optimizer = torch.optim.AdamW(train_layer_updater.get_layer_params(), lr=lr)\n",
    "            train_layer_updater.update_optimizer_and_requires_grad(optimizer)\n",
    "            #optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "            #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = 0 #checkpoint.get('epoch', 0)\n",
    "            if log_step: print(f\"Loaded checkpoint from reference {ref_checkpoint_path} at epoch {start_epoch}\")\n",
    "            print_memory(7)\n",
    "\n",
    "        else:\n",
    "            assert False, \"prompt_last_checkpoint_path must exist\"\n",
    "\n",
    "    dups = check_optimizer_duplicates(optimizer)\n",
    "    if dups:\n",
    "        print(\"Warning: The optimizer contains duplicate parameters!\")\n",
    "        print(f\"Duplicate parameter count: {len(dups)}\")\n",
    "    else:\n",
    "        print(\"No duplicate parameters found in the optimizer.\")\n",
    "\n",
    "    # Clear cached memory that is no longer used\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print_memory(9)\n",
    "\n",
    "    # Reference model (for KL)\n",
    "    ref_model = None\n",
    "    ref_model = copy.deepcopy(model).half().eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    print_memory(10)\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def lr_schedule(step):\n",
    "        # Linear warm-up to 1.0, then constant\n",
    "        return min(1.0, step / warming_up_step)\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_schedule)\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        lr=lr,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        group_size=group_size,\n",
    "        num_grpo=num_grpo,\n",
    "        epsilon=epsilon,\n",
    "        kl_lambda=kl_lambda,\n",
    "        scaler=scaler,\n",
    "        save_epochs=save_epochs,\n",
    "        start_epoch=start_epoch,\n",
    "        scheduler=scheduler,\n",
    "        gradient_accumulation_step=gradient_accumulation_step,\n",
    "        skip_validation_step=skip_validation_step,\n",
    "        temperature=temperature,\n",
    "        train_layer_updater=train_layer_updater,\n",
    "        category_count_start=category_count_start,\n",
    "        is_finding_opt=is_finding_opt\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global group_size\n",
    "    num_epochs, lr, kl_lambda, epsilon, num_grpo, warming_up_step, gradient_accumulation_step, temperature, category_count_start = object_hiper_param(trial)\n",
    "\n",
    "    print(\n",
    "        f\"[Optuna] Trial hyperparameters -> lr: {lr}, kl_lambda: {kl_lambda}, epsilon: {epsilon}, num_grpo: {num_grpo}, warming_up_step: {warming_up_step}, gradient_accumulation_step: {gradient_accumulation_step}, temperature: {temperature}, category_count_start: {category_count_start}\")\n",
    "    return train(\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        kl_lambda=kl_lambda,\n",
    "        epsilon=epsilon,\n",
    "        num_grpo=num_grpo,\n",
    "        group_size=group_size,\n",
    "        warming_up_step=warming_up_step, \n",
    "        gradient_accumulation_step=gradient_accumulation_step,\n",
    "        skip_validation_step=True,\n",
    "        is_finding_opt=True,\n",
    "        temperature=temperature,\n",
    "        category_count_start=category_count_start\n",
    "    )\n",
    "\n",
    "\n",
    "def main(\n",
    "        skip_validation_step,\n",
    "        objective,\n",
    "        is_finding_opt=False,\n",
    "        category_count_start=1):\n",
    "    if is_finding_opt:\n",
    "        # Create study to minimize final loss\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "\n",
    "        print(\"Study completed!\")\n",
    "        print(\"Best trial:\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Value: {best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"#    {key}: {value}\")\n",
    "        with open(\"hiper_param.json\", \"w\") as f:\n",
    "            json.dump(dict(best_trial.params.items()), f, indent=4)\n",
    "    else:\n",
    "        global num_epochs, lr, kl_lambda, epsilon, num_grpo, save_epochs, warming_up_step, gradient_accumulation_step, temperature\n",
    "        train(\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            kl_lambda=kl_lambda,\n",
    "            epsilon=epsilon,\n",
    "            num_grpo=num_grpo,\n",
    "            group_size=group_size,\n",
    "            save_epochs=save_epochs,\n",
    "            warming_up_step=warming_up_step,\n",
    "            gradient_accumulation_step=gradient_accumulation_step,\n",
    "            skip_validation_step=skip_validation_step,\n",
    "            temperature=temperature,\n",
    "            category_count_start=category_count_start\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb6833-2b7e-41bd-9166-a97df258dadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 09:59:10,037] A new study created in memory with name: no-name-4dae1035-7529-4793-ab91-f30acf00d3b4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reasoning logic....\n",
      "[Optuna] Trial hyperparameters -> lr: 0.0002581746143279661, kl_lambda: 0.11056613361305812, epsilon: 0.09572867840913253, num_grpo: 1, warming_up_step: 1, gradient_accumulation_step: 1, temperature: 0.5881472509842769, category_count_start: 1\n",
      "==USING REFERENCE MODEL==\n",
      "No duplicate parameters found in the optimizer.\n",
      "Dummy file written: runs/starcoder2-3b_reasoning_FULL_LAYER_30_09-59_0.00025817_0.11057_0.095729_1_0.58815/2025-03-30_10-00-02.txt\n",
      "Total categories: 9 ['simple arithmetic', 'simple if', 'simple loop', 'loop and if', 'simple state', 'recursive function', 'pointer manipulation', 'string manipulation', 'sort algorithm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4b337d003f48a8b2287bc5bf13012a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfab19fe36964c469f8ddd200a254e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data counts: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0936108bbc094e24ac3f059c2cd423bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6886f997bb478ea972a201e32c3eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 - Validation\n",
      "Training Start ....\n",
      "Epoch 1/4 - Training Gradient Group 1/1, category include =simple arithmetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3565.04248046875 , max= 48567 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3565.04248046875 , max= 48567 )\n",
      "respone_ids torch.Size([4, 466]) (min= 45 , avg= 3667.2548828125 , max= 48567 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -32.46875 , avg= 0.1549072265625 , max= 42.5 )\n",
      "response_truncated_logits torch.Size([4, 466, 49152]) (min= -32.46875 , avg= 0.227783203125 , max= 42.5 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -32.46875 , avg= 0.1549072265625 , max= 42.5 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3564.5419921875 , max= 48567 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.125 , avg= 0.315185546875 , max= 45.34375 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3564.5419921875 , max= 48567 )\n",
      "model response(logits) torch.Size([4, 466, 49152]) (min= -32.46875 , avg= 0.227783203125 , max= 42.5 )\n",
      "model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3667.2548828125 , max= 48567 )\n",
      "model_log_logits torch.Size([4, 466]) (min= -14.375030517578125 , avg= -0.16529260575771332 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 466, 49152]) (min= -25.125 , avg= 0.361328125 , max= 45.34375 )\n",
      "old model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3667.2548828125 , max= 48567 )\n",
      "old_model_log_logits torch.Size([4, 466]) (min= -15.313397407531738 , avg= -0.1990078091621399 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.0009471949888393283 , avg= 612.6781616210938 , max= 1060597.375 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -21.406251907348633 , avg= -0.2779674530029297 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -22.03125 , avg= -0.32148274779319763 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.348608136177063 , avg= 0.03617575764656067 , max= 6.1158342361450195 )\n",
      "kl_div torch.Size([4]) (min= 0.02565106563270092 , avg= 0.03617575764656067 , max= 0.04336627200245857 )\n",
      "completion_mask torch.Size([4, 466]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.7400000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.7400000095367432 , avg= 1.7400000095367432 , max= 1.7400000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5773502588272095 , avg= 0.5773502588272095 , max= 0.5773502588272095 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.7400000095367432 std_rewards: 0.5773502588272095 Rewards: [2.24, 2.24, 1.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.7400000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.8658754229545593 , avg= 0.0 , max= 0.8658754229545593 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.0009471949888393283 , avg= 612.6781616210938 , max= 1060597.375 )\n",
      "before A_hat multiply torch.Size([4, 466]) (min= -1060597.375 , avg= -584.4771118164062 , max= -0.0009471949888393283 )\n",
      "_grouped_ppo_loss torch.Size([4, 466]) (min= -0.9487645030021667 , avg= 505.23297119140625 , max= 918345.1875 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.8553625345230103 , avg= 505.23297119140625 , max= 2005.3260498046875 )\n",
      "_combined_loss torch.Size([4]) (min= -0.8525263667106628 , avg= 505.23699951171875 , max= 2005.3306884765625 )\n",
      "combined_loss torch.Size([]) (min= 505.23699951171875 , avg= 505.23699951171875 , max= 505.23699951171875 )\n",
      "Final Loss: 505.23699951171875 , ppo_loss: 505.23297119140625 , kl_div: 0.03617575764656067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                                                                   | 1/10 [00:19<02:57, 19.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3403.22412109375 , max= 41623 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3403.22412109375 , max= 41623 )\n",
      "respone_ids torch.Size([4, 449]) (min= 45 , avg= 3503.42041015625 , max= 41623 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -35.25 , avg= 0.673828125 , max= 45.0 )\n",
      "response_truncated_logits torch.Size([4, 449, 49152]) (min= -35.25 , avg= 0.76220703125 , max= 45.0 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -35.25 , avg= 0.673828125 , max= 45.0 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3400.77978515625 , max= 41623 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -27.765625 , avg= 0.51025390625 , max= 46.34375 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3400.77978515625 , max= 41623 )\n",
      "model response(logits) torch.Size([4, 449, 49152]) (min= -35.25 , avg= 0.76220703125 , max= 45.0 )\n",
      "model response(ids) torch.Size([4, 449]) (min= 45 , avg= 3503.42041015625 , max= 41623 )\n",
      "model_log_logits torch.Size([4, 449]) (min= -15.85076904296875 , avg= -0.22681286931037903 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 449, 49152]) (min= -27.765625 , avg= 0.52099609375 , max= 46.34375 )\n",
      "old model response(ids) torch.Size([4, 449]) (min= 45 , avg= 3503.42041015625 , max= 41623 )\n",
      "old_model_log_logits torch.Size([4, 449]) (min= -22.781274795532227 , avg= -0.27624449133872986 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 1.3064669701634557e-07 , avg= 3850.87255859375 , max= 6625081.5 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -20.597728729248047 , avg= -0.4670431315898895 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -22.781274795532227 , avg= -0.5022138357162476 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3674285411834717 , avg= 0.08620141446590424 , max= 15.85076904296875 )\n",
      "kl_div torch.Size([4]) (min= 0.06285612285137177 , avg= 0.08620141446590424 , max= 0.10174507647752762 )\n",
      "completion_mask torch.Size([4, 449]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.7400000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.7400000095367432 , avg= 1.7400000095367432 , max= 1.7400000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5773502588272095 , avg= 0.5773502588272095 , max= 0.5773502588272095 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.7400000095367432 std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 2.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.7400000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.8658754229545593 , avg= 0.0 , max= 0.8658754229545593 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 1.3064669701634557e-07 , avg= 3850.87255859375 , max= 6625081.5 )\n",
      "before A_hat multiply torch.Size([4, 449]) (min= -212700.46875 , avg= -141.42889404296875 , max= -1.3064669701634557e-07 )\n",
      "_grouped_ppo_loss torch.Size([4, 449]) (min= -0.9487645030021667 , avg= 121.61613464355469 , max= 184172.109375 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.8441548347473145 , avg= 121.61611938476562 , max= 465.4475402832031 )\n",
      "_combined_loss torch.Size([4]) (min= -0.8330994844436646 , avg= 121.62564086914062 , max= 465.4544982910156 )\n",
      "combined_loss torch.Size([]) (min= 121.62564086914062 , avg= 121.62564086914062 , max= 121.62564086914062 )\n",
      "Final Loss: 121.62564086914062 , ppo_loss: 121.61611938476562 , kl_div: 0.08620141446590424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                                             | 2/10 [00:36<02:25, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3189.853515625 , max= 44260 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3189.853515625 , max= 44260 )\n",
      "respone_ids torch.Size([4, 466]) (min= 45 , avg= 3267.33056640625 , max= 44260 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -32.875 , avg= 0.09521484375 , max= 42.53125 )\n",
      "response_truncated_logits torch.Size([4, 466, 49152]) (min= -32.875 , avg= 0.1793212890625 , max= 42.53125 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -32.875 , avg= 0.09521484375 , max= 42.53125 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3187.681640625 , max= 44260 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.015625 , avg= 0.313232421875 , max= 45.875 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3187.681640625 , max= 44260 )\n",
      "model response(logits) torch.Size([4, 466, 49152]) (min= -32.875 , avg= 0.1793212890625 , max= 42.53125 )\n",
      "model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3267.33056640625 , max= 44260 )\n",
      "model_log_logits torch.Size([4, 466]) (min= -13.32853889465332 , avg= -0.18501858413219452 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 466, 49152]) (min= -25.015625 , avg= 0.351318359375 , max= 45.875 )\n",
      "old model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3267.33056640625 , max= 44260 )\n",
      "old_model_log_logits torch.Size([4, 466]) (min= -17.773454666137695 , avg= -0.20089560747146606 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.00022792484378442168 , avg= 14.114476203918457 , max= 9649.1162109375 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -19.243724822998047 , avg= -0.2870715260505676 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -21.484376907348633 , avg= -0.3186838626861572 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3600562512874603 , avg= 0.0336066372692585 , max= 5.030936241149902 )\n",
      "kl_div torch.Size([4]) (min= 0.013132067397236824 , avg= 0.0336066372692585 , max= 0.06296071410179138 )\n",
      "completion_mask torch.Size([4, 466]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -1.0957286357879639 , avg= -1.0957286357879639 , max= -1.0957286357879639 )\n",
      "_combined_loss torch.Size([4]) (min= -1.0942766666412354 , avg= -1.092012882232666 , max= -1.0887672901153564 )\n",
      "combined_loss torch.Size([]) (min= -1.092012882232666 , avg= -1.092012882232666 , max= -1.092012882232666 )\n",
      "Final Loss: -1.092012882232666 , ppo_loss: -1.0957286357879639 , kl_div: 0.0336066372692585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                        | 3/10 [00:54<02:05, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3725.986328125 , max= 48567 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3725.986328125 , max= 48567 )\n",
      "respone_ids torch.Size([4, 468]) (min= 45 , avg= 3804.085693359375 , max= 48567 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -33.6875 , avg= 0.431884765625 , max= 42.9375 )\n",
      "response_truncated_logits torch.Size([4, 468, 49152]) (min= -33.6875 , avg= 0.493408203125 , max= 42.9375 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -33.6875 , avg= 0.431884765625 , max= 42.9375 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3723.96240234375 , max= 48567 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -27.109375 , avg= 0.385498046875 , max= 47.5 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3723.96240234375 , max= 48567 )\n",
      "model response(logits) torch.Size([4, 468, 49152]) (min= -33.6875 , avg= 0.493408203125 , max= 42.9375 )\n",
      "model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3804.085693359375 , max= 48567 )\n",
      "model_log_logits torch.Size([4, 468]) (min= -12.859915733337402 , avg= -0.20204050838947296 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 468, 49152]) (min= -25.703125 , avg= 0.402587890625 , max= 47.5 )\n",
      "old model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3804.085693359375 , max= 48567 )\n",
      "old_model_log_logits torch.Size([4, 468]) (min= -21.296875 , avg= -0.25012609362602234 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 6.411459617083892e-05 , avg= 17935.57421875 , max= 32903376.0 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -20.231224060058594 , avg= -0.3371634781360626 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -21.359376907348633 , avg= -0.39064228534698486 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3642284870147705 , avg= 0.06073831766843796 , max= 12.8125 )\n",
      "kl_div torch.Size([4]) (min= 0.028989670798182487 , avg= 0.06073831766843796 , max= 0.10194763541221619 )\n",
      "completion_mask torch.Size([4, 468]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 0.20999999344348907 , avg= 1.7325000762939453 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.7325000762939453 , avg= 1.7325000762939453 , max= 1.7325000762939453 )\n",
      "std_rewards torch.Size([4]) (min= 1.0149999856948853 , avg= 1.0149999856948853 , max= 1.0149999856948853 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.7325000762939453 std_rewards: 1.0149999856948853 Rewards: [0.21000000000000005, 2.24, 2.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 0.20999999344348907 , avg= 1.7325000762939453 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.4998522996902466 , avg= -7.450580596923828e-08 , max= 0.49995067715644836 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 6.411459617083892e-05 , avg= 17935.57421875 , max= 32903376.0 )\n",
      "before A_hat multiply torch.Size([4, 468]) (min= -359975.34375 , avg= -268.6206970214844 , max= -6.411459617083892e-05 )\n",
      "_grouped_ppo_loss torch.Size([4, 468]) (min= -0.5478102564811707 , avg= 401.43096923828125 , max= 539909.875 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.4940260052680969 , avg= 401.4309387207031 , max= 1607.1842041015625 )\n",
      "_combined_loss torch.Size([4]) (min= -0.4908207356929779 , avg= 401.4376525878906 , max= 1607.1915283203125 )\n",
      "combined_loss torch.Size([]) (min= 401.4376525878906 , avg= 401.4376525878906 , max= 401.4376525878906 )\n",
      "Final Loss: 401.4376525878906 , ppo_loss: 401.4309387207031 , kl_div: 0.06073831766843796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                  | 4/10 [01:12<01:47, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 2647.8427734375 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 2647.8427734375 , max= 36923 )\n",
      "respone_ids torch.Size([4, 466]) (min= 45 , avg= 2717.0654296875 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -39.03125 , avg= 0.3935546875 , max= 42.28125 )\n",
      "response_truncated_logits torch.Size([4, 466, 49152]) (min= -39.03125 , avg= 0.47021484375 , max= 42.28125 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -39.03125 , avg= 0.3935546875 , max= 42.28125 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2645.486328125 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -26.359375 , avg= 0.409912109375 , max= 45.6875 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2645.486328125 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 466, 49152]) (min= -39.03125 , avg= 0.47021484375 , max= 42.28125 )\n",
      "model response(ids) torch.Size([4, 466]) (min= 45 , avg= 2717.0654296875 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 466]) (min= -26.894569396972656 , avg= -0.1298256665468216 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 466, 49152]) (min= -26.359375 , avg= 0.453369140625 , max= 45.6875 )\n",
      "old model response(ids) torch.Size([4, 466]) (min= 45 , avg= 2717.0654296875 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 466]) (min= -30.333988189697266 , avg= -0.13537608087062836 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.0002115609240718186 , avg= 663.31640625 , max= 1226205.375 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -26.894569396972656 , avg= -0.23357081413269043 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -30.333988189697266 , avg= -0.25743919610977173 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3191543221473694 , avg= 0.021450195461511612 , max= 4.279670238494873 )\n",
      "kl_div torch.Size([4]) (min= 0.017240900546312332 , avg= 0.021450193598866463 , max= 0.02621135488152504 )\n",
      "completion_mask torch.Size([4, 466]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -1.0957286357879639 , avg= -1.0957286357879639 , max= -1.0957286357879639 )\n",
      "_combined_loss torch.Size([4]) (min= -1.0938223600387573 , avg= -1.093356966972351 , max= -1.0928305387496948 )\n",
      "combined_loss torch.Size([]) (min= -1.093356966972351 , avg= -1.093356966972351 , max= -1.093356966972351 )\n",
      "Final Loss: -1.093356966972351 , ppo_loss: -1.0957286357879639 , kl_div: 0.021450193598866463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                            | 5/10 [01:30<01:29, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3237.087890625 , max= 38475 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3237.087890625 , max= 38475 )\n",
      "respone_ids torch.Size([4, 449]) (min= 45 , avg= 3351.951171875 , max= 38475 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -32.9375 , avg= 0.501953125 , max= 42.3125 )\n",
      "response_truncated_logits torch.Size([4, 449, 49152]) (min= -32.9375 , avg= 0.5927734375 , max= 42.3125 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -32.9375 , avg= 0.501953125 , max= 42.3125 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3236.294921875 , max= 38475 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -27.46875 , avg= 0.337890625 , max= 44.65625 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3236.294921875 , max= 38475 )\n",
      "model response(logits) torch.Size([4, 449, 49152]) (min= -32.9375 , avg= 0.5927734375 , max= 42.3125 )\n",
      "model response(ids) torch.Size([4, 449]) (min= 45 , avg= 3351.951171875 , max= 38475 )\n",
      "model_log_logits torch.Size([4, 449]) (min= -18.953176498413086 , avg= -0.19619476795196533 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 449, 49152]) (min= -27.46875 , avg= 0.365234375 , max= 44.65625 )\n",
      "old model response(ids) torch.Size([4, 449]) (min= 45 , avg= 3351.951171875 , max= 38475 )\n",
      "old_model_log_logits torch.Size([4, 449]) (min= -27.2734375 , avg= -0.2695896327495575 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 0.0001177062513306737 , avg= 7085.11474609375 , max= 11231596.0 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -20.328323364257812 , avg= -0.36499089002609253 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -27.2734375 , avg= -0.45230865478515625 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3291412889957428 , avg= 0.04187711328268051 , max= 9.042661666870117 )\n",
      "kl_div torch.Size([4]) (min= 0.027432801201939583 , avg= 0.04187711328268051 , max= 0.056259434670209885 )\n",
      "completion_mask torch.Size([4, 449]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.4900000095367432 , avg= 1.4900000095367432 , max= 1.4900000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5 , avg= 0.5 , max= 0.5 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.4900000095367432 std_rewards: 0.5 Rewards: [1.24, 1.24, 2.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.4999000132083893 , avg= 1.4901161193847656e-08 , max= 1.4997000694274902 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 0.0001177062513306737 , avg= 7085.11474609375 , max= 11231596.0 )\n",
      "before A_hat multiply torch.Size([4, 449]) (min= -27976.5625 , avg= -38.582462310791016 , max= -0.0001177062513306737 )\n",
      "_grouped_ppo_loss torch.Size([4, 449]) (min= -1.6432642936706543 , avg= 18.793710708618164 , max= 13985.484375 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -1.480986475944519 , avg= 18.793710708618164 , max= 45.39803695678711 )\n",
      "_combined_loss torch.Size([4]) (min= -1.4754713773727417 , avg= 18.798341751098633 , max= 45.401790618896484 )\n",
      "combined_loss torch.Size([]) (min= 18.798341751098633 , avg= 18.798341751098633 , max= 18.798341751098633 )\n",
      "Final Loss: 18.798341751098633 , ppo_loss: 18.793710708618164 , kl_div: 0.04187711328268051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                      | 6/10 [01:47<01:10, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 2984.0947265625 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 2984.0947265625 , max= 36923 )\n",
      "respone_ids torch.Size([4, 461]) (min= 45 , avg= 3101.977294921875 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -37.5625 , avg= 0.5458984375 , max= 43.59375 )\n",
      "response_truncated_logits torch.Size([4, 461, 49152]) (min= -37.5625 , avg= 0.63427734375 , max= 43.59375 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -37.5625 , avg= 0.5458984375 , max= 43.59375 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2982.1337890625 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.578125 , avg= 0.31591796875 , max= 47.0625 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2982.1337890625 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 461, 49152]) (min= -37.5625 , avg= 0.63427734375 , max= 43.59375 )\n",
      "model response(ids) torch.Size([4, 461]) (min= 45 , avg= 3101.977294921875 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 461]) (min= -16.670907974243164 , avg= -0.2363598644733429 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 461, 49152]) (min= -25.578125 , avg= 0.350341796875 , max= 47.0625 )\n",
      "old model response(ids) torch.Size([4, 461]) (min= 45 , avg= 3101.977294921875 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 461]) (min= -21.856321334838867 , avg= -0.23582227528095245 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 461]) (min= 0.00021801237016916275 , avg= 6.720668315887451 , max= 2313.9619140625 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -18.768503189086914 , avg= -0.37579232454299927 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -21.856321334838867 , avg= -0.40251797437667847 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3534989655017853 , avg= 0.07152475416660309 , max= 8.54865837097168 )\n",
      "kl_div torch.Size([4]) (min= 0.03457776829600334 , avg= 0.07152475416660309 , max= 0.1383536159992218 )\n",
      "completion_mask torch.Size([4, 461]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.2400000095367432 , max= 1.2400000095367432 )\n",
      "mean_rewards torch.Size([4]) (min= 1.2400000095367432 , avg= 1.2400000095367432 , max= 1.2400000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.2400000095367432 std_rewards: 0.0 Rewards: [1.24, 1.24, 1.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.2400000095367432 , max= 1.2400000095367432 )\n",
      "A_hat torch.Size([4, 1]) (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 461]) (min= 0.00021801237016916275 , avg= 6.720668315887451 , max= 2313.9619140625 )\n",
      "before A_hat multiply torch.Size([4, 461]) (min= -1.0957286357879639 , avg= -0.9679449796676636 , max= -0.00021801237016916275 )\n",
      "_grouped_ppo_loss torch.Size([4, 461]) (min= -0.0 , avg= 0.0 , max= -0.0 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
      "_combined_loss torch.Size([4]) (min= 0.0038231301587074995 , avg= 0.007908214814960957 , max= 0.015297223813831806 )\n",
      "combined_loss torch.Size([]) (min= 0.007908214814960957 , avg= 0.007908214814960957 , max= 0.007908214814960957 )\n",
      "Final Loss: 0.007908214814960957 , ppo_loss: 0.0 , kl_div: 0.07152475416660309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                 | 7/10 [02:04<00:52, 17.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3394.58984375 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3394.58984375 , max= 36923 )\n",
      "respone_ids torch.Size([4, 469]) (min= 45 , avg= 3488.315673828125 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -32.53125 , avg= 0.57080078125 , max= 43.9375 )\n",
      "response_truncated_logits torch.Size([4, 469, 49152]) (min= -32.53125 , avg= 0.64501953125 , max= 43.9375 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -32.53125 , avg= 0.57080078125 , max= 43.9375 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3392.90576171875 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.578125 , avg= 0.44921875 , max= 48.21875 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3392.90576171875 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 469, 49152]) (min= -32.53125 , avg= 0.64501953125 , max= 43.9375 )\n",
      "model response(ids) torch.Size([4, 469]) (min= 45 , avg= 3488.315673828125 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 469]) (min= -18.914836883544922 , avg= -0.22550970315933228 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 469, 49152]) (min= -24.796875 , avg= 0.492431640625 , max= 48.21875 )\n",
      "old model response(ids) torch.Size([4, 469]) (min= 45 , avg= 3488.315673828125 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 469]) (min= -20.684171676635742 , avg= -0.2784874141216278 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 469]) (min= 6.12426598323168e-09 , avg= 445.22686767578125 , max= 344345.25 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -20.203144073486328 , avg= -0.316547155380249 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -20.8125057220459 , avg= -0.4104577898979187 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.36765503883361816 , avg= 0.05082859843969345 , max= 18.838708877563477 )\n",
      "kl_div torch.Size([4]) (min= 0.025209568440914154 , avg= 0.05082859843969345 , max= 0.11526771634817123 )\n",
      "completion_mask torch.Size([4, 469]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.4900000095367432 , avg= 1.4900000095367432 , max= 1.4900000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5 , avg= 0.5 , max= 0.5 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.4900000095367432 std_rewards: 0.5 Rewards: [1.24, 1.24, 2.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.4999000132083893 , avg= 1.4901161193847656e-08 , max= 1.4997000694274902 )\n",
      "probability_ratio torch.Size([4, 469]) (min= 6.12426598323168e-09 , avg= 445.22686767578125 , max= 344345.25 )\n",
      "before A_hat multiply torch.Size([4, 469]) (min= -344345.25 , avg= -348.316162109375 , max= -0.0009979797760024667 )\n",
      "_grouped_ppo_loss torch.Size([4, 469]) (min= -1.6432642936706543 , avg= 173.6314697265625 , max= 172138.1875 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -1.47536301612854 , avg= 173.6314697265625 , max= 424.986083984375 )\n",
      "_combined_loss torch.Size([4]) (min= -1.472118616104126 , avg= 173.6370849609375 , max= 424.9897766113281 )\n",
      "combined_loss torch.Size([]) (min= 173.6370849609375 , avg= 173.6370849609375 , max= 173.6370849609375 )\n",
      "Final Loss: 173.6370849609375 , ppo_loss: 173.6314697265625 , kl_div: 0.05082859843969345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 8/10 [02:22<00:35, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3095.18359375 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3095.18359375 , max= 36923 )\n",
      "respone_ids torch.Size([4, 468]) (min= 45 , avg= 3173.94677734375 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -37.21875 , avg= 0.384765625 , max= 42.78125 )\n",
      "response_truncated_logits torch.Size([4, 468, 49152]) (min= -37.21875 , avg= 0.46533203125 , max= 42.78125 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -37.21875 , avg= 0.384765625 , max= 42.78125 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3092.4482421875 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -26.9375 , avg= 0.384521484375 , max= 47.03125 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3092.4482421875 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 468, 49152]) (min= -37.21875 , avg= 0.46533203125 , max= 42.78125 )\n",
      "model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3173.94677734375 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 468]) (min= -16.125011444091797 , avg= -0.19828177988529205 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 468, 49152]) (min= -26.140625 , avg= 0.409912109375 , max= 47.03125 )\n",
      "old model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3173.94677734375 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 468]) (min= -21.734375 , avg= -0.21002058684825897 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 7.644988363608718e-05 , avg= 13.199980735778809 , max= 14243.234375 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -19.453662872314453 , avg= -0.30304890871047974 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -21.734375 , avg= -0.347099244594574 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.36311107873916626 , avg= 0.04650288075208664 , max= 8.634960174560547 )\n",
      "kl_div torch.Size([4]) (min= 0.022837385535240173 , avg= 0.04650288447737694 , max= 0.06790591031312943 )\n",
      "completion_mask torch.Size([4, 468]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.7400000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.7400000095367432 , avg= 1.7400000095367432 , max= 1.7400000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5773502588272095 , avg= 0.5773502588272095 , max= 0.5773502588272095 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.7400000095367432 std_rewards: 0.5773502588272095 Rewards: [1.24, 2.24, 1.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.7400000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.8658754229545593 , avg= 0.0 , max= 0.8658754229545593 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 7.644988363608718e-05 , avg= 13.199980735778809 , max= 14243.234375 )\n",
      "before A_hat multiply torch.Size([4, 468]) (min= -14243.234375 , avg= -12.588796615600586 , max= -7.644988363608718e-05 )\n",
      "_grouped_ppo_loss torch.Size([4, 468]) (min= -0.9487645030021667 , avg= 10.052281379699707 , max= 12332.8662109375 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.8527912497520447 , avg= 10.05228328704834 , max= 40.135841369628906 )\n",
      "_combined_loss torch.Size([4]) (min= -0.8502662181854248 , avg= 10.057424545288086 , max= 40.142452239990234 )\n",
      "combined_loss torch.Size([]) (min= 10.057424545288086 , avg= 10.057424545288086 , max= 10.057424545288086 )\n",
      "Final Loss: 10.057424545288086 , ppo_loss: 10.05228328704834 , kl_div: 0.04650288447737694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 9/10 [02:39<00:17, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3513.5830078125 , max= 44170 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3513.5830078125 , max= 44170 )\n",
      "respone_ids torch.Size([4, 467]) (min= 45 , avg= 3615.887451171875 , max= 44170 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -35.34375 , avg= 0.74462890625 , max= 42.59375 )\n",
      "response_truncated_logits torch.Size([4, 467, 49152]) (min= -35.34375 , avg= 0.8349609375 , max= 42.59375 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -35.34375 , avg= 0.74462890625 , max= 42.59375 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3511.99462890625 , max= 44170 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -26.484375 , avg= 0.77197265625 , max= 47.125 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3511.99462890625 , max= 44170 )\n",
      "model response(logits) torch.Size([4, 467, 49152]) (min= -35.34375 , avg= 0.8349609375 , max= 42.59375 )\n",
      "model response(ids) torch.Size([4, 467]) (min= 45 , avg= 3615.887451171875 , max= 44170 )\n",
      "model_log_logits torch.Size([4, 467]) (min= -16.578250885009766 , avg= -0.24821539223194122 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 467, 49152]) (min= -26.484375 , avg= 0.8447265625 , max= 47.125 )\n",
      "old model response(ids) torch.Size([4, 467]) (min= 45 , avg= 3615.887451171875 , max= 44170 )\n",
      "old_model_log_logits torch.Size([4, 467]) (min= -25.335941314697266 , avg= -0.2819480001926422 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 467]) (min= 1.1782927685999312e-05 , avg= 2235694.0 , max= 4174091264.0 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -20.023439407348633 , avg= -0.3558456003665924 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -25.335941314697266 , avg= -0.4200325608253479 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.34823983907699585 , avg= 0.07429580390453339 , max= 10.71170711517334 )\n",
      "kl_div torch.Size([4]) (min= 0.008603503927588463 , avg= 0.07429580390453339 , max= 0.17089928686618805 )\n",
      "completion_mask torch.Size([4, 467]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 0.0 , avg= 0.675000011920929 , max= 1.2400000095367432 )\n",
      "mean_rewards torch.Size([4]) (min= 0.675000011920929 , avg= 0.675000011920929 , max= 0.675000011920929 )\n",
      "std_rewards torch.Size([4]) (min= 0.6585590243339539 , avg= 0.6585590243339539 , max= 0.6585590243339539 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 0.675000011920929 std_rewards: 0.6585590243339539 Rewards: [0.22000000000000003, 1.24, 0.0, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 0.0 , avg= 0.675000011920929 , max= 1.2400000095367432 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.02480947971344 , avg= -1.4901161193847656e-08 , max= 0.857803463935852 )\n",
      "probability_ratio torch.Size([4, 467]) (min= 1.1782927685999312e-05 , avg= 2235694.0 , max= 4174091264.0 )\n",
      "before A_hat multiply torch.Size([4, 467]) (min= -4174091264.0 , avg= -2235683.5 , max= -0.0025160934310406446 )\n",
      "_grouped_ppo_loss torch.Size([4, 467]) (min= -0.9399198293685913 , avg= 1544415.75 , max= 2883451904.0 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.8502081036567688 , avg= 1544416.0 , max= 6177517.5 )\n",
      "_combined_loss torch.Size([4]) (min= -0.8492568731307983 , avg= 1544416.0 , max= 6177517.5 )\n",
      "combined_loss torch.Size([]) (min= 1544416.0 , avg= 1544416.0 , max= 1544416.0 )\n",
      "Final Loss: 1544416.0 , ppo_loss: 1544416.0 , kl_div: 0.07429580390453339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:57<00:00, 17.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training End ....\n",
      "Epoch 2/4 - Validation\n",
      "Training Start ....\n",
      "Epoch 2/4 - Training Gradient Group 1/1, category include =simple arithmetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 39 , avg= 3686.4677734375 , max= 41623 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 39 , avg= 3686.4677734375 , max= 41623 )\n",
      "respone_ids torch.Size([4, 449]) (min= 39 , avg= 3826.406494140625 , max= 41623 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -31.578125 , avg= 0.916015625 , max= 44.65625 )\n",
      "response_truncated_logits torch.Size([4, 449, 49152]) (min= -31.578125 , avg= 0.99853515625 , max= 44.65625 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -31.578125 , avg= 0.916015625 , max= 44.65625 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 39 , avg= 3683.81591796875 , max= 41623 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -27.515625 , avg= 0.4560546875 , max= 45.21875 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 39 , avg= 3683.81591796875 , max= 41623 )\n",
      "model response(logits) torch.Size([4, 449, 49152]) (min= -31.578125 , avg= 0.99853515625 , max= 44.65625 )\n",
      "model response(ids) torch.Size([4, 449]) (min= 39 , avg= 3826.406494140625 , max= 41623 )\n",
      "model_log_logits torch.Size([4, 449]) (min= -17.343149185180664 , avg= -0.2500355839729309 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 449, 49152]) (min= -27.203125 , avg= 0.658203125 , max= 44.5625 )\n",
      "old model response(ids) torch.Size([4, 449]) (min= 39 , avg= 3826.406494140625 , max= 41623 )\n",
      "old_model_log_logits torch.Size([4, 449]) (min= -25.138832092285156 , avg= -0.29849904775619507 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 1.8339907910558395e-05 , avg= 1345.2860107421875 , max= 1141045.375 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -22.398117065429688 , avg= -0.43298453092575073 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -26.417098999023438 , avg= -0.5706825256347656 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.36623862385749817 , avg= 0.040855176746845245 , max= 10.906373023986816 )\n",
      "kl_div torch.Size([4]) (min= 0.021658116951584816 , avg= 0.040855176746845245 , max= 0.06792612373828888 )\n",
      "completion_mask torch.Size([4, 449]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "TimeoutError (ObjectPool.get_results): score=1.40, CPU load=4.19/32, input:['### Instruction\\n\\nn<Test Target>\\nint modulo(int a, int b) {     if (b == 0) return 0; // Handle division by zero     return a % b; }\\n</Test Target>\\nWrtie a Clang-repl Test\\n### Response\\n\\n[REASON]\\n<Test Target>\\nint modulo(int a, int b) {\\nif (b == 0) return 0; // Handle division by 0 return 0; return a % b; }\\n</Test Target>\\n<Test Object>\\nVerify that modulo returns 0 when divisor is 0.\\n</Test Object>\\n<Input Data>\\n>>> int num = 4;\\n>>> int divisor = 0;\\n</Input Data>\\n<Expected Output>\\n>>> %<< result == 0;\\ntrue\\n</Expected Output>\\n[/REASON]\\n[ANSWER]\\n<Clang-repl Test>\\n>>> // Global Settings:\\n>>>\\n>>> // Test Object: Verify that modulo returns 0 when divisor is 0.\\n>>> // Test Case: ModuloZeroTest\\n>>> int num = 4;\\n>>> int divisor = 0;\\n>>> int result = modulo(num, divisor);\\n>>> %<< result == 0;\\ntrue\\n</Clang-repl Test>\\n[/ANSWER]\\n\\n<Need More Test>\\n[REASON]\\n<Test Target>\\nint modulo(int a, int b) {\\nif (b == 0) return 0; // Handle division by zero return a % b; }\\n</Test Target>\\n<Test Object>\\nVerify that modulo returns 0 when divisor is 0.\\n</Test Object>\\n<Input Data>\\n>>> int num = 4;\\n>>> int divisor = 0;\\n</Input Data>\\n<Expected Output>\\n>>> %<< result == 0;\\ntrue\\n</Expected Output>\\n[/REASON]\\n[ANSWER]\\n<Clang-repl Test>\\n>>> // Global Settings:\\n>>>\\n>>> // Test Object: Verify that modulo returns 0 when the divisor is 0.\\n>>> // Test Case: ModByZeroTest\\n>>> int num = 4;\\n>>> int divisor = 0;\\n>>> int result = modulo(num, divisor);\\n>>> %<< result == 0;\\ntrue\\n</', <torch.utils.tensorboard.writer.SummaryWriter object at 0x7d6beeecf950>, 'Training', 40]\n",
      "Retry failed: TimeoutError (ObjectPool.get_results): score=1.40, CPU load=5.01/32\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.7799999713897705 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.7799999713897705 , avg= 1.7799999713897705 , max= 1.7799999713897705 )\n",
      "std_rewards torch.Size([4]) (min= 0.5351635217666626 , avg= 0.5351635217666626 , max= 0.5351635217666626 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.7799999713897705 std_rewards: 0.5351635217666626 Rewards: [1.4, 2.24, 1.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.7799999713897705 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.008848786354065 , avg= 5.960464477539063e-08 , max= 0.8593898415565491 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 1.8339907910558395e-05 , avg= 1345.2860107421875 , max= 1141045.375 )\n",
      "before A_hat multiply torch.Size([4, 449]) (min= -265033.0625 , avg= -158.6201629638672 , max= -0.005310708656907082 )\n",
      "_grouped_ppo_loss torch.Size([4, 449]) (min= -0.941658079624176 , avg= 112.0690689086914 , max= 188155.09375 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.8415195941925049 , avg= 112.06906127929688 , max= 446.9978332519531 )\n",
      "_combined_loss torch.Size([4]) (min= -0.8391249179840088 , avg= 112.0735855102539 , max= 447.00146484375 )\n",
      "combined_loss torch.Size([]) (min= 112.0735855102539 , avg= 112.0735855102539 , max= 112.0735855102539 )\n",
      "Final Loss: 112.0735855102539 , ppo_loss: 112.06906127929688 , kl_div: 0.040855176746845245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                                                                  | 1/10 [03:16<29:24, 196.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3309.22265625 , max= 44170 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3309.22265625 , max= 44170 )\n",
      "respone_ids torch.Size([4, 467]) (min= 45 , avg= 3391.8349609375 , max= 44170 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -36.875 , avg= 0.5703125 , max= 41.3125 )\n",
      "response_truncated_logits torch.Size([4, 467, 49152]) (min= -36.875 , avg= 0.630859375 , max= 41.3125 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -36.875 , avg= 0.5703125 , max= 41.3125 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3308.701171875 , max= 44170 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -24.9375 , avg= 0.38232421875 , max= 46.5 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3308.701171875 , max= 44170 )\n",
      "model response(logits) torch.Size([4, 467, 49152]) (min= -36.875 , avg= 0.630859375 , max= 41.3125 )\n",
      "model response(ids) torch.Size([4, 467]) (min= 45 , avg= 3391.8349609375 , max= 44170 )\n",
      "model_log_logits torch.Size([4, 467]) (min= -29.74200439453125 , avg= -0.1900007277727127 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 467, 49152]) (min= -26.59375 , avg= 0.45703125 , max= 48.5625 )\n",
      "old model response(ids) torch.Size([4, 467]) (min= 45 , avg= 3391.8349609375 , max= 44170 )\n",
      "old_model_log_logits torch.Size([4, 467]) (min= -31.51930046081543 , avg= -0.17949587106704712 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 467]) (min= 0.0003800204140134156 , avg= 3.9418561458587646 , max= 3122.48046875 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -29.74200439453125 , avg= -0.3021116256713867 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -32.50777816772461 , avg= -0.3424519896507263 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.33566564321517944 , avg= 0.04158660024404526 , max= 6.63850212097168 )\n",
      "kl_div torch.Size([4]) (min= 0.022567104548215866 , avg= 0.04158660024404526 , max= 0.04915718361735344 )\n",
      "completion_mask torch.Size([4, 467]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.4900000095367432 , avg= 1.4900000095367432 , max= 1.4900000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5 , avg= 0.5 , max= 0.5 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.4900000095367432 std_rewards: 0.5 Rewards: [2.24, 1.24, 1.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.4999000132083893 , avg= 1.4901161193847656e-08 , max= 1.4997000694274902 )\n",
      "probability_ratio torch.Size([4, 467]) (min= 0.0003800204140134156 , avg= 3.9418561458587646 , max= 3122.48046875 )\n",
      "before A_hat multiply torch.Size([4, 467]) (min= -3122.48046875 , avg= -3.117694616317749 , max= -0.002163821132853627 )\n",
      "_grouped_ppo_loss torch.Size([4, 467]) (min= -1.6432642936706543 , avg= 1.0672175884246826 , max= 1560.927978515625 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -1.4739532470703125 , avg= 1.067217469215393 , max= 4.502128601074219 )\n",
      "_combined_loss torch.Size([4]) (min= -1.471458077430725 , avg= 1.0718154907226562 , max= 4.507563591003418 )\n",
      "combined_loss torch.Size([]) (min= 1.0718154907226562 , avg= 1.0718154907226562 , max= 1.0718154907226562 )\n",
      "Final Loss: 1.0718154907226562 , ppo_loss: 1.067217469215393 , kl_div: 0.04158660024404526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                                             | 2/10 [03:33<12:09, 91.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 2907.6640625 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 2907.6640625 , max= 36923 )\n",
      "respone_ids torch.Size([4, 461]) (min= 45 , avg= 3017.091064453125 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -38.4375 , avg= 0.5048828125 , max= 43.25 )\n",
      "response_truncated_logits torch.Size([4, 461, 49152]) (min= -38.4375 , avg= 0.5634765625 , max= 43.25 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -38.4375 , avg= 0.5048828125 , max= 43.25 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2905.92529296875 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.34375 , avg= 0.30810546875 , max= 47.28125 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2905.92529296875 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 461, 49152]) (min= -38.4375 , avg= 0.5634765625 , max= 43.25 )\n",
      "model response(ids) torch.Size([4, 461]) (min= 45 , avg= 3017.091064453125 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 461]) (min= -19.9453125 , avg= -0.22213254868984222 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 461, 49152]) (min= -27.015625 , avg= 0.3271484375 , max= 46.53125 )\n",
      "old model response(ids) torch.Size([4, 461]) (min= 45 , avg= 3017.091064453125 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 461]) (min= -28.34375 , avg= -0.2051544487476349 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 461]) (min= 1.105435239878716e-05 , avg= 10.09723949432373 , max= 4440.12353515625 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -19.9453125 , avg= -0.3381924629211426 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -29.21875 , avg= -0.39725443720817566 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.34456324577331543 , avg= 0.06417687237262726 , max= 7.729233741760254 )\n",
      "kl_div torch.Size([4]) (min= 0.04489396512508392 , avg= 0.06417687237262726 , max= 0.11286187171936035 )\n",
      "completion_mask torch.Size([4, 461]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 0.20999999344348907 , avg= 1.2325000762939453 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.2325000762939453 , avg= 1.2325000762939453 , max= 1.2325000762939453 )\n",
      "std_rewards torch.Size([4]) (min= 1.1634252071380615 , avg= 1.1634252071380615 , max= 1.1634252071380615 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.2325000762939453 std_rewards: 1.1634252071380615 Rewards: [2.24, 0.24000000000000005, 2.24, 0.21000000000000005]\n",
      "advantages before A_hat torch.Size([4]) (min= 0.20999999344348907 , avg= 1.2325000762939453 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -0.8787949085235596 , avg= -4.470348358154297e-08 , max= 0.8659029603004456 )\n",
      "probability_ratio torch.Size([4, 461]) (min= 1.105435239878716e-05 , avg= 10.09723949432373 , max= 4440.12353515625 )\n",
      "before A_hat multiply torch.Size([4, 461]) (min= -4440.12353515625 , avg= -7.869781970977783 , max= -0.002050240058451891 )\n",
      "_grouped_ppo_loss torch.Size([4, 461]) (min= -0.9487946629524231 , avg= 5.91901159286499 , max= 3787.47509765625 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.8448492884635925 , avg= 5.91901159286499 , max= 19.349180221557617 )\n",
      "_combined_loss torch.Size([4]) (min= -0.839178740978241 , avg= 5.926107406616211 , max= 19.361658096313477 )\n",
      "combined_loss torch.Size([]) (min= 5.926107406616211 , avg= 5.926107406616211 , max= 5.926107406616211 )\n",
      "Final Loss: 5.926107406616211 , ppo_loss: 5.91901159286499 , kl_div: 0.06417687237262726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                        | 3/10 [03:51<06:43, 57.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3108.8828125 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3108.8828125 , max= 36923 )\n",
      "respone_ids torch.Size([4, 449]) (min= 45 , avg= 3205.75732421875 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -36.875 , avg= 0.91015625 , max= 41.71875 )\n",
      "response_truncated_logits torch.Size([4, 449, 49152]) (min= -36.875 , avg= 1.0087890625 , max= 41.71875 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -36.875 , avg= 0.91015625 , max= 41.71875 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3106.91552734375 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -28.046875 , avg= 0.25927734375 , max= 45.0625 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3106.91552734375 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 449, 49152]) (min= -36.875 , avg= 1.0087890625 , max= 41.71875 )\n",
      "model response(ids) torch.Size([4, 449]) (min= 45 , avg= 3205.75732421875 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 449]) (min= -24.7814998626709 , avg= -0.1783333122730255 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 449, 49152]) (min= -29.0 , avg= 0.5224609375 , max= 44.15625 )\n",
      "old model response(ids) torch.Size([4, 449]) (min= 45 , avg= 3205.75732421875 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 449]) (min= -21.3671932220459 , avg= -0.16817715764045715 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 1.7279804323533376e-11 , avg= 632.5736694335938 , max= 1063456.625 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -24.7814998626709 , avg= -0.3139094114303589 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -27.6875057220459 , avg= -0.38615936040878296 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3284919857978821 , avg= 0.06984642148017883 , max= 24.78114128112793 )\n",
      "kl_div torch.Size([4]) (min= 0.026862256228923798 , avg= 0.06984642148017883 , max= 0.09439803659915924 )\n",
      "completion_mask torch.Size([4, 449]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 0.2199999988079071 , avg= 1.4850000143051147 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.4850000143051147 , avg= 1.4850000143051147 , max= 1.4850000143051147 )\n",
      "std_rewards torch.Size([4]) (min= 0.9661434888839722 , avg= 0.9661434888839722 , max= 0.9661434888839722 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.4850000143051147 std_rewards: 0.9661434888839722 Rewards: [0.22000000000000003, 1.24, 2.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 0.2199999988079071 , avg= 1.4850000143051147 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.309193730354309 , avg= 2.9802322387695312e-08 , max= 0.7813765406608582 )\n",
      "probability_ratio torch.Size([4, 449]) (min= 1.7279804323533376e-11 , avg= 632.5736694335938 , max= 1063456.625 )\n",
      "before A_hat multiply torch.Size([4, 449]) (min= -13561.2373046875 , avg= -19.177297592163086 , max= -8.898950021318797e-09 )\n",
      "_grouped_ppo_loss torch.Size([4, 449]) (min= -0.8561766743659973 , avg= 13.24239444732666 , max= 17754.287109375 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.7618218064308167 , avg= 13.242393493652344 , max= 44.07077407836914 )\n",
      "_combined_loss torch.Size([4]) (min= -0.7521135807037354 , avg= 13.250115394592285 , max= 44.078548431396484 )\n",
      "combined_loss torch.Size([]) (min= 13.250115394592285 , avg= 13.250115394592285 , max= 13.250115394592285 )\n",
      "Final Loss: 13.250115394592285 , ppo_loss: 13.242393493652344 , kl_div: 0.06984642148017883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                  | 4/10 [04:09<04:10, 41.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3069.67626953125 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3069.67626953125 , max= 36923 )\n",
      "respone_ids torch.Size([4, 468]) (min= 45 , avg= 3146.041259765625 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -35.875 , avg= 0.728515625 , max= 45.25 )\n",
      "response_truncated_logits torch.Size([4, 468, 49152]) (min= -35.875 , avg= 0.80029296875 , max= 45.25 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -35.875 , avg= 0.728515625 , max= 45.25 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3074.02099609375 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -26.9375 , avg= 0.35693359375 , max= 47.65625 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3074.02099609375 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 468, 49152]) (min= -35.875 , avg= 0.80029296875 , max= 45.25 )\n",
      "model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3146.041259765625 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 468]) (min= -19.50402069091797 , avg= -0.1592944860458374 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 468, 49152]) (min= -27.140625 , avg= 0.470458984375 , max= 48.0 )\n",
      "old model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3146.041259765625 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 468]) (min= -24.31119728088379 , avg= -0.1471780687570572 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 3.550769906723872e-05 , avg= 6.611259937286377 , max= 6544.84033203125 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -19.50402069091797 , avg= -0.2513095736503601 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -24.734773635864258 , avg= -0.31864282488822937 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3634923994541168 , avg= 0.03892422467470169 , max= 5.265113353729248 )\n",
      "kl_div torch.Size([4]) (min= 0.028515473008155823 , avg= 0.03892422467470169 , max= 0.04758070409297943 )\n",
      "completion_mask torch.Size([4, 468]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.9900000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.9900000095367432 , avg= 1.9900000095367432 , max= 1.9900000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5 , avg= 0.5 , max= 0.5 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.9900000095367432 std_rewards: 0.5 Rewards: [2.24, 1.24, 2.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.9900000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.4997000694274902 , avg= -1.4901161193847656e-08 , max= 0.4999000132083893 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 3.550769906723872e-05 , avg= 6.611259937286377 , max= 6544.84033203125 )\n",
      "before A_hat multiply torch.Size([4, 468]) (min= -324.15704345703125 , avg= -1.4374715089797974 , max= -3.550769906723872e-05 )\n",
      "_grouped_ppo_loss torch.Size([4, 468]) (min= -0.5477547645568848 , avg= 0.6910557150840759 , max= 486.1383361816406 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.49061319231987 , avg= 0.6910556554794312 , max= 4.22894287109375 )\n",
      "_combined_loss torch.Size([4]) (min= -0.48701968789100647 , avg= 0.6953593492507935 , max= 4.234150409698486 )\n",
      "combined_loss torch.Size([]) (min= 0.6953593492507935 , avg= 0.6953593492507935 , max= 0.6953593492507935 )\n",
      "Final Loss: 0.6953593492507935 , ppo_loss: 0.6910556554794312 , kl_div: 0.03892422467470169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                            | 5/10 [04:26<02:45, 33.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3124.46484375 , max= 46716 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3124.46484375 , max= 46716 )\n",
      "respone_ids torch.Size([4, 469]) (min= 45 , avg= 3193.42431640625 , max= 46716 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -34.53125 , avg= 0.57421875 , max= 41.625 )\n",
      "response_truncated_logits torch.Size([4, 469, 49152]) (min= -34.53125 , avg= 0.64208984375 , max= 41.625 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -34.53125 , avg= 0.57421875 , max= 41.625 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3123.87646484375 , max= 46716 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.65625 , avg= 0.402099609375 , max= 46.75 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3123.87646484375 , max= 46716 )\n",
      "model response(logits) torch.Size([4, 469, 49152]) (min= -34.53125 , avg= 0.64208984375 , max= 41.625 )\n",
      "model response(ids) torch.Size([4, 469]) (min= 45 , avg= 3193.42431640625 , max= 46716 )\n",
      "model_log_logits torch.Size([4, 469]) (min= -16.203369140625 , avg= -0.12884992361068726 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 469, 49152]) (min= -26.671875 , avg= 0.55908203125 , max= 48.8125 )\n",
      "old model response(ids) torch.Size([4, 469]) (min= 45 , avg= 3193.42431640625 , max= 46716 )\n",
      "old_model_log_logits torch.Size([4, 469]) (min= -24.8046875 , avg= -0.12053215503692627 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 469]) (min= 0.0031850154045969248 , avg= 30.742111206054688 , max= 48451.40625 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -19.566701889038086 , avg= -0.22737205028533936 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -26.6015625 , avg= -0.28934627771377563 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.36142686009407043 , avg= 0.026594113558530807 , max= 3.01859974861145 )\n",
      "kl_div torch.Size([4]) (min= 0.01979978010058403 , avg= 0.026594115421175957 , max= 0.0323878638446331 )\n",
      "completion_mask torch.Size([4, 469]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.9900000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.9900000095367432 , avg= 1.9900000095367432 , max= 1.9900000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5 , avg= 0.5 , max= 0.5 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.9900000095367432 std_rewards: 0.5 Rewards: [2.24, 2.24, 2.24, 1.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.9900000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.4997000694274902 , avg= -1.4901161193847656e-08 , max= 0.4999000132083893 )\n",
      "probability_ratio torch.Size([4, 469]) (min= 0.0031850154045969248 , avg= 30.742111206054688 , max= 48451.40625 )\n",
      "before A_hat multiply torch.Size([4, 469]) (min= -183.2437744140625 , avg= -1.0982873439788818 , max= -0.014110156334936619 )\n",
      "_grouped_ppo_loss torch.Size([4, 469]) (min= -0.5477547645568848 , avg= 0.17888270318508148 , max= 274.8106994628906 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.4917091727256775 , avg= 0.17888274788856506 , max= 2.1837496757507324 )\n",
      "_combined_loss torch.Size([4]) (min= -0.48951998353004456 , avg= 0.18182310461997986 , max= 2.1871142387390137 )\n",
      "combined_loss torch.Size([]) (min= 0.18182310461997986 , avg= 0.18182310461997986 , max= 0.18182310461997986 )\n",
      "Final Loss: 0.18182310461997986 , ppo_loss: 0.17888274788856506 , kl_div: 0.026594115421175957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                      | 6/10 [04:44<01:52, 28.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 2590.80126953125 , max= 36923 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 2590.80126953125 , max= 36923 )\n",
      "respone_ids torch.Size([4, 466]) (min= 45 , avg= 2654.393310546875 , max= 36923 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -37.5625 , avg= 0.5732421875 , max= 43.28125 )\n",
      "response_truncated_logits torch.Size([4, 466, 49152]) (min= -37.5625 , avg= 0.64404296875 , max= 43.28125 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -37.5625 , avg= 0.5732421875 , max= 43.28125 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2588.38623046875 , max= 36923 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -25.15625 , avg= 0.413818359375 , max= 46.84375 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 2588.38623046875 , max= 36923 )\n",
      "model response(logits) torch.Size([4, 466, 49152]) (min= -37.5625 , avg= 0.64404296875 , max= 43.28125 )\n",
      "model response(ids) torch.Size([4, 466]) (min= 45 , avg= 2654.393310546875 , max= 36923 )\n",
      "model_log_logits torch.Size([4, 466]) (min= -19.726247787475586 , avg= -0.09712965786457062 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 466, 49152]) (min= -25.21875 , avg= 0.513671875 , max= 48.21875 )\n",
      "old model response(ids) torch.Size([4, 466]) (min= 45 , avg= 2654.393310546875 , max= 36923 )\n",
      "old_model_log_logits torch.Size([4, 466]) (min= -18.9826717376709 , avg= -0.0784931406378746 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.00045563391176983714 , avg= 1.8895528316497803 , max= 1236.4053955078125 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -21.162240982055664 , avg= -0.19705946743488312 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -21.460939407348633 , avg= -0.22465762495994568 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3552449345588684 , avg= 0.022579234093427658 , max= 6.643537998199463 )\n",
      "kl_div torch.Size([4]) (min= 0.0029642507433891296 , avg= 0.022579234093427658 , max= 0.047310102730989456 )\n",
      "completion_mask torch.Size([4, 466]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -1.0957286357879639 , avg= -1.0957286357879639 , max= -1.0957286357879639 )\n",
      "_combined_loss torch.Size([4]) (min= -1.0954009294509888 , avg= -1.0932321548461914 , max= -1.0904977321624756 )\n",
      "combined_loss torch.Size([]) (min= -1.0932321548461914 , avg= -1.0932321548461914 , max= -1.0932321548461914 )\n",
      "Final Loss: -1.0932321548461914 , ppo_loss: -1.0957286357879639 , kl_div: 0.022579234093427658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                 | 7/10 [05:02<01:14, 24.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3740.3076171875 , max= 47274 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3740.3076171875 , max= 47274 )\n",
      "respone_ids torch.Size([4, 468]) (min= 45 , avg= 3819.75341796875 , max= 47274 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -33.875 , avg= 0.560546875 , max= 44.0 )\n",
      "response_truncated_logits torch.Size([4, 468, 49152]) (min= -33.875 , avg= 0.66064453125 , max= 44.0 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -33.875 , avg= 0.560546875 , max= 44.0 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3739.15283203125 , max= 47274 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -27.109375 , avg= 0.388427734375 , max= 46.25 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3739.15283203125 , max= 47274 )\n",
      "model response(logits) torch.Size([4, 468, 49152]) (min= -33.875 , avg= 0.66064453125 , max= 44.0 )\n",
      "model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3819.75341796875 , max= 47274 )\n",
      "model_log_logits torch.Size([4, 468]) (min= -18.186214447021484 , avg= -0.16267624497413635 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 468, 49152]) (min= -27.359375 , avg= 0.497314453125 , max= 47.6875 )\n",
      "old model response(ids) torch.Size([4, 468]) (min= 45 , avg= 3819.75341796875 , max= 47274 )\n",
      "old_model_log_logits torch.Size([4, 468]) (min= -25.58675765991211 , avg= -0.15031814575195312 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 7.312863453989848e-05 , avg= 10.693257331848145 , max= 11026.72265625 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -18.186214447021484 , avg= -0.2588600814342499 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -25.89261817932129 , avg= -0.3181566596031189 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.2984228730201721 , avg= 0.0362112782895565 , max= 10.022096633911133 )\n",
      "kl_div torch.Size([4]) (min= 0.017291616648435593 , avg= 0.0362112782895565 , max= 0.05605778843164444 )\n",
      "completion_mask torch.Size([4, 468]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 1.2400000095367432 , avg= 1.9900000095367432 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.9900000095367432 , avg= 1.9900000095367432 , max= 1.9900000095367432 )\n",
      "std_rewards torch.Size([4]) (min= 0.5 , avg= 0.5 , max= 0.5 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.9900000095367432 std_rewards: 0.5 Rewards: [2.24, 1.24, 2.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 1.2400000095367432 , avg= 1.9900000095367432 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.4997000694274902 , avg= -1.4901161193847656e-08 , max= 0.4999000132083893 )\n",
      "probability_ratio torch.Size([4, 468]) (min= 7.312863453989848e-05 , avg= 10.693257331848145 , max= 11026.72265625 )\n",
      "before A_hat multiply torch.Size([4, 468]) (min= -11026.72265625 , avg= -7.478662967681885 , max= -7.312863453989848e-05 )\n",
      "_grouped_ppo_loss torch.Size([4, 468]) (min= -0.5477547645568848 , avg= 9.746414184570312 , max= 16536.77734375 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.49244770407676697 , avg= 9.746415138244629 , max= 40.454994201660156 )\n",
      "_combined_loss torch.Size([4]) (min= -0.49053582549095154 , avg= 9.750418663024902 , max= 40.46052551269531 )\n",
      "combined_loss torch.Size([]) (min= 9.750418663024902 , avg= 9.750418663024902 , max= 9.750418663024902 )\n",
      "Final Loss: 9.750418663024902 , ppo_loss: 9.746415138244629 , kl_div: 0.0362112782895565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 8/10 [05:20<00:45, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3198.75927734375 , max= 48567 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3198.75927734375 , max= 48567 )\n",
      "respone_ids torch.Size([4, 466]) (min= 45 , avg= 3264.81494140625 , max= 48567 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -35.28125 , avg= 0.412841796875 , max= 43.875 )\n",
      "response_truncated_logits torch.Size([4, 466, 49152]) (min= -35.28125 , avg= 0.4619140625 , max= 43.875 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -35.28125 , avg= 0.412841796875 , max= 43.875 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3209.6171875 , max= 48567 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -26.265625 , avg= 0.312255859375 , max= 45.84375 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3209.6171875 , max= 48567 )\n",
      "model response(logits) torch.Size([4, 466, 49152]) (min= -35.28125 , avg= 0.4619140625 , max= 43.875 )\n",
      "model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3264.81494140625 , max= 48567 )\n",
      "model_log_logits torch.Size([4, 466]) (min= -23.782655715942383 , avg= -0.06985548138618469 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 466, 49152]) (min= -28.953125 , avg= 0.419189453125 , max= 46.90625 )\n",
      "old model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3264.81494140625 , max= 48567 )\n",
      "old_model_log_logits torch.Size([4, 466]) (min= -27.413745880126953 , avg= -0.09987699240446091 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.04631832242012024 , avg= 1837.770751953125 , max= 2311085.25 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -23.782655715942383 , avg= -0.1623345911502838 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -27.492603302001953 , avg= -0.23993411660194397 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3394191861152649 , avg= 0.01742137037217617 , max= 3.5325653553009033 )\n",
      "kl_div torch.Size([4]) (min= 0.008241591043770313 , avg= 0.01742136850953102 , max= 0.02994436025619507 )\n",
      "completion_mask torch.Size([4, 466]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 0.20999999344348907 , avg= 1.4825000762939453 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.4825000762939453 , avg= 1.4825000762939453 , max= 1.4825000762939453 )\n",
      "std_rewards torch.Size([4]) (min= 0.9705110192298889 , avg= 0.9705110192298889 , max= 0.9705110192298889 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.4825000762939453 std_rewards: 0.9705110192298889 Rewards: [0.21000000000000005, 2.24, 1.24, 2.24]\n",
      "advantages before A_hat torch.Size([4]) (min= 0.20999999344348907 , avg= 1.4825000762939453 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.3110297918319702 , avg= -4.470348358154297e-08 , max= 0.7804361581802368 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 0.04631832242012024 , avg= 1837.770751953125 , max= 2311085.25 )\n",
      "before A_hat multiply torch.Size([4, 466]) (min= -2311085.25 , avg= -1832.9822998046875 , max= -0.16243869066238403 )\n",
      "_grouped_ppo_loss torch.Size([4, 466]) (min= -0.8551462292671204 , avg= 2401.50341796875 , max= 3029901.5 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.7704375982284546 , avg= 2401.503662109375 , max= 9607.029296875 )\n",
      "_combined_loss torch.Size([4]) (min= -0.7695263624191284 , avg= 2401.505615234375 , max= 9607.03125 )\n",
      "combined_loss torch.Size([]) (min= 2401.505615234375 , avg= 2401.505615234375 , max= 2401.505615234375 )\n",
      "Final Loss: 2401.505615234375 , ppo_loss: 2401.503662109375 , kl_div: 0.01742136850953102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 9/10 [05:38<00:21, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_ids torch.Size([4, 512]) (min= 45 , avg= 3300.8203125 , max= 44260 )\n",
      "truncated_ids torch.Size([4, 512]) (min= 45 , avg= 3300.8203125 , max= 44260 )\n",
      "respone_ids torch.Size([4, 466]) (min= 45 , avg= 3389.2509765625 , max= 44260 )\n",
      "_full_shift_logits torch.Size([4, 512, 49152]) (min= -34.34375 , avg= 0.802734375 , max= 44.40625 )\n",
      "response_truncated_logits torch.Size([4, 466, 49152]) (min= -34.34375 , avg= 0.92333984375 , max= 44.40625 )\n",
      "model full(logits) torch.Size([4, 512, 49152]) (min= -34.34375 , avg= 0.802734375 , max= 44.40625 )\n",
      "model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3306.560546875 , max= 44260 )\n",
      "ref model full(logits) torch.Size([4, 512, 49152]) (min= -24.578125 , avg= 0.435546875 , max= 46.5 )\n",
      "ref model full(ids) torch.Size([4, 512]) (min= 45 , avg= 3306.560546875 , max= 44260 )\n",
      "model response(logits) torch.Size([4, 466, 49152]) (min= -34.34375 , avg= 0.92333984375 , max= 44.40625 )\n",
      "model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3389.2509765625 , max= 44260 )\n",
      "model_log_logits torch.Size([4, 466]) (min= -22.441532135009766 , avg= -0.12489484250545502 , max= 0.0 )\n",
      "old model response(logits) torch.Size([4, 466, 49152]) (min= -24.9375 , avg= 0.669921875 , max= 46.90625 )\n",
      "old model response(ids) torch.Size([4, 466]) (min= 45 , avg= 3389.2509765625 , max= 44260 )\n",
      "old_model_log_logits torch.Size([4, 466]) (min= -34.171875 , avg= -0.11464350670576096 , max= 0.0 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 5.774077882492179e-10 , avg= 22127.330078125 , max= 24357128.0 )\n",
      "model_log_probs torch.Size([4, 512]) (min= -22.441532135009766 , avg= -0.2066141664981842 , max= 0.0 )\n",
      "ref_log_probs torch.Size([4, 512]) (min= -34.734375 , avg= -0.2582613229751587 , max= 0.0 )\n",
      "token_kl_div torch.Size([4, 512]) (min= -0.3633985221385956 , avg= 0.045386943966150284 , max= 12.465653419494629 )\n",
      "kl_div torch.Size([4]) (min= 0.024009471759200096 , avg= 0.04538694769144058 , max= 0.0753815621137619 )\n",
      "completion_mask torch.Size([4, 466]) (min= 1.0 , avg= 1.0 , max= 1.0 )\n",
      "advantages torch.Size([1, 4]) (min= 0.15000000596046448 , avg= 1.7174999713897705 , max= 2.240000009536743 )\n",
      "mean_rewards torch.Size([4]) (min= 1.7174999713897705 , avg= 1.7174999713897705 , max= 1.7174999713897705 )\n",
      "std_rewards torch.Size([4]) (min= 1.0449999570846558 , avg= 1.0449999570846558 , max= 1.0449999570846558 )\n",
      ">>>>>>>>>>>>>>>>>>>> mean_rewards: 1.7174999713897705 std_rewards: 1.0449999570846558 Rewards: [2.24, 2.24, 2.24, 0.15000000000000002]\n",
      "advantages before A_hat torch.Size([4]) (min= 0.15000000596046448 , avg= 1.7174999713897705 , max= 2.240000009536743 )\n",
      "A_hat torch.Size([4, 1]) (min= -1.4998564720153809 , avg= 2.9802322387695312e-08 , max= 0.49995219707489014 )\n",
      "probability_ratio torch.Size([4, 466]) (min= 5.774077882492179e-10 , avg= 22127.330078125 , max= 24357128.0 )\n",
      "before A_hat multiply torch.Size([4, 466]) (min= -16858920.0 , avg= -9059.9716796875 , max= -5.774077882492179e-10 )\n",
      "_grouped_ppo_loss torch.Size([4, 466]) (min= -0.5478119254112244 , avg= 13587.181640625 , max= 25285960.0 )\n",
      "grouped_ppo_loss torch.Size([4]) (min= -0.4935513734817505 , avg= 13587.1806640625 , max= 54350.19921875 )\n",
      "_combined_loss torch.Size([4]) (min= -0.4907792806625366 , avg= 13587.185546875 , max= 54350.20703125 )\n",
      "combined_loss torch.Size([]) (min= 13587.185546875 , avg= 13587.185546875 , max= 13587.185546875 )\n",
      "Final Loss: 13587.185546875 , ppo_loss: 13587.1806640625 , kl_div: 0.04538694769144058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:56<00:00, 35.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training End ....\n",
      "Epoch 3/4 - Validation\n",
      "Training Start ....\n",
      "Epoch 3/4 - Training Gradient Group 1/1, category include =simple arithmetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Core logic\n",
    "# ------------------------------------------------\n",
    "\n",
    "def generate_ids(model, batch, tokenizer, temperature):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Determine prompt length for each example in the batch based on the first occurrence of EOS.\n",
    "    prompt_lengths = []\n",
    "    for i in range(input_ids.size(0)):\n",
    "        seq = input_ids[i]\n",
    "        # Find indices where the token equals the eos_token_id.\n",
    "        eos_positions = (seq == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        # If there's at least one occurrence, use its index + 1 (if you want to include the EOS in the prompt).\n",
    "        # Otherwise, fallback to the full sequence length.\n",
    "        if eos_positions.numel() > 0:\n",
    "            first_eos = eos_positions[0].item() + 1\n",
    "        else:\n",
    "            first_eos = seq.size(0)\n",
    "        prompt_lengths.append(first_eos)\n",
    "    \n",
    "    print_memory(\"Prompt lengths per batch element: \" + str(prompt_lengths))\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,  # assuming max_length is defined globally\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    full_ids = output.sequences.detach()\n",
    "    truncated_ids = cut_ids_on_eos_tensor(full_ids, tokenizer.eos_token_id)    \n",
    "    respone_ids =  pad_sequence([truncated_ids[idx][p_len:] for idx, p_len in enumerate(prompt_lengths)],\n",
    "                                               batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    truncated_ids = pad_sequence(truncated_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    full_ids = pad_sequence(full_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    output = None\n",
    "    print_memory(\"full_ids.shape[-1]: \" + str(full_ids.shape[-1]))\n",
    "    return full_ids, truncated_ids, respone_ids, prompt_lengths\n",
    "\n",
    "\n",
    "def compute_logits(model, full_ids, prompt_lengths, respone_ids, tokenizer, detach_out=False):\n",
    "    # Pad the list of full_ids to a whole tensor with shape (batch, max_seq_length)\n",
    "    full_ids = pad_sequence(full_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(dtype=torch.int32)\n",
    "    \n",
    "    # Create an attention mask where non-pad tokens are 1 and pad tokens are 0\n",
    "    full_ids_mask = (full_ids != tokenizer.pad_token_id).to(dtype=torch.int32, device=full_ids.device)\n",
    "    \n",
    "    # Compute logits for the whole padded tensor.\n",
    "    logits = model(input_ids=full_ids, attention_mask=full_ids_mask, early_stop=False).logits\n",
    "    \n",
    "    truncated_response_ids_list = []\n",
    "    truncated_response_logits_list = []\n",
    "    batch_size = full_ids.size(0)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        p_len = prompt_lengths[i]\n",
    "        # Determine the true sequence length (ignoring padding) for this batch element.\n",
    "        actual_length = full_ids_mask[i].sum().item()\n",
    "        # Ensure prompt length does not exceed actual length.\n",
    "        if p_len > actual_length:\n",
    "            p_len = actual_length\n",
    "\n",
    "        # Extract completion token IDs for this example.\n",
    "        comp_ids = full_ids[i, p_len:actual_length].detach()\n",
    "        # For logits, if you want to include the token just before the completion, slice from p_len-1.\n",
    "        comp_logits = logits[i, p_len-1:actual_length-1, :]\n",
    "        \n",
    "        # Optionally, adjust lengths to be consistent (if needed by downstream code)\n",
    "        #comp_ids, comp_logits = cut_tensors_by_min(comp_ids, comp_logits, 0)\n",
    "        expected_len = respone_ids.shape[1]\n",
    "        truncated_response_ids_list.append((comp_ids.detach() if detach_out else comp_ids)[:expected_len])\n",
    "        truncated_response_logits_list.append((comp_logits.detach() if detach_out else comp_logits)[:expected_len, :])\n",
    "\n",
    "    truncated_response_logits = pad_sequence(truncated_response_logits_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    truncated_response_ids = pad_sequence(truncated_response_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return logits, truncated_response_logits, truncated_response_ids\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def run(model, old_model, ref_model, dataloader, optimizer, device, tokenizer,\n",
    "        group_size, epsilon, kl_lambda, scaler, writer, global_step, log_group,\n",
    "        scheduler, gradient_accumulation_step, reward_work, is_validation=False, temperature=1.0):\n",
    "    running_loss = 0.0\n",
    "    mean_reward = 0.0\n",
    "    sum_reward = 0.0\n",
    "    run_start_global_step=global_step\n",
    "    print_memory(\"_.1. run() enter\")\n",
    "    # For accumulation mode, ensure gradients are zeroed at the start.\n",
    "    if not is_validation:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, total=len(dataloader)), start=1):\n",
    "        print_step(f\"Processing batch {step}/{len(dataloader)}: Start Loop\")\n",
    "        # Move batch to device and expand the tensors.\n",
    "        batch = {k: v.to(device=device, dtype=torch.int32) for k, v in batch.items()}\n",
    "        batch_size = len(batch)\n",
    "        input_ids = batch['input_ids'].repeat_interleave(group_size, dim=0)\n",
    "        attention_mask = batch['attention_mask'].repeat_interleave(group_size, dim=0)\n",
    "        batch_size = input_ids.size(0)\n",
    "        batch['input_ids'] = input_ids\n",
    "        batch['attention_mask'] = attention_mask\n",
    "\n",
    "        start_tensor_ids = cur_memory_ids()\n",
    "\n",
    "        # LOGIT sample (min= -34.09375 , avg= 0.364013671875 , max= 42.25 )\n",
    "        # LOG_LOGIT sample: (min= -18.188087463378906 , avg= -0.3128775656223297 , max= 0.0 )\n",
    "        # LOG_PROBE sample: (min= -22.589847564697266 , avg= -0.4811277985572815 , max= 0.0 )\n",
    "        # kl_div sample: (min= 0.07530781626701355 , avg= 0.0904245376586914 , max= 0.10227474570274353 )\n",
    "\n",
    "        # std_rewards: 0.5049999952316284 \n",
    "        # advantages: (min= 1.2400000095367432 , avg= 1.4900000095367432 , max= 2.240000009536743 )\n",
    "        # A_hat: (min= -1.4997029304504395 , avg= 1.4901161193847656e-08 , max= 0.4999009966850281 )\n",
    "        # unclipped_objective: (min= -483673.78125 , avg= -212.85939025878906 , max= 221674.734375 )\n",
    "        # clipped_objective: (min= 0.7576461434364319 , avg= 0.7710149884223938 , max= 1.2423537969589233 )\n",
    "        # ppo_loss: -0.5694103240966797 , kl_div: 0.0904245376586914\n",
    "\n",
    "        # std_rewards: 0.0 Rewards: [1.24, 1.24, 1.24, 1.24]\n",
    "        # advantages: (min= 1.2400000095367432 , avg= 1.2400000095367432 , max= 1.2400000095367432 )\n",
    "        # A_hat: (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
    "        # unclipped_objective: (min= 0.0 , avg= 0.0 , max= 0.0 )\n",
    "        # clipped_objective: (min= 0.7576461434364319 , avg= 0.7576462030410767 , max= 0.7576461434364319 )\n",
    "        # ppo_loss: 0.0 , kl_div: 0.0454302616417408\n",
    "\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            # 1. Model forward pass for generation.\n",
    "            print_step(\"1. Model train\")\n",
    "            with torch.no_grad():\n",
    "                full_ids, truncated_ids, respone_ids, prompt_lengths = generate_ids(model, batch, tokenizer, temperature)\n",
    "                full_ids.log() # FULL_IDS\n",
    "                truncated_ids.log() # TRUNCATED_IDS\n",
    "                respone_ids.log() # RESPONSE_IDS\n",
    "                full_text_lists = tokenizer.batch_decode(truncated_ids, skip_special_tokens=True)\n",
    "                reward_work.reward(full_text_lists, writer, log_group, global_step)\n",
    "                # Release unused tensors from generation.\n",
    "                full_text_lists = None\n",
    "            _full_shift_logits, response_truncated_logits, _ = compute_logits(model, full_ids, prompt_lengths, respone_ids, tokenizer) \n",
    "            _full_shift_logits.log() # FULL_LOGITS\n",
    "            response_truncated_logits.log() # RESPONSE_LOGITS\n",
    "\n",
    "            full_shift_ids = shift_ids_with_logits(full_ids, _full_shift_logits)\n",
    "            full_shift_logits = pad_sequence(_full_shift_logits, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "            print_logits_ids(\"model full\", full_shift_logits, full_shift_ids)  # good format confirmed, full_shift_logits: FULL_LOGITS, full_shift_ids: FULL_IDS\n",
    "            \n",
    "            FULL_IDS = full_ids.shape  # [batch, full_ids_len], sample: batch=4, full_ids=512\n",
    "            TRUNCATED_IDS = truncated_ids.shape # [batch, truncated_ids_len],  sample: truncated_ids_len=512 or less. FULL_IDS with cut out end parts after eos\n",
    "            RESPONSE_IDS = respone_ids.shape  # [batch, respone_ids_len], sample: respone_ids_len = 466 = truncated_ids_len-prompt_length\n",
    "            FULL_LOGITS = full_shift_logits.shape # [batch, full_ids_len, embedding_len], sample: embedding_len=49152\n",
    "            RESPONSE_LOGITS = response_truncated_logits.shape #  [batch, respone_ids_len, embedding_len]\n",
    "            # GROUPED_BATCH  = advantages.shape # [grouped_batch, group_size], example grouped_batch=1, group_size=4 see advantages creation\n",
    "            \n",
    "\n",
    "            # 2. Run legacy models (old and reference models).\n",
    "            print_step(\"2. Legacy Models Run\")\n",
    "            with torch.no_grad():\n",
    "                _, old_response_truncated_logits, _ = compute_logits(old_model, truncated_ids, prompt_lengths, respone_ids, tokenizer, detach_out=True)\n",
    "                ref_full_shift_logits, _, _ = compute_logits(ref_model, full_ids, prompt_lengths, respone_ids, tokenizer, detach_out=True)\n",
    "                print_logits_ids(\"ref model full\", ref_full_shift_logits, full_shift_ids) # good format confirmed, ref_full_shift_logits: FULL_LOGITS, full_shift_ids: FULL_IDS\n",
    "            truncated_ids = None\n",
    "            prompt_lengths = None\n",
    "            full_ids = None\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                print_logits_ids(\"model response\", response_truncated_logits, respone_ids) # good format confirmed, response_truncated_logits: RESPONSE_LOGITS, respone_ids: RESPONSE_IDS\n",
    "                model_log_logits = selective_log_softmax(response_truncated_logits, respone_ids, tokenizer).check_shape(RESPONSE_IDS)\n",
    "                model_log_logits.log()\n",
    "                print_logits_ids(\"old model response\", old_response_truncated_logits, respone_ids) # good format confirmed, old_response_truncated_logits: RESPONSE_LOGITS, respone_ids: RESPONSE_IDS\n",
    "                old_model_log_logits = selective_log_softmax(old_response_truncated_logits, respone_ids, tokenizer).check_shape(RESPONSE_IDS)\n",
    "                old_model_log_logits.log() \n",
    "                probability_ratio = torch.exp(model_log_logits - old_model_log_logits).check_shape(RESPONSE_IDS)\n",
    "                probability_ratio.log() \n",
    "                \n",
    "                # Remove legacy model intermediates (no longer needed)\n",
    "                full_truncated_full_logits = None\n",
    "                response_truncated_logits = None\n",
    "                old_response_truncated_logits = None\n",
    "                ref_completion_ids = None\n",
    "                model_log_logits = None\n",
    "                old_model_log_logits = None\n",
    "\n",
    "\n",
    "            # 3. kl_div Loss Calc\n",
    "            print_step(\"3. kl_div Loss Calc\")    \n",
    "            # Calculate token-level log probabilities.\n",
    "            model_log_probs = selective_log_softmax(full_shift_logits, full_shift_ids, tokenizer)\n",
    "            model_log_probs.log() # RESPONSE_IDS\n",
    "            ref_log_probs = selective_log_softmax(ref_full_shift_logits, full_shift_ids, tokenizer)\n",
    "            ref_log_probs.log() # RESPONSE_IDS\n",
    "            \n",
    "            # Compute token-level KL divergence.\n",
    "            token_kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='none', log_target=True).check_shape(FULL_IDS) # it is not an ids but parts of logits content. the shape is just like ids)\n",
    "            token_kl_div.log()\n",
    "            kl_div = token_kl_div.mean(dim=-1).check_shape([batch_size])\n",
    "            kl_div.log() # average over tokens. range (0, infite) but for output of similar model. It is very small. sample: kl_div=0.09\n",
    "            \n",
    "            # Create a mask for non-padding tokens.\n",
    "            completion_mask = (respone_ids != tokenizer.pad_token_id).to(dtype=torch.float32, device='cuda').check_shape(RESPONSE_IDS) \n",
    "            completion_mask.log() \n",
    "\n",
    "            # Save scalar values for logging before clearing.\n",
    "            kl_div_val=kl_div.mean().item()\n",
    "            \n",
    "            # Remove now-unused intermediate tensors.\n",
    "            ref_log_probs = None\n",
    "            ref_full_shift_logits = None\n",
    "            full_shift_logits = None\n",
    "            full_shift_ids = None\n",
    "            model_log_probs = None\n",
    "            token_kl = None \n",
    "            \n",
    "            # 4. Calculate rewards.\n",
    "            print_step(\"4. Reward calc\")\n",
    "            \n",
    "            reward_work_result = reward_work.take_result()\n",
    "            if not reward_work_result:  # reward list is empty\n",
    "                response_texts = tokenizer.batch_decode(respone_ids, skip_special_tokens=True)\n",
    "                writer.add_text(f\"{log_group}/reward_empty_response\", str(response_texts), global_step=step)\n",
    "                rewards = [0.0]*group_size\n",
    "                if False:\n",
    "                    # Remove now-unused intermediate tensors.\n",
    "                    kl_div = None\n",
    "                    respone_ids = None\n",
    "                    probability_ratio = None\n",
    "                    completion_mask = None\n",
    "                    continue  # skip to next batch\n",
    "            else:\n",
    "                rewards, responses = reward_work_result\n",
    "            sum_reward += sum(rewards)\n",
    "            # rewards list[batch]\n",
    "            \n",
    "            if all(reward > 2.0 for reward in rewards):\n",
    "                # perfect no loss in grouped_ppo\n",
    "                grouped_ppo_loss = -(torch.ones(len(rewards), dtype=torch.float32, device=device) + epsilon).check_shape([batch_size])\n",
    "                grouped_ppo_loss.log()\n",
    "            else:\n",
    "                # Convert rewards to tensor\n",
    "                grouped_batch_size = len(rewards) // group_size\n",
    "                advantages = torch.tensor(rewards, dtype=torch.float32, device=device).view(grouped_batch_size, group_size).check_range(0, 2.24)\n",
    "                advantages.log() \n",
    "                \n",
    "                # Calculate mean and std per batch (along dim=1) and repeat to match original size\n",
    "                mean_rewards = advantages.mean(dim=1).repeat_interleave(group_size).check_shape([batch_size]).check_range(0, 2.24)\n",
    "                mean_rewards.log()\n",
    "                std_rewards = advantages.std(dim=1).repeat_interleave(group_size).check_shape([batch_size]).check_range(0, float('inf'))\n",
    "                std_rewards.log() \n",
    "                print(\">>>>>>>>>>>>>>>>>>>> mean_rewards:\", mean_rewards[0].item(), \"std_rewards:\", std_rewards[0].item(), \"Rewards:\", rewards)\n",
    "    \n",
    "                # Reshape back to original form\n",
    "                advantages = advantages.view(-1)\n",
    "                advantages.check_shape([batch_size]).log(\"advantages before A_hat\")\n",
    "                A_hat = ((advantages - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1).check_shape([batch_size, 1]) # range (-infinite, infinite)\n",
    "                A_hat.log() \n",
    "                \n",
    "                # Clear rewards intermediates.\n",
    "                advantages = None\n",
    "                # 5. grouped_ppo Loss Calc\n",
    "                print_step(\"5. Grouped ppo Loss Calc\")            \n",
    "                # PPO objective calculations.\n",
    "                unclipped_objective = probability_ratio\n",
    "                unclipped_objective.check_shape(RESPONSE_IDS).log()\n",
    "                epsilon_high = torch.full_like(unclipped_objective, 1 + epsilon).check_shape(RESPONSE_IDS)\n",
    "                epsilon_low  = torch.full_like(unclipped_objective, 1 - epsilon).check_shape(RESPONSE_IDS)\n",
    "                \n",
    "    \n",
    "                # if advantage > 0: take the minimum (to avoid too large an update)\n",
    "                # if advantage < 0: take the maximum\n",
    "                _grouped_ppo_loss = - torch.where(A_hat >= 0 -1e-4, \n",
    "                            torch.minimum(unclipped_objective, epsilon_high), # do not use torch.max()\n",
    "                            torch.maximum(unclipped_objective, epsilon_low))  # do not use torch.min()\n",
    "                _grouped_ppo_loss.log(\"before A_hat multiply\")\n",
    "                _grouped_ppo_loss = _grouped_ppo_loss * A_hat\n",
    "                _grouped_ppo_loss.check_shape(RESPONSE_IDS).log() \n",
    "                grouped_ppo_loss = _grouped_ppo_loss.mean(dim=-1).check_shape([batch_size])\n",
    "                grouped_ppo_loss.log() # sample epsilon=0.2\n",
    "    \n",
    "                # Remove now-unused intermediate tensors.\n",
    "                A_hat = None\n",
    "                unclipped_objective = None\n",
    "                clipped_ratio = None\n",
    "                clipped_objective = None\n",
    "            \n",
    "            \n",
    "            # Assume kl_lambda is a scaling factor for the KL term\n",
    "            _combined_loss = grouped_ppo_loss + kl_lambda * kl_div\n",
    "            _combined_loss.check_shape([batch_size]).log() \n",
    "            combined_loss = _combined_loss.mean()\n",
    "            combined_loss.log() # []\n",
    "\n",
    "            # Save scalar values for logging before clearing.\n",
    "            ppo_loss_val = grouped_ppo_loss.mean().item()\n",
    "            combined_loss_val = combined_loss.mean().item()\n",
    "\n",
    "            # Remove now-unused intermediate tensors.\n",
    "            respone_ids = None\n",
    "            grouped_ppo_loss = None\n",
    "            kl_div = None\n",
    "            probability_ratio = None\n",
    "\n",
    "            # Save scalar values for logging before clearing.\n",
    "            print(\"Final Loss:\", combined_loss_val, \", ppo_loss:\", ppo_loss_val, \", kl_div:\", kl_div_val)\n",
    "\n",
    "            # Remove now-unused intermediate tensors.\n",
    "            completion_mask = None\n",
    "            per_token_loss = None\n",
    "            \n",
    "            # 6. Backpropagation and parameter update (only if not in validation mode).\n",
    "            print_step(\"6. Backpropagation and parameter update\") \n",
    "            is_param_updated = False\n",
    "\n",
    "        if not is_validation:\n",
    "            scaler.scale(combined_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            write_weight_state(model, writer, step, log_group+'_weights')\n",
    "            if False:\n",
    "                change_grad(model, 0, 2, multiple=0.01)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            is_param_updated = True\n",
    "            # Remove unused variables from the current iteration.\n",
    "            combined_loss = None\n",
    "            #compare_memory_ids(start_tensor_ids)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(f\"{log_group}/lr\", current_lr, step)\n",
    "        else:\n",
    "            combined_loss = None\n",
    "\n",
    "        running_loss += combined_loss_val\n",
    "\n",
    "        # 7. Logging with dynamic log group.\n",
    "        print_step(\"7. Logging\")\n",
    "        writer.add_scalar(f\"{log_group}/combined_loss\", combined_loss_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/ppo_loss\", ppo_loss_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/kl_div\", kl_div_val, global_step)\n",
    "        writer.add_scalar(f\"{log_group}/mean_reward\", sum(rewards) / len(rewards), global_step)\n",
    "        if is_param_updated:\n",
    "            writer.add_scalar(f\"{log_group}/model_update_combined_loss\", combined_loss_val, global_step)\n",
    "\n",
    "        print_step(\"8. End Loop\")\n",
    "        global_step += 1\n",
    "\n",
    "    mean_reward = sum_reward/((global_step-run_start_global_step)*batch_size)\n",
    "    writer.add_scalar(f\"{log_group}_epoch/mean_reward\", mean_reward, global_step)\n",
    "\n",
    "    print_step(\"_.1. run() exit\")\n",
    "    return running_loss, mean_reward, global_step\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start reasoning logic....\")\n",
    "    main(\n",
    "        skip_validation_step,\n",
    "        objective,\n",
    "        is_finding_opt, \n",
    "        category_count_start=category_count_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9d8ad-168d-493f-8bb2-b4ea85872d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
