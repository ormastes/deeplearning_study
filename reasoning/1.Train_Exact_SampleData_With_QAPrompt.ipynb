{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchtext-0.18.0a0+9bed85d-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchaudio-2.6.0a0+d883142-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
      "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.12/dist-packages (0.3.0)\n",
      "Requirement already satisfied: lion_pytorch in /usr/local/lib/python3.12/dist-packages (0.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (2.7.0a0+ecf3bae40a.nv25.2)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import and setup\n",
    "# ------------------------------------------------\n",
    "\n",
    "!pip install datasets torch_optimizer lion_pytorch --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145ce4a5-2703-4960-97b9-b6ca27a9a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf \"runs/starcoder2_optuna_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f313db-ce11-4717-b852-8cbae73cbddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Custom model which added 2 layer in front on starcoder2\n",
    "# ------------------------------------------------\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "class CustomStarcoder2ForCausalLM(AutoModelForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # The model is initialized normally.\n",
    "    \n",
    "    def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "        \"\"\"\n",
    "        Inserts a new transformer block at the beginning of the transformer's block list.\n",
    "        \n",
    "        Args:\n",
    "            copy_weights (bool): If True, the new block is initialized as a deep copy of the \n",
    "                                 current first block. Otherwise, it is freshly constructed.\n",
    "        \"\"\"\n",
    "        # Verify that the model has the expected attribute.\n",
    "        if not hasattr(self, \"transformer\") or not hasattr(self.transformer, \"h\"):\n",
    "            raise AttributeError(\"The model does not have attribute 'transformer.h'. \"\n",
    "                                 \"Please adjust the code to match your model's architecture.\")\n",
    "        \n",
    "        # Retrieve the current first transformer block.\n",
    "        original_first_block = self.transformer.h[0]\n",
    "        \n",
    "        # Create a new block.\n",
    "        new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "        \n",
    "        # Insert the new block at index 0.\n",
    "        self.transformer.h.insert(0, new_block)\n",
    "        \n",
    "        # Update the configuration to reflect the extra layer.\n",
    "        self.config.num_hidden_layers += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238b7afe-d05e-4ff2-914c-85d01d64eb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the first transformer block: 95979008\n"
     ]
    }
   ],
   "source": [
    "# add 2 layer in front on starcoder2\n",
    "# ------------------------------------------------\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "ref_model = None\n",
    "model = None\n",
    "old_model = None\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    original_first_block =  self.model.layers[0]\n",
    "    \n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "    \n",
    "    self.model.layers.insert(0, new_block)\n",
    "    \n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def get_layer_params(self, layer_index: int = 0):\n",
    "    return list(self.model.layers[layer_index].parameters())\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage:\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "\n",
    "# Load the original model and its config.\n",
    "orig_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "config = orig_model.config\n",
    "\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "\n",
    "# Suppose your config model_type is \"starcoder2\".\n",
    "config.model_type = \"Starcoder2Model2\"\n",
    "\n",
    "\n",
    "# Register your custom model class.\n",
    "MODEL_FOR_CAUSAL_LM_MAPPING.register(CustomStarcoder2ForCausalLM, \"Starcoder2Model2\")\n",
    "\n",
    "# Now from_config will return your custom class.\n",
    "custom_model = CustomStarcoder2ForCausalLM.from_config(config)\n",
    "\n",
    "# (Optionally) load weights from the original model using strict=False.\n",
    "custom_model.load_state_dict(orig_model.state_dict(), strict=False)\n",
    "\n",
    "add_front_transformer_block(custom_model, copy_weights=True)\n",
    "first_block_params = get_layer_params(custom_model, layer_index=0)\n",
    "print(\"Number of parameters in the first transformer block:\",\n",
    "      sum(p.numel() for p in first_block_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from saved checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8c965013f14a6b843975facb5bebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n",
      "/tmp/ipykernel_17191/2812455108.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "### REPLY\n",
      "\n",
      "### REPLY\n",
      "\n",
      "### REPLY \n",
      "\n",
      "### REPLY\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "Expected: Custom Clang-repl takes only one line input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17191/2812455108.py:201: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 7.2102\n",
      "Checkpoint saved at epoch 1 to ./saved_models/prompt/epoch_1/checkpoint.pt\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "```\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "\n",
      "###\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "```\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "\n",
      "### Instruction\n",
      "\n",
      "```\n",
      "\n",
      "###\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 2 completed. Average Loss: 6.4614\n",
      "Epoch 3 completed. Average Loss: 5.8126\n",
      "Epoch 4 completed. Average Loss: 5.6045\n",
      "Epoch 5 completed. Average Loss: 5.4338\n",
      "Epoch 6 completed. Average Loss: 5.1429\n",
      "Epoch 7 completed. Average Loss: 5.0833\n",
      "Epoch 8 completed. Average Loss: 4.9608\n",
      "Epoch 9 completed. Average Loss: 4.6655\n",
      "Epoch 10 completed. Average Loss: 4.5836\n",
      "Epoch 11 completed. Average Loss: 4.5224\n",
      "Checkpoint saved at epoch 11 to ./saved_models/prompt/epoch_11/checkpoint.pt\n",
      "Sample Output (Epoch 11): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is the same as in the REPL.\n",
      "\n",
      "### Solution\n",
      "\n",
      "The prompt\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 11): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "No, we don't.\n",
      "\n",
      "### Discussion\n",
      "\n",
      "I'm not sure if\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 11): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "```python\n",
      "def reverse(s):\n",
      "\treturn s[::-1]\n",
      "```\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 12 completed. Average Loss: 4.3945\n",
      "Epoch 13 completed. Average Loss: 4.4714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 425\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Study completed!\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;66;03m# Best trial:\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m#  Value: 715.3611988491482\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m#    num_grpo: 2\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_grpo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_grpo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_epochs\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 359\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, lr, kl_lambda, epsilon, num_grpo, save_epochs)\u001b[0m\n\u001b[1;32m    356\u001b[0m scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler()\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Train & get final metric\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m final_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_grpo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_grpo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkl_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Return the final average loss to Optuna\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_avg_loss\n",
      "Cell \u001b[0;32mIn[5], line 265\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, ref_model, dataloader, optimizer, device, num_epochs, num_grpo, epsilon, kl_lambda, scaler, save_epochs, start_epoch)\u001b[0m\n\u001b[1;32m    258\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch/Average_Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_loss, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    260\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    264\u001b[0m }\n\u001b[0;32m--> 265\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_checkpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39msave_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m checkpoint_dir_pre\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py:938\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    935\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    939\u001b[0m         _save(\n\u001b[1;32m    940\u001b[0m             obj,\n\u001b[1;32m    941\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py:805\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    804\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py:776\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    773\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, get_crc32_options())\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "# ------------------------------------------------\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "last_checkpoint_path = \"./saved_models/prompt/checkpoint.pt\"\n",
    "checkpoint_dir_pre = \"./saved_models/prompt/epoch_\"\n",
    "\n",
    "# num_iterations=1, num_steps=500, batch_size=4, num_generations=4, max_completion_length=128, kl=0.1,\n",
    "# learning_rate=5e-6, mu=3, epsilon=0.2,\n",
    "#\n",
    "# lr: 7.205691481165551e-05 kl_lambda: 0.2654706177039008 epsilon: 0.019437902361559744 num_grpo: 1\n",
    "# lr: 1.1111588431283189e-06 kl_lambda: 0.15842765249477542 epsilon: 0.11144786260484413 num_grpo: 3\n",
    "is_finding_opt=False\n",
    "if not is_finding_opt:\n",
    "    num_epochs=100\n",
    "    lr=1.1111588431283189e-06\n",
    "    kl_lambda=0.15842765249477542\n",
    "    epsilon=0.11144786260484413\n",
    "    num_grpo=1\n",
    "    save_epochs=10\n",
    "\n",
    "def object_hiper_param(trial):\n",
    "    # Shortened training for demonstration:\n",
    "    num_epochs = 1#2   # or 2–3, to save time during hyperparameter search\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    kl_lambda = trial.suggest_float(\"kl_lambda\", 0.0, 1.0)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.2)\n",
    "    num_grpo = trial.suggest_int(\"num_grpo\", 1, 3, step=1)\n",
    "\n",
    "    return num_epochs, lr, kl_lambda, epsilon, num_grpo\n",
    "\n",
    "\n",
    "def samping(model, tokenizer, device, epoch, writer, sample_prompt, expected):\n",
    "    # Include attention_mask in the tokenization\n",
    "    sample_prompt = f\"### Instruction\\n\\n{sample_prompt}\\n\\n### Response\"\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Pass the attention_mask and explicitly set pad_token_id to eos_token_id for reliable generation\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    sample_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    sample_text = sample_text.strip()\n",
    "    print(f\"Sample Output (Epoch {epoch+1}): {sample_text}\")\n",
    "    print(\"Expected:\", expected)\n",
    "    writer.add_text(\"Sample Output\", f\"Epoch {epoch+1}: {sample_text}\", epoch)\n",
    "\n",
    "def selective_log_softmax(logits, input_ids):\n",
    "    # https://blog.gopenai.com/coding-grpo-from-scratch-a-guide-to-distributed-implementation-with-qwen2-5-1-5b-instruct-59b34227edac\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    layer_index = 1\n",
    "    original_first_block =  self.model.layers[layer_index]\n",
    "    \n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "    \n",
    "    self.model.layers.insert(layer_index, new_block)\n",
    "    \n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def get_layer_params(self, layer_index: int = 0):\n",
    "    first_params = list(self.model.layers[0].parameters())\n",
    "    sec_params = list(self.model.layers[0].parameters())\n",
    "    #last_params = list(self.model.layers[-1].parameters())\n",
    "    return first_params + sec_params# + last_params\n",
    "    \n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "        train_examples.append({\"content\": content})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "train_data = load_qa_dataset(qa_json_path)\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "# Load tokenizer from saved directory if exists; otherwise, load from pretrained.\n",
    "tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "if os.path.exists(tokenizer_save_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_dir)\n",
    "    print(\"Loaded tokenizer from saved checkpoint.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    ref_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    num_grpo,\n",
    "    epsilon,\n",
    "    kl_lambda,\n",
    "    scaler,\n",
    "    save_epochs,\n",
    "    start_epoch\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for `num_epochs` with `num_grpo` PPO groups each epoch,\n",
    "    and return a metric (e.g., final average loss).\n",
    "    \"\"\"\n",
    "    global last_checkpoint_dir\n",
    "    # Initialize TensorBoard writer (optional)\n",
    "    writer = SummaryWriter(log_dir=\"runs/starcoder2_optuna_experiment\")\n",
    "\n",
    "    # --- Generate sample output text after each epoch ---\n",
    "    model.eval()  # Switch to eval mode for generation\n",
    "    with torch.no_grad():\n",
    "        samping(model, tokenizer, device, 0, writer, \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\", \"```\\n>>> (prompt)\\n```\")\n",
    "        samping(model, tokenizer, device, 0, writer, \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\", \"Custom Clang-repl takes only one line input.\")\n",
    "    model.train()  # Switch back to training mode\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        old_model = None\n",
    "        old_model = copy.deepcopy(model)\n",
    "        old_model = old_model.half()\n",
    "        old_model.eval()\n",
    "        for param in old_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for grpo_idx in range(num_grpo):\n",
    "            for step, batch in enumerate(dataloader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    # A placeholder for advantage (you'd replace this with real advantage if doing PPO)\n",
    "                    advantages = -loss\n",
    "\n",
    "                    # old model forward\n",
    "                    with torch.no_grad():\n",
    "                        old_outputs = old_model(**batch)\n",
    "\n",
    "                    model_logits     = outputs.logits\n",
    "                    old_model_logits = old_outputs.logits\n",
    "\n",
    "                    # reference model forward\n",
    "                    with torch.no_grad():\n",
    "                        ref_outputs = ref_model(**batch)\n",
    "                    ref_logits = ref_outputs.logits\n",
    "\n",
    "                    # Probability ratio\n",
    "                    # In real PPO, you'd convert logits -> log_probs, then ratio = exp(new_log_prob - old_log_prob)\n",
    "                    probability_ratio = model_logits / (old_model_logits + 1e-8)\n",
    "\n",
    "                    # Unclipped objective\n",
    "                    unclipped_objective = probability_ratio * advantages\n",
    "\n",
    "                    # Clipped objective\n",
    "                    clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                    clipped_objective = clipped_ratio * advantages\n",
    "\n",
    "                    #ppo_loss = -clipped_objective.mean()\n",
    "                    #ppo_loss = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "                    ppo_loss = loss.mean()\n",
    "\n",
    "                    # KL\n",
    "                    model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                    ref_log_probs   = F.softmax(ref_logits, dim=-1)\n",
    "                    kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "\n",
    "                    combined_loss = ppo_loss #+ kl_lambda * kl_div\n",
    "\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += combined_loss.item()\n",
    "\n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar(\"Loss/combined_loss\", combined_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/ppo_loss\", ppo_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/kl_div\", kl_div.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/original_loss\", loss.item(), global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Epoch/Average_Loss\", avg_loss, epoch+1)\n",
    "\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1\n",
    "        }\n",
    "        torch.save(checkpoint, last_checkpoint_path)\n",
    "\n",
    "        if save_epochs is not None and epoch%save_epochs == 0:\n",
    "            global checkpoint_dir_pre\n",
    "            checkpoint_dir = checkpoint_dir_pre + str(epoch+1)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch + 1\n",
    "            }\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1} to {checkpoint_path}\")\n",
    "\n",
    "            # Save tokenizer only if it has not been saved before.\n",
    "            tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "            \n",
    "            if not os.path.exists(tokenizer_save_dir):\n",
    "                os.makedirs(tokenizer_save_dir, exist_ok=True)\n",
    "                tokenizer.save_pretrained(tokenizer_save_dir)\n",
    "                print(\"Tokenizer saved.\")\n",
    "\n",
    "            # --- Generate sample output text after each epoch ---\n",
    "            model.eval()  # Switch to eval mode for generation\n",
    "            with torch.no_grad():\n",
    "                samping(model, tokenizer, device, epoch, writer, \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\", \"```\\n>>> (prompt)\\n```\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\", \"Custom Clang-repl takes only one line input.\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"Make python string reverse function\", \"def reverse(text):\\n    return reverse(text[1:])+text[0]\")\n",
    "\n",
    "                print(\"=====================================================================================================\")\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "    # Return final average loss as the metric to minimize\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        kl_lambda,\n",
    "        epsilon,\n",
    "        num_grpo,\n",
    "        save_epochs=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Check if a latest checkpoint exists to load model and optimizer states\n",
    "    if False: #os.path.exists(last_checkpoint_path):\n",
    "        checkpoint = torch.load(last_checkpoint_path, map_location=device)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path} at epoch {start_epoch}\")\n",
    "    else:\n",
    "        _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        config = _model.config\n",
    "        config.num_hidden_layers += 2\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        model.load_state_dict(_model.state_dict(), strict=False)\n",
    "        for i in range(len(model.model.layers)-1, 0, -1):\n",
    "            model.model.layers[i].load_state_dict(model.model.layers[i-1].state_dict())\n",
    "        for i in range(len(model.model.layers)-1, 0, -1):\n",
    "            model.model.layers[i].load_state_dict(model.model.layers[i-1].state_dict())\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        start_epoch = 0\n",
    "        \n",
    "    #for name, module in model.named_modules():\n",
    "    #    print(f\"{name}: {module}\")    \n",
    "\n",
    "    # Reference model (for KL)\n",
    "    old_model = None\n",
    "    ref_model = copy.deepcopy(model).half().eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # DataLoader\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        dataloader=dataloader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        num_grpo=num_grpo,\n",
    "        epsilon=epsilon,\n",
    "        kl_lambda=kl_lambda,\n",
    "        scaler=scaler,\n",
    "        save_epochs=save_epochs,\n",
    "        start_epoch=start_epoch\n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "    \n",
    "# ------------------------------------------------\n",
    "# Optuna Objective Function\n",
    "# ------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines how Optuna will run each trial:\n",
    "    - sample hyperparameters\n",
    "    - set up the model & optimizer with those\n",
    "    - run a short training loop\n",
    "    - return a metric (the final avg loss) to minimize\n",
    "    \"\"\"\n",
    "    num_epochs, lr, kl_lambda, epsilon, num_grpo = object_hiper_param(trial)\n",
    "\n",
    "    print(f\"[Optuna] Trial hyperparameters -> lr: {lr}, kl_lambda: {kl_lambda}, epsilon: {epsilon}, num_grpo: {num_grpo}\")\n",
    "    return train( \n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        kl_lambda=kl_lambda,\n",
    "        epsilon=epsilon,\n",
    "        num_grpo=num_grpo)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    if is_finding_opt:\n",
    "        # Create study to minimize final loss\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "    \n",
    "        print(\"Study completed!\")\n",
    "        print(\"Best trial:\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Value: {best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"#    {key}: {value}\")\n",
    "        # Study completed!\n",
    "        # Best trial:\n",
    "        #  Value: 715.3611988491482\n",
    "        #  Params: \n",
    "        #    lr: 0.0002746775018590349\n",
    "        #    kl_lambda: 0.10527608699361579\n",
    "        #    epsilon: 0.12442505216944565\n",
    "        #    num_grpo: 2\n",
    "    else:\n",
    "        train(\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            kl_lambda=kl_lambda,\n",
    "            epsilon=epsilon,\n",
    "            num_grpo=num_grpo,\n",
    "            save_epochs=save_epochs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71048004-5b1c-4335-a0cd-bbafca1d9433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Copy data to \n",
    "# ------------------------------------------------\n",
    "\n",
    "!cp ./saved_models/prompt/prompt_epoch_91/checkpoint.pt ./saved_models/prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d453b-7836-427d-a3f7-522bf7b4d197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
