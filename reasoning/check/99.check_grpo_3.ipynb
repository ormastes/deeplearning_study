{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchtext-0.18.0a0+9bed85d-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/torchaudio-2.6.0a0+d883142-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (3.4.1)\n",
      "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.12/dist-packages (0.3.0)\n",
      "Requirement already satisfied: lion_pytorch in /usr/local/lib/python3.12/dist-packages (0.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (2.7.0a0+ecf3bae40a.nv25.2)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets torch_optimizer lion_pytorch --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a399b2ba-6133-4e75-b6f9-2b903bae12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = None\n",
    "model = None\n",
    "old_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145ce4a5-2703-4960-97b9-b6ca27a9a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"runs/starcoder2_optuna_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:369: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:369: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from saved checkpoint.\n",
      "eos:  <|endoftext|> 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f3a9b74deb4b52aa74be8abd7654f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60362/3570149279.py:369: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(False, \"prompt_last_checkpoint_path must exist\")\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ./saved_models/sample/checkpoint.pt at epoch 91\n",
      "9\n",
      "Memory allocated: 12292.54 MB\n",
      "Memory reserved: 12322.00 MB\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 58.25 MiB is free. Process 860618 has 32.62 GiB memory in use. Process 868728 has 14.72 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 32.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 461\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# Study completed!\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# Best trial:\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m#  Value: 715.3611988491482\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m#    num_grpo: 2\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_grpo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_grpo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_epochs\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 378\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, lr, kl_lambda, epsilon, num_grpo, save_epochs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Reference model (for KL)\u001b[39;00m\n\u001b[1;32m    377\u001b[0m old_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m ref_model \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m ref_model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    380\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "    \u001b[0;31m[... skipping similar frames: _deepcopy_dict at line 221 (8 times), deepcopy at line 136 (8 times), _reconstruct at line 259 (3 times), deepcopy at line 162 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "    \u001b[0;31m[... skipping similar frames: _deepcopy_dict at line 221 (1 times), deepcopy at line 136 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.12/copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/parameter.py:68\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 58.25 MiB is free. Process 860618 has 32.62 GiB memory in use. Process 868728 has 14.72 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 32.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "import pickle\n",
    "from Config import SimpleConfig\n",
    "\n",
    "prompt_last_checkpoint_path = \"./saved_models/prompt/checkpoint.pt\"\n",
    "last_checkpoint_path = \"./saved_models/sample/checkpoint.pt\"\n",
    "checkpoint_dir_pre = \"./saved_models/sample/epoch_\"\n",
    "\n",
    "config = SimpleConfig()\n",
    "\n",
    "# num_iterations=1, num_steps=500, batch_size=4, num_generations=4, max_completion_length=128, kl=0.1,\n",
    "# learning_rate=5e-6, mu=3, epsilon=0.2,\n",
    "#\n",
    "# lr: 7.205691481165551e-05 kl_lambda: 0.2654706177039008 epsilon: 0.019437902361559744 num_grpo: 1\n",
    "# lr: 1.1111588431283189e-06 kl_lambda: 0.15842765249477542 epsilon: 0.11144786260484413 num_grpo: 3\n",
    "is_finding_opt=False\n",
    "if not is_finding_opt:\n",
    "    num_epochs=400\n",
    "    lr=1.1111588431283189e-06\n",
    "    kl_lambda=0.15842765249477542\n",
    "    epsilon=0.11144786260484413\n",
    "    num_grpo=1\n",
    "    save_epochs=10\n",
    "\n",
    "def object_hiper_param(trial):\n",
    "    # Shortened training for demonstration:\n",
    "    num_epochs = 1#2   # or 2â€“3, to save time during hyperparameter search\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    kl_lambda = trial.suggest_float(\"kl_lambda\", 0.0, 1.0)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.2)\n",
    "    num_grpo = trial.suggest_int(\"num_grpo\", 1, 3, step=1)\n",
    "\n",
    "    return num_epochs, lr, kl_lambda, epsilon, num_grpo\n",
    "\n",
    "def print_memory(tag):\n",
    "    # Make sure you have a GPU device available.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Print current allocated and reserved memory in MB:\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "    print(tag)\n",
    "    print(f\"Memory allocated: {allocated:.2f} MB\")\n",
    "    print(f\"Memory reserved: {reserved:.2f} MB\")\n",
    "\n",
    "def samping(model, tokenizer, device, epoch, writer, sample_prompt, expected):\n",
    "    # Include attention_mask in the tokenization\n",
    "    sample_prompt = f\"### Instruction\\n\\n{sample_prompt}\\nWrtie a Clang-repl Test\\n### Response\"\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Pass the attention_mask and explicitly set pad_token_id to eos_token_id for reliable generation\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=200,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    sample_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    sample_text = sample_text.strip()\n",
    "    print(f\"Sample Output (Epoch {epoch+1}): {sample_text}\")\n",
    "    print(\"Expected:\", expected)\n",
    "    writer.add_text(\"Sample Output\", f\"Epoch {epoch+1}: {sample_text}\", epoch)\n",
    "\n",
    "def selective_log_softmax(logits, input_ids):\n",
    "    # https://blog.gopenai.com/coding-grpo-from-scratch-a-guide-to-distributed-implementation-with-qwen2-5-1-5b-instruct-59b34227edac\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    layer_index = 1\n",
    "    original_first_block =  self.model.layers[layer_index]\n",
    "    \n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "    \n",
    "    self.model.layers.insert(layer_index, new_block)\n",
    "    \n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def get_layer_params(self, layer_index: int = 0):\n",
    "    first_params = list(self.model.layers[0].parameters())\n",
    "    sec_params = list(self.model.layers[0].parameters())\n",
    "    #last_params = list(self.model.layers[-1].parameters())\n",
    "    return first_params + sec_params# + last_params\n",
    "    \n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "        train_examples.append({\"content\": content+\"<|endoftext|>\"})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "def load_sample_dataset(pk_file):\n",
    "    with open(config.dataset_file, \"rb\") as f:\n",
    "        global_samples = pickle.load(f)\n",
    "        sample_dataset = []\n",
    "        for sample in global_samples:\n",
    "                sample_dataset.append({\"content\": sample+\"<|endoftext|>\"})\n",
    "        return sample_dataset\n",
    "\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "train_data_prompt = load_qa_dataset(qa_json_path)\n",
    "train_data_sample = load_sample_dataset(config.dataset_file)\n",
    "train_data = train_data_prompt + (train_data_sample*10)\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "# Load tokenizer from saved directory if exists; otherwise, load from pretrained.\n",
    "tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "if os.path.exists(tokenizer_save_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_dir)\n",
    "    print(\"Loaded tokenizer from saved checkpoint.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "print(\"eos: \", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    ref_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    num_grpo,\n",
    "    epsilon,\n",
    "    kl_lambda,\n",
    "    scaler,\n",
    "    save_epochs,\n",
    "    start_epoch\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for `num_epochs` with `num_grpo` PPO groups each epoch,\n",
    "    and return a metric (e.g., final average loss).\n",
    "    \"\"\"\n",
    "    # Initialize TensorBoard writer (optional)\n",
    "    writer = SummaryWriter(log_dir=\"runs/starcoder2_optuna_experiment\")\n",
    "\n",
    "    # --- Generate sample output text after each epoch ---\n",
    "    model.eval()  # Switch to eval mode for generation\n",
    "    with torch.no_grad():\n",
    "        samping(model, tokenizer, device, 0, writer, \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\", \"```\\n>>> (prompt)\\n```\")\n",
    "        samping(model, tokenizer, device, 0, writer, \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\", \"Custom Clang-repl takes only one line input.\")\n",
    "    model.train()  # Switch back to training mode\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print_memory(20)\n",
    "        old_model = None\n",
    "        old_model = copy.deepcopy(model)\n",
    "        old_model = old_model.half()\n",
    "        old_model.eval()\n",
    "        for param in old_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print_memory(21)\n",
    "\n",
    "        for grpo_idx in range(num_grpo):\n",
    "            for step, batch in enumerate(dataloader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    # A placeholder for advantage (you'd replace this with real advantage if doing PPO)\n",
    "                    advantages = -loss\n",
    "\n",
    "                    # old model forward\n",
    "                    with torch.no_grad():\n",
    "                        old_outputs = old_model(**batch)\n",
    "\n",
    "                    model_logits     = outputs.logits\n",
    "                    old_model_logits = old_outputs.logits\n",
    "\n",
    "                    # reference model forward\n",
    "                    with torch.no_grad():\n",
    "                        ref_outputs = ref_model(**batch)\n",
    "                    ref_logits = ref_outputs.logits\n",
    "\n",
    "                    # Probability ratio\n",
    "                    # In real PPO, you'd convert logits -> log_probs, then ratio = exp(new_log_prob - old_log_prob)\n",
    "                    probability_ratio = model_logits / (old_model_logits + 1e-8)\n",
    "\n",
    "                    # Unclipped objective\n",
    "                    unclipped_objective = probability_ratio * advantages\n",
    "\n",
    "                    # Clipped objective\n",
    "                    clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                    clipped_objective = clipped_ratio * advantages\n",
    "\n",
    "                    #ppo_loss = -clipped_objective.mean()\n",
    "                    #ppo_loss = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "                    ppo_loss = loss.mean()\n",
    "\n",
    "                    # KL\n",
    "                    model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                    ref_log_probs   = F.log_softmax(ref_logits, dim=-1)\n",
    "                    kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "\n",
    "                    combined_loss = ppo_loss #+ kl_lambda * kl_div\n",
    "\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += combined_loss.item()\n",
    "\n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar(\"Loss/combined_loss\", combined_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/ppo_loss\", ppo_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/kl_div\", kl_div.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/original_loss\", loss.item(), global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Epoch/Average_Loss\", avg_loss, epoch+1)\n",
    "\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1\n",
    "        }\n",
    "        torch.save(checkpoint, last_checkpoint_path)\n",
    "\n",
    "        if save_epochs is not None and epoch%save_epochs == 0:\n",
    "            global checkpoint_dir_pre\n",
    "            checkpoint_dir = checkpoint_dir_pre + str(epoch+1)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch + 1\n",
    "            }\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1} to {checkpoint_path}\")\n",
    "\n",
    "            # Save tokenizer only if it has not been saved before.\n",
    "            tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "            if not os.path.exists(tokenizer_save_dir):\n",
    "                os.makedirs(tokenizer_save_dir, exist_ok=True)\n",
    "                tokenizer.save_pretrained(tokenizer_save_dir)\n",
    "                print(\"Tokenizer saved.\")\n",
    "\n",
    "            # --- Generate sample output text after each epoch ---\n",
    "            model.eval()  # Switch to eval mode for generation\n",
    "            with torch.no_grad():\n",
    "                samping(model, tokenizer, device, epoch, writer, \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\", \"```\\n>>> (prompt)\\n```\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\", \"Custom Clang-repl takes only one line input.\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"Make python string reverse function\", \"def reverse(text):\\n    return reverse(text[1:])+text[0]\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"<Test Target Object>\\nAdd two integers. and return the sum.\\n</Test Target Object>\\n\", \"....\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"<Test Target>\\nbool isEven(int x) {\\n    return (x % 2) == 0;\\n}\\n</Test Target>\\n\", \"....\")\n",
    "                print(\"=====================================================================================================\")\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "    # Return final average loss as the metric to minimize\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        kl_lambda,\n",
    "        epsilon,\n",
    "        num_grpo,\n",
    "        save_epochs=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Check if a latest checkpoint exists to load model and optimizer states\n",
    "    if os.path.exists(last_checkpoint_path):\n",
    "        print_memory(1)\n",
    "        checkpoint = torch.load(prompt_last_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "        _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        config = _model.config\n",
    "        config.num_hidden_layers += 2\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        print_memory(7)\n",
    "        print(f\"Loaded checkpoint from {last_checkpoint_path} at epoch {start_epoch}\")\n",
    "    else:\n",
    "        if os.path.exists(prompt_last_checkpoint_path):\n",
    "            print_memory(1)\n",
    "            checkpoint = torch.load(prompt_last_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "            _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            config = copy.deepcopy(_model.config)\n",
    "            config.num_hidden_layers += 2\n",
    "            model = AutoModelForCausalLM.from_config(config)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.to(device)\n",
    "            optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint.get('epoch', 0)\n",
    "            print(f\"Loaded checkpoint from {prompt_last_checkpoint_path} at epoch {start_epoch}\")\n",
    "            print_memory(7)\n",
    "            \n",
    "        else:\n",
    "            assert(False, \"prompt_last_checkpoint_path must exist\")\n",
    "    \n",
    "    # Clear cached memory that is no longer used\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()   \n",
    "    print_memory(9)\n",
    "\n",
    "    # Reference model (for KL)\n",
    "    old_model = None\n",
    "    ref_model = copy.deepcopy(model).half().eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print_memory(10)\n",
    "\n",
    "    # DataLoader\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        dataloader=dataloader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        num_grpo=num_grpo,\n",
    "        epsilon=epsilon,\n",
    "        kl_lambda=kl_lambda,\n",
    "        scaler=scaler,\n",
    "        save_epochs=save_epochs,\n",
    "        start_epoch=start_epoch\n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "    \n",
    "# ------------------------------------------------\n",
    "# Optuna Objective Function\n",
    "# ------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines how Optuna will run each trial:\n",
    "    - sample hyperparameters\n",
    "    - set up the model & optimizer with those\n",
    "    - run a short training loop\n",
    "    - return a metric (the final avg loss) to minimize\n",
    "    \"\"\"\n",
    "    num_epochs, lr, kl_lambda, epsilon, num_grpo = object_hiper_param(trial)\n",
    "\n",
    "    print(f\"[Optuna] Trial hyperparameters -> lr: {lr}, kl_lambda: {kl_lambda}, epsilon: {epsilon}, num_grpo: {num_grpo}\")\n",
    "    return train( \n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        kl_lambda=kl_lambda,\n",
    "        epsilon=epsilon,\n",
    "        num_grpo=num_grpo)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    if is_finding_opt:\n",
    "        # Create study to minimize final loss\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "    \n",
    "        print(\"Study completed!\")\n",
    "        print(\"Best trial:\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Value: {best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"#    {key}: {value}\")\n",
    "        # Study completed!\n",
    "        # Best trial:\n",
    "        #  Value: 715.3611988491482\n",
    "        #  Params: \n",
    "        #    lr: 0.0002746775018590349\n",
    "        #    kl_lambda: 0.10527608699361579\n",
    "        #    epsilon: 0.12442505216944565\n",
    "        #    num_grpo: 2\n",
    "    else:\n",
    "        train(\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            kl_lambda=kl_lambda,\n",
    "            epsilon=epsilon,\n",
    "            num_grpo=num_grpo,\n",
    "            save_epochs=save_epochs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71048004-5b1c-4335-a0cd-bbafca1d9433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d453b-7836-427d-a3f7-522bf7b4d197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
