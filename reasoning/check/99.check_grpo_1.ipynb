{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets torch_optimizer lion_pytorch --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a399b2ba-6133-4e75-b6f9-2b903bae12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = None\n",
    "model = None\n",
    "old_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26468d1a-5e28-4343-bade-583cffde7db0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff133bac8ee4e7b942a8d7665789925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1983/271713972.py:99: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_1983/271713972.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1/36 Loss: 4.7925\n",
      "Epoch 1 Step 2/36 Loss: 5.2342\n",
      "Epoch 1 Step 3/36 Loss: 5.9867\n",
      "Epoch 1 Step 4/36 Loss: 5.8720\n",
      "Epoch 1 Step 5/36 Loss: 5.3160\n",
      "Epoch 1 Step 6/36 Loss: 6.8686\n",
      "Epoch 1 Step 7/36 Loss: 5.4841\n",
      "Epoch 1 Step 8/36 Loss: 5.3843\n",
      "Epoch 1 Step 9/36 Loss: 5.1209\n",
      "Epoch 1 Step 10/36 Loss: 6.4761\n",
      "Epoch 1 Step 11/36 Loss: 5.7799\n",
      "Epoch 1 Step 12/36 Loss: 6.2239\n",
      "Epoch 1 Step 13/36 Loss: 6.1394\n",
      "Epoch 1 Step 14/36 Loss: 4.2185\n",
      "Epoch 1 Step 15/36 Loss: 4.8698\n",
      "Epoch 1 Step 16/36 Loss: 4.5611\n",
      "Epoch 1 Step 17/36 Loss: 5.9608\n",
      "Epoch 1 Step 18/36 Loss: 5.5476\n",
      "Epoch 1 Step 19/36 Loss: 5.0294\n",
      "Epoch 1 Step 20/36 Loss: 4.4260\n",
      "Epoch 1 Step 21/36 Loss: 6.1924\n",
      "Epoch 1 Step 22/36 Loss: 5.3547\n",
      "Epoch 1 Step 23/36 Loss: 4.9466\n",
      "Epoch 1 Step 24/36 Loss: 5.5774\n",
      "Epoch 1 Step 25/36 Loss: 6.0174\n",
      "Epoch 1 Step 26/36 Loss: 5.1029\n",
      "Epoch 1 Step 27/36 Loss: 5.8129\n",
      "Epoch 1 Step 28/36 Loss: 5.2120\n",
      "Epoch 1 Step 29/36 Loss: 5.6221\n",
      "Epoch 1 Step 30/36 Loss: 6.3168\n",
      "Epoch 1 Step 31/36 Loss: 4.5538\n",
      "Epoch 1 Step 32/36 Loss: 6.2303\n",
      "Epoch 1 Step 33/36 Loss: 4.5739\n",
      "Epoch 1 Step 34/36 Loss: 5.2357\n",
      "Epoch 1 Step 35/36 Loss: 5.0738\n",
      "Epoch 1 Step 36/36 Loss: 6.3497\n",
      "Epoch 1 Step 1/36 Loss: 5.3144\n",
      "Epoch 1 Step 2/36 Loss: 5.8435\n",
      "Epoch 1 Step 3/36 Loss: 6.1140\n",
      "Epoch 1 Step 4/36 Loss: 5.4149\n",
      "Epoch 1 Step 5/36 Loss: 5.3945\n",
      "Epoch 1 Step 6/36 Loss: 6.0197\n",
      "Epoch 1 Step 7/36 Loss: 5.3432\n",
      "Epoch 1 Step 8/36 Loss: 5.3432\n",
      "Epoch 1 Step 9/36 Loss: 6.4476\n",
      "Epoch 1 Step 10/36 Loss: 6.4839\n",
      "Epoch 1 Step 11/36 Loss: 5.4626\n",
      "Epoch 1 Step 12/36 Loss: 5.6182\n",
      "Epoch 1 Step 13/36 Loss: 4.9541\n",
      "Epoch 1 Step 14/36 Loss: 5.6678\n",
      "Epoch 1 Step 15/36 Loss: 3.9938\n",
      "Epoch 1 Step 16/36 Loss: 5.6721\n",
      "Epoch 1 Step 17/36 Loss: 5.2744\n",
      "Epoch 1 Step 18/36 Loss: 4.4526\n",
      "Epoch 1 Step 19/36 Loss: 4.8552\n",
      "Epoch 1 Step 20/36 Loss: 4.4543\n",
      "Epoch 1 Step 21/36 Loss: 6.7400\n",
      "Epoch 1 Step 22/36 Loss: 5.9114\n",
      "Epoch 1 Step 23/36 Loss: 6.5024\n",
      "Epoch 1 Step 24/36 Loss: 5.2742\n",
      "Epoch 1 Step 25/36 Loss: 5.3029\n",
      "Epoch 1 Step 26/36 Loss: 5.6521\n",
      "Epoch 1 Step 27/36 Loss: 5.0552\n",
      "Epoch 1 Step 28/36 Loss: 4.7371\n",
      "Epoch 1 Step 29/36 Loss: 4.8631\n",
      "Epoch 1 Step 30/36 Loss: 5.7667\n",
      "Epoch 1 Step 31/36 Loss: 5.2574\n",
      "Epoch 1 Step 32/36 Loss: 4.1070\n",
      "Epoch 1 Step 33/36 Loss: 5.4828\n",
      "Epoch 1 Step 34/36 Loss: 5.9312\n",
      "Epoch 1 Step 35/36 Loss: 5.8181\n",
      "Epoch 1 Step 36/36 Loss: 5.3089\n",
      "Epoch 1 Step 1/36 Loss: 4.0361\n",
      "Epoch 1 Step 2/36 Loss: 6.6584\n",
      "Epoch 1 Step 3/36 Loss: 6.1748\n",
      "Epoch 1 Step 4/36 Loss: 7.0762\n",
      "Epoch 1 Step 5/36 Loss: 4.5486\n",
      "Epoch 1 Step 6/36 Loss: 5.1888\n",
      "Epoch 1 Step 7/36 Loss: 4.6190\n",
      "Epoch 1 Step 8/36 Loss: 5.5562\n",
      "Epoch 1 Step 9/36 Loss: 5.0937\n",
      "Epoch 1 Step 10/36 Loss: 6.1134\n",
      "Epoch 1 Step 11/36 Loss: 5.2398\n",
      "Epoch 1 Step 12/36 Loss: 6.6357\n",
      "Epoch 1 Step 13/36 Loss: 4.9811\n",
      "Epoch 1 Step 14/36 Loss: 6.0352\n",
      "Epoch 1 Step 15/36 Loss: 5.2376\n",
      "Epoch 1 Step 16/36 Loss: 7.5704\n",
      "Epoch 1 Step 17/36 Loss: 5.1286\n",
      "Epoch 1 Step 18/36 Loss: 5.9782\n",
      "Epoch 1 Step 19/36 Loss: 5.5124\n",
      "Epoch 1 Step 20/36 Loss: 4.9514\n",
      "Epoch 1 Step 21/36 Loss: 5.3983\n",
      "Epoch 1 Step 22/36 Loss: 5.9164\n",
      "Epoch 1 Step 23/36 Loss: 6.2428\n",
      "Epoch 1 Step 24/36 Loss: 4.8605\n",
      "Epoch 1 Step 25/36 Loss: 5.1503\n",
      "Epoch 1 Step 26/36 Loss: 4.4941\n",
      "Epoch 1 Step 27/36 Loss: 5.4570\n",
      "Epoch 1 Step 28/36 Loss: 6.6504\n",
      "Epoch 1 Step 29/36 Loss: 4.3474\n",
      "Epoch 1 Step 30/36 Loss: 5.2507\n",
      "Epoch 1 Step 31/36 Loss: 5.2716\n",
      "Epoch 1 Step 32/36 Loss: 4.3869\n",
      "Epoch 1 Step 33/36 Loss: 5.2933\n",
      "Epoch 1 Step 34/36 Loss: 6.0309\n",
      "Epoch 1 Step 35/36 Loss: 7.9790\n",
      "Epoch 1 Step 36/36 Loss: 4.8099\n",
      "Epoch 1 Step 1/36 Loss: 6.6667\n",
      "Epoch 1 Step 2/36 Loss: 5.2442\n",
      "Epoch 1 Step 3/36 Loss: 4.7953\n",
      "Epoch 1 Step 4/36 Loss: 5.5486\n",
      "Epoch 1 Step 5/36 Loss: 5.3851\n",
      "Epoch 1 Step 6/36 Loss: 5.1382\n",
      "Epoch 1 Step 7/36 Loss: 6.2195\n",
      "Epoch 1 Step 8/36 Loss: 5.0612\n",
      "Epoch 1 Step 9/36 Loss: 4.1314\n",
      "Epoch 1 Step 10/36 Loss: 5.1153\n",
      "Epoch 1 Step 11/36 Loss: 5.6424\n",
      "Epoch 1 Step 12/36 Loss: 5.6382\n",
      "Epoch 1 Step 13/36 Loss: 4.4950\n",
      "Epoch 1 Step 14/36 Loss: 5.6726\n",
      "Epoch 1 Step 15/36 Loss: 5.4590\n",
      "Epoch 1 Step 16/36 Loss: 6.1836\n",
      "Epoch 1 Step 17/36 Loss: 5.5752\n",
      "Epoch 1 Step 18/36 Loss: 6.7125\n",
      "Epoch 1 Step 19/36 Loss: 6.4853\n",
      "Epoch 1 Step 20/36 Loss: 4.9480\n",
      "Epoch 1 Step 21/36 Loss: 4.7238\n",
      "Epoch 1 Step 22/36 Loss: 5.7719\n",
      "Epoch 1 Step 23/36 Loss: 6.1868\n",
      "Epoch 1 Step 24/36 Loss: 5.8903\n",
      "Epoch 1 Step 25/36 Loss: 5.9873\n",
      "Epoch 1 Step 26/36 Loss: 4.1362\n",
      "Epoch 1 Step 27/36 Loss: 4.8954\n",
      "Epoch 1 Step 28/36 Loss: 5.5735\n",
      "Epoch 1 Step 29/36 Loss: 4.5924\n",
      "Epoch 1 Step 30/36 Loss: 5.2132\n",
      "Epoch 1 Step 31/36 Loss: 5.0328\n",
      "Epoch 1 Step 32/36 Loss: 4.1421\n",
      "Epoch 1 Step 33/36 Loss: 4.7960\n",
      "Epoch 1 Step 34/36 Loss: 5.7323\n",
      "Epoch 1 Step 35/36 Loss: 5.6606\n",
      "Epoch 1 Step 36/36 Loss: 4.5300\n",
      "Epoch 1 Step 1/36 Loss: 5.8275\n",
      "Epoch 1 Step 2/36 Loss: 7.0049\n",
      "Epoch 1 Step 3/36 Loss: 5.3693\n",
      "Epoch 1 Step 4/36 Loss: 6.7956\n",
      "Epoch 1 Step 5/36 Loss: 4.3757\n",
      "Epoch 1 Step 6/36 Loss: 4.9710\n",
      "Epoch 1 Step 7/36 Loss: 4.8819\n",
      "Epoch 1 Step 8/36 Loss: 5.5084\n",
      "Epoch 1 Step 9/36 Loss: 4.0060\n",
      "Epoch 1 Step 10/36 Loss: 6.2666\n",
      "Epoch 1 Step 11/36 Loss: 5.7199\n",
      "Epoch 1 Step 12/36 Loss: 4.8785\n",
      "Epoch 1 Step 13/36 Loss: 5.3965\n",
      "Epoch 1 Step 14/36 Loss: 5.6728\n",
      "Epoch 1 Step 15/36 Loss: 4.9719\n",
      "Epoch 1 Step 16/36 Loss: 5.5352\n",
      "Epoch 1 Step 17/36 Loss: 4.8558\n",
      "Epoch 1 Step 18/36 Loss: 5.4708\n",
      "Epoch 1 Step 19/36 Loss: 5.9683\n",
      "Epoch 1 Step 20/36 Loss: 6.2147\n",
      "Epoch 1 Step 21/36 Loss: 4.2623\n",
      "Epoch 1 Step 22/36 Loss: 4.7724\n",
      "Epoch 1 Step 23/36 Loss: 6.2132\n",
      "Epoch 1 Step 24/36 Loss: 6.0760\n",
      "Epoch 1 Step 25/36 Loss: 4.9768\n",
      "Epoch 1 Step 26/36 Loss: 4.5592\n",
      "Epoch 1 Step 27/36 Loss: 4.7696\n",
      "Epoch 1 Step 28/36 Loss: 4.8354\n",
      "Epoch 1 Step 29/36 Loss: 5.9861\n",
      "Epoch 1 Step 30/36 Loss: 4.8300\n",
      "Epoch 1 Step 31/36 Loss: 5.9063\n",
      "Epoch 1 Step 32/36 Loss: 6.3385\n",
      "Epoch 1 Step 33/36 Loss: 4.7624\n",
      "Epoch 1 Step 34/36 Loss: 4.7100\n",
      "Epoch 1 Step 35/36 Loss: 7.0848\n",
      "Epoch 1 Step 36/36 Loss: 5.5077\n",
      "Epoch 1 Step 1/36 Loss: 5.3323\n",
      "Epoch 1 Step 2/36 Loss: 5.1413\n",
      "Epoch 1 Step 3/36 Loss: 6.0591\n",
      "Epoch 1 Step 4/36 Loss: 4.5938\n",
      "Epoch 1 Step 5/36 Loss: 5.6481\n",
      "Epoch 1 Step 6/36 Loss: 6.0531\n",
      "Epoch 1 Step 7/36 Loss: 5.3810\n",
      "Epoch 1 Step 8/36 Loss: 4.7719\n",
      "Epoch 1 Step 9/36 Loss: 6.1363\n",
      "Epoch 1 Step 10/36 Loss: 6.0991\n",
      "Epoch 1 Step 11/36 Loss: 5.5358\n",
      "Epoch 1 Step 12/36 Loss: 6.3819\n",
      "Epoch 1 Step 13/36 Loss: 4.9496\n",
      "Epoch 1 Step 14/36 Loss: 4.6545\n",
      "Epoch 1 Step 15/36 Loss: 4.5190\n",
      "Epoch 1 Step 16/36 Loss: 4.9729\n",
      "Epoch 1 Step 17/36 Loss: 5.4870\n",
      "Epoch 1 Step 18/36 Loss: 5.7631\n",
      "Epoch 1 Step 19/36 Loss: 5.6952\n",
      "Epoch 1 Step 20/36 Loss: 4.9467\n",
      "Epoch 1 Step 21/36 Loss: 4.7643\n",
      "Epoch 1 Step 22/36 Loss: 4.2350\n",
      "Epoch 1 Step 23/36 Loss: 6.0172\n",
      "Epoch 1 Step 24/36 Loss: 5.6400\n",
      "Epoch 1 Step 25/36 Loss: 4.9582\n",
      "Epoch 1 Step 26/36 Loss: 4.4204\n",
      "Epoch 1 Step 27/36 Loss: 4.6139\n",
      "Epoch 1 Step 28/36 Loss: 4.7165\n",
      "Epoch 1 Step 29/36 Loss: 5.0429\n",
      "Epoch 1 Step 30/36 Loss: 5.5442\n",
      "Epoch 1 Step 31/36 Loss: 5.7539\n",
      "Epoch 1 Step 32/36 Loss: 4.1576\n",
      "Epoch 1 Step 33/36 Loss: 4.7042\n",
      "Epoch 1 Step 34/36 Loss: 5.1425\n",
      "Epoch 1 Step 35/36 Loss: 6.2662\n",
      "Epoch 1 Step 36/36 Loss: 5.5748\n",
      "Epoch 1 Step 1/36 Loss: 5.2894\n",
      "Epoch 1 Step 2/36 Loss: 5.2047\n",
      "Epoch 1 Step 3/36 Loss: 6.2419\n",
      "Epoch 1 Step 4/36 Loss: 4.9533\n",
      "Epoch 1 Step 5/36 Loss: 5.1661\n",
      "Epoch 1 Step 6/36 Loss: 4.7116\n",
      "Epoch 1 Step 7/36 Loss: 6.4161\n",
      "Epoch 1 Step 8/36 Loss: 4.6918\n",
      "Epoch 1 Step 9/36 Loss: 5.0290\n",
      "Epoch 1 Step 10/36 Loss: 6.4262\n",
      "Epoch 1 Step 11/36 Loss: 5.2978\n",
      "Epoch 1 Step 12/36 Loss: 6.6253\n",
      "Epoch 1 Step 13/36 Loss: 6.8328\n",
      "Epoch 1 Step 14/36 Loss: 4.1208\n",
      "Epoch 1 Step 15/36 Loss: 5.3574\n",
      "Epoch 1 Step 16/36 Loss: 5.1498\n",
      "Epoch 1 Step 17/36 Loss: 5.8386\n",
      "Epoch 1 Step 18/36 Loss: 5.2591\n",
      "Epoch 1 Step 19/36 Loss: 6.5138\n",
      "Epoch 1 Step 20/36 Loss: 5.5923\n",
      "Epoch 1 Step 21/36 Loss: 6.1389\n",
      "Epoch 1 Step 22/36 Loss: 4.4524\n",
      "Epoch 1 Step 23/36 Loss: 6.4480\n",
      "Epoch 1 Step 24/36 Loss: 4.4094\n",
      "Epoch 1 Step 25/36 Loss: 5.5506\n",
      "Epoch 1 Step 26/36 Loss: 6.8228\n",
      "Epoch 1 Step 27/36 Loss: 4.2332\n",
      "Epoch 1 Step 28/36 Loss: 6.1896\n",
      "Epoch 1 Step 29/36 Loss: 6.2666\n",
      "Epoch 1 Step 30/36 Loss: 7.7517\n",
      "Epoch 1 Step 31/36 Loss: 4.6998\n",
      "Epoch 1 Step 32/36 Loss: 4.1187\n",
      "Epoch 1 Step 33/36 Loss: 5.6009\n",
      "Epoch 1 Step 34/36 Loss: 5.1798\n",
      "Epoch 1 Step 35/36 Loss: 5.1593\n",
      "Epoch 1 Step 36/36 Loss: 6.0729\n",
      "Epoch 1 Step 1/36 Loss: 6.9669\n",
      "Epoch 1 Step 2/36 Loss: 5.0394\n",
      "Epoch 1 Step 3/36 Loss: 4.4885\n",
      "Epoch 1 Step 4/36 Loss: 5.7877\n",
      "Epoch 1 Step 5/36 Loss: 6.3306\n",
      "Epoch 1 Step 6/36 Loss: 6.6279\n",
      "Epoch 1 Step 7/36 Loss: 5.6974\n",
      "Epoch 1 Step 8/36 Loss: 5.7783\n",
      "Epoch 1 Step 9/36 Loss: 6.3049\n",
      "Epoch 1 Step 10/36 Loss: 4.2581\n",
      "Epoch 1 Step 11/36 Loss: 5.1540\n",
      "Epoch 1 Step 12/36 Loss: 4.3985\n",
      "Epoch 1 Step 13/36 Loss: 4.6162\n",
      "Epoch 1 Step 14/36 Loss: 5.7601\n",
      "Epoch 1 Step 15/36 Loss: 4.2937\n",
      "Epoch 1 Step 16/36 Loss: 4.3804\n",
      "Epoch 1 Step 17/36 Loss: 4.6290\n",
      "Epoch 1 Step 18/36 Loss: 5.8049\n",
      "Epoch 1 Step 19/36 Loss: 6.0544\n",
      "Epoch 1 Step 20/36 Loss: 5.3817\n",
      "Epoch 1 Step 21/36 Loss: 6.6379\n",
      "Epoch 1 Step 22/36 Loss: 5.8626\n",
      "Epoch 1 Step 23/36 Loss: 6.7050\n",
      "Epoch 1 Step 24/36 Loss: 5.9078\n",
      "Epoch 1 Step 25/36 Loss: 5.9250\n",
      "Epoch 1 Step 26/36 Loss: 5.2396\n",
      "Epoch 1 Step 27/36 Loss: 4.5602\n",
      "Epoch 1 Step 28/36 Loss: 5.0111\n",
      "Epoch 1 Step 29/36 Loss: 4.5169\n",
      "Epoch 1 Step 30/36 Loss: 4.4581\n",
      "Epoch 1 Step 31/36 Loss: 5.0160\n",
      "Epoch 1 Step 32/36 Loss: 5.9953\n",
      "Epoch 1 Step 33/36 Loss: 6.3334\n",
      "Epoch 1 Step 34/36 Loss: 5.9789\n",
      "Epoch 1 Step 35/36 Loss: 6.6735\n",
      "Epoch 1 Step 36/36 Loss: 5.5678\n",
      "Epoch 1 Step 1/36 Loss: 5.7650\n",
      "Epoch 1 Step 2/36 Loss: 5.2296\n",
      "Epoch 1 Step 3/36 Loss: 5.3843\n",
      "Epoch 1 Step 4/36 Loss: 4.4592\n",
      "Epoch 1 Step 5/36 Loss: 5.5693\n",
      "Epoch 1 Step 6/36 Loss: 6.0082\n",
      "Epoch 1 Step 7/36 Loss: 5.6019\n",
      "Epoch 1 Step 8/36 Loss: 5.0005\n",
      "Epoch 1 Step 9/36 Loss: 5.2015\n",
      "Epoch 1 Step 10/36 Loss: 4.7996\n",
      "Epoch 1 Step 11/36 Loss: 5.2616\n",
      "Epoch 1 Step 12/36 Loss: 4.8719\n",
      "Epoch 1 Step 13/36 Loss: 5.4093\n",
      "Epoch 1 Step 14/36 Loss: 6.1645\n",
      "Epoch 1 Step 15/36 Loss: 6.2993\n",
      "Epoch 1 Step 16/36 Loss: 5.5427\n",
      "Epoch 1 Step 17/36 Loss: 5.5852\n",
      "Epoch 1 Step 18/36 Loss: 5.1292\n",
      "Epoch 1 Step 19/36 Loss: 5.7145\n",
      "Epoch 1 Step 20/36 Loss: 5.4254\n",
      "Epoch 1 Step 21/36 Loss: 4.3008\n",
      "Epoch 1 Step 22/36 Loss: 4.8127\n",
      "Epoch 1 Step 23/36 Loss: 5.3590\n",
      "Epoch 1 Step 24/36 Loss: 5.3245\n",
      "Epoch 1 Step 25/36 Loss: 5.4666\n",
      "Epoch 1 Step 26/36 Loss: 5.6269\n",
      "Epoch 1 Step 27/36 Loss: 6.4538\n",
      "Epoch 1 Step 28/36 Loss: 5.3371\n",
      "Epoch 1 Step 29/36 Loss: 5.0187\n",
      "Epoch 1 Step 30/36 Loss: 4.9056\n",
      "Epoch 1 Step 31/36 Loss: 5.1855\n",
      "Epoch 1 Step 32/36 Loss: 5.5072\n",
      "Epoch 1 Step 33/36 Loss: 5.9640\n",
      "Epoch 1 Step 34/36 Loss: 5.4734\n",
      "Epoch 1 Step 35/36 Loss: 5.5131\n",
      "Epoch 1 Step 36/36 Loss: 5.5277\n",
      "Epoch 1 Step 1/36 Loss: 4.6412\n",
      "Epoch 1 Step 2/36 Loss: 5.4015\n",
      "Epoch 1 Step 3/36 Loss: 5.8281\n",
      "Epoch 1 Step 4/36 Loss: 6.0973\n",
      "Epoch 1 Step 5/36 Loss: 4.9479\n",
      "Epoch 1 Step 6/36 Loss: 5.5329\n",
      "Epoch 1 Step 7/36 Loss: 6.0228\n",
      "Epoch 1 Step 8/36 Loss: 6.1993\n",
      "Epoch 1 Step 9/36 Loss: 7.1323\n",
      "Epoch 1 Step 10/36 Loss: 4.7027\n",
      "Epoch 1 Step 11/36 Loss: 5.0372\n",
      "Epoch 1 Step 12/36 Loss: 6.4157\n",
      "Epoch 1 Step 13/36 Loss: 4.2444\n",
      "Epoch 1 Step 14/36 Loss: 5.9220\n",
      "Epoch 1 Step 15/36 Loss: 5.1175\n",
      "Epoch 1 Step 16/36 Loss: 4.9211\n",
      "Epoch 1 Step 17/36 Loss: 4.2570\n",
      "Epoch 1 Step 18/36 Loss: 4.2760\n",
      "Epoch 1 Step 19/36 Loss: 4.4002\n",
      "Epoch 1 Step 20/36 Loss: 5.1554\n",
      "Epoch 1 Step 21/36 Loss: 6.2075\n",
      "Epoch 1 Step 22/36 Loss: 5.8391\n",
      "Epoch 1 Step 23/36 Loss: 5.1536\n",
      "Epoch 1 Step 24/36 Loss: 6.4644\n",
      "Epoch 1 Step 25/36 Loss: 4.1742\n",
      "Epoch 1 Step 26/36 Loss: 5.9109\n",
      "Epoch 1 Step 27/36 Loss: 6.2731\n",
      "Epoch 1 Step 28/36 Loss: 5.6220\n",
      "Epoch 1 Step 29/36 Loss: 6.0776\n",
      "Epoch 1 Step 30/36 Loss: 6.0551\n",
      "Epoch 1 Step 31/36 Loss: 4.9676\n",
      "Epoch 1 Step 32/36 Loss: 4.8866\n",
      "Epoch 1 Step 33/36 Loss: 5.7204\n",
      "Epoch 1 Step 34/36 Loss: 6.7889\n",
      "Epoch 1 Step 35/36 Loss: 4.9547\n",
      "Epoch 1 Step 36/36 Loss: 4.2412\n",
      "Epoch 1 Step 1/36 Loss: 5.4338\n",
      "Epoch 1 Step 2/36 Loss: 6.1999\n",
      "Epoch 1 Step 3/36 Loss: 6.4087\n",
      "Epoch 1 Step 4/36 Loss: 5.6233\n",
      "Epoch 1 Step 5/36 Loss: 5.0994\n",
      "Epoch 1 Step 6/36 Loss: 5.5171\n",
      "Epoch 1 Step 7/36 Loss: 6.3432\n",
      "Epoch 1 Step 8/36 Loss: 5.0006\n",
      "Epoch 1 Step 9/36 Loss: 6.4856\n",
      "Epoch 1 Step 10/36 Loss: 4.7496\n",
      "Epoch 1 Step 11/36 Loss: 4.8014\n",
      "Epoch 1 Step 12/36 Loss: 5.0643\n",
      "Epoch 1 Step 13/36 Loss: 4.9969\n",
      "Epoch 1 Step 14/36 Loss: 5.1010\n",
      "Epoch 1 Step 15/36 Loss: 5.5180\n",
      "Epoch 1 Step 16/36 Loss: 5.6691\n",
      "Epoch 1 Step 17/36 Loss: 5.9757\n",
      "Epoch 1 Step 18/36 Loss: 5.1035\n",
      "Epoch 1 Step 19/36 Loss: 5.8668\n",
      "Epoch 1 Step 20/36 Loss: 6.6998\n",
      "Epoch 1 Step 21/36 Loss: 4.5078\n",
      "Epoch 1 Step 22/36 Loss: 6.2843\n",
      "Epoch 1 Step 23/36 Loss: 6.3341\n",
      "Epoch 1 Step 24/36 Loss: 6.1296\n",
      "Epoch 1 Step 25/36 Loss: 4.1957\n",
      "Epoch 1 Step 26/36 Loss: 6.2155\n",
      "Epoch 1 Step 27/36 Loss: 6.4839\n",
      "Epoch 1 Step 28/36 Loss: 4.1625\n",
      "Epoch 1 Step 29/36 Loss: 6.1340\n",
      "Epoch 1 Step 30/36 Loss: 7.2173\n",
      "Epoch 1 Step 31/36 Loss: 4.8150\n",
      "Epoch 1 Step 32/36 Loss: 5.9531\n",
      "Epoch 1 Step 33/36 Loss: 5.9261\n",
      "Epoch 1 Step 34/36 Loss: 5.2418\n",
      "Epoch 1 Step 35/36 Loss: 4.8841\n",
      "Epoch 1 Step 36/36 Loss: 6.2432\n",
      "Epoch 1 Step 1/36 Loss: 5.3102\n",
      "Epoch 1 Step 2/36 Loss: 5.7798\n",
      "Epoch 1 Step 3/36 Loss: 4.8399\n",
      "Epoch 1 Step 4/36 Loss: 4.9221\n",
      "Epoch 1 Step 5/36 Loss: 5.9002\n",
      "Epoch 1 Step 6/36 Loss: 6.5405\n",
      "Epoch 1 Step 7/36 Loss: 5.9983\n",
      "Epoch 1 Step 8/36 Loss: 5.3169\n",
      "Epoch 1 Step 9/36 Loss: 6.0910\n",
      "Epoch 1 Step 10/36 Loss: 5.9140\n",
      "Epoch 1 Step 11/36 Loss: 6.3637\n",
      "Epoch 1 Step 12/36 Loss: 3.7856\n",
      "Epoch 1 Step 13/36 Loss: 4.8915\n",
      "Epoch 1 Step 14/36 Loss: 6.3301\n",
      "Epoch 1 Step 15/36 Loss: 5.4028\n",
      "Epoch 1 Step 16/36 Loss: 5.2969\n",
      "Epoch 1 Step 17/36 Loss: 4.7112\n",
      "Epoch 1 Step 18/36 Loss: 6.3290\n",
      "Epoch 1 Step 19/36 Loss: 5.8962\n",
      "Epoch 1 Step 20/36 Loss: 5.7268\n",
      "Epoch 1 Step 21/36 Loss: 5.7706\n",
      "Epoch 1 Step 22/36 Loss: 5.1126\n",
      "Epoch 1 Step 23/36 Loss: 6.7010\n",
      "Epoch 1 Step 24/36 Loss: 4.8007\n",
      "Epoch 1 Step 25/36 Loss: 6.1736\n",
      "Epoch 1 Step 26/36 Loss: 4.4660\n",
      "Epoch 1 Step 27/36 Loss: 6.1213\n",
      "Epoch 1 Step 28/36 Loss: 5.2984\n",
      "Epoch 1 Step 29/36 Loss: 5.0230\n",
      "Epoch 1 Step 30/36 Loss: 5.2783\n",
      "Epoch 1 Step 31/36 Loss: 6.0775\n",
      "Epoch 1 Step 32/36 Loss: 5.8362\n",
      "Epoch 1 Step 33/36 Loss: 5.1253\n",
      "Epoch 1 Step 34/36 Loss: 5.9264\n",
      "Epoch 1 Step 35/36 Loss: 6.3579\n",
      "Epoch 1 Step 36/36 Loss: 5.8429\n",
      "Epoch 1 Step 1/36 Loss: 5.1764\n",
      "Epoch 1 Step 2/36 Loss: 6.7153\n",
      "Epoch 1 Step 3/36 Loss: 5.2024\n",
      "Epoch 1 Step 4/36 Loss: 5.9523\n",
      "Epoch 1 Step 5/36 Loss: 6.0888\n",
      "Epoch 1 Step 6/36 Loss: 5.9530\n",
      "Epoch 1 Step 7/36 Loss: 5.1852\n",
      "Epoch 1 Step 8/36 Loss: 4.4649\n",
      "Epoch 1 Step 9/36 Loss: 5.6418\n",
      "Epoch 1 Step 10/36 Loss: 5.1005\n",
      "Epoch 1 Step 11/36 Loss: 5.8606\n",
      "Epoch 1 Step 12/36 Loss: 4.7362\n",
      "Epoch 1 Step 13/36 Loss: 4.6678\n",
      "Epoch 1 Step 14/36 Loss: 5.6322\n",
      "Epoch 1 Step 15/36 Loss: 4.7077\n",
      "Epoch 1 Step 16/36 Loss: 6.0245\n",
      "Epoch 1 Step 17/36 Loss: 6.3547\n",
      "Epoch 1 Step 18/36 Loss: 5.5922\n",
      "Epoch 1 Step 19/36 Loss: 5.7040\n",
      "Epoch 1 Step 20/36 Loss: 6.2509\n",
      "Epoch 1 Step 21/36 Loss: 4.8721\n",
      "Epoch 1 Step 22/36 Loss: 4.9604\n",
      "Epoch 1 Step 23/36 Loss: 6.6257\n",
      "Epoch 1 Step 24/36 Loss: 5.7130\n",
      "Epoch 1 Step 25/36 Loss: 6.0997\n",
      "Epoch 1 Step 26/36 Loss: 6.0090\n",
      "Epoch 1 Step 27/36 Loss: 5.8500\n",
      "Epoch 1 Step 28/36 Loss: 5.3180\n",
      "Epoch 1 Step 29/36 Loss: 4.9187\n",
      "Epoch 1 Step 30/36 Loss: 5.7232\n",
      "Epoch 1 Step 31/36 Loss: 4.2061\n",
      "Epoch 1 Step 32/36 Loss: 6.7474\n",
      "Epoch 1 Step 33/36 Loss: 5.9795\n",
      "Epoch 1 Step 34/36 Loss: 4.6187\n",
      "Epoch 1 Step 35/36 Loss: 5.8494\n",
      "Epoch 1 Step 36/36 Loss: 6.6862\n",
      "Epoch 1 Step 1/36 Loss: 5.3047\n",
      "Epoch 1 Step 2/36 Loss: 5.0532\n",
      "Epoch 1 Step 3/36 Loss: 4.6420\n",
      "Epoch 1 Step 4/36 Loss: 5.6970\n",
      "Epoch 1 Step 5/36 Loss: 5.6299\n",
      "Epoch 1 Step 6/36 Loss: 4.8560\n",
      "Epoch 1 Step 7/36 Loss: 6.3293\n",
      "Epoch 1 Step 8/36 Loss: 4.7127\n",
      "Epoch 1 Step 9/36 Loss: 4.6470\n",
      "Epoch 1 Step 10/36 Loss: 6.2670\n",
      "Epoch 1 Step 11/36 Loss: 5.7193\n",
      "Epoch 1 Step 12/36 Loss: 6.5571\n",
      "Epoch 1 Step 13/36 Loss: 5.5469\n",
      "Epoch 1 Step 14/36 Loss: 6.1737\n",
      "Epoch 1 Step 15/36 Loss: 5.9523\n",
      "Epoch 1 Step 16/36 Loss: 5.7661\n",
      "Epoch 1 Step 17/36 Loss: 5.0167\n",
      "Epoch 1 Step 18/36 Loss: 7.1061\n",
      "Epoch 1 Step 19/36 Loss: 5.2035\n",
      "Epoch 1 Step 20/36 Loss: 4.6248\n",
      "Epoch 1 Step 21/36 Loss: 5.9812\n",
      "Epoch 1 Step 22/36 Loss: 4.9666\n",
      "Epoch 1 Step 23/36 Loss: 5.6593\n",
      "Epoch 1 Step 24/36 Loss: 5.1949\n",
      "Epoch 1 Step 25/36 Loss: 4.7749\n",
      "Epoch 1 Step 26/36 Loss: 5.2484\n",
      "Epoch 1 Step 27/36 Loss: 6.2339\n",
      "Epoch 1 Step 28/36 Loss: 7.0141\n",
      "Epoch 1 Step 29/36 Loss: 6.5235\n",
      "Epoch 1 Step 30/36 Loss: 5.8443\n",
      "Epoch 1 Step 31/36 Loss: 5.1867\n",
      "Epoch 1 Step 32/36 Loss: 6.3360\n",
      "Epoch 1 Step 33/36 Loss: 5.6267\n",
      "Epoch 1 Step 34/36 Loss: 4.9223\n",
      "Epoch 1 Step 35/36 Loss: 6.3902\n",
      "Epoch 1 Step 36/36 Loss: 6.5299\n",
      "Epoch 1 Step 1/36 Loss: 5.4860\n",
      "Epoch 1 Step 2/36 Loss: 6.9578\n",
      "Epoch 1 Step 3/36 Loss: 4.6820\n",
      "Epoch 1 Step 4/36 Loss: 5.8068\n",
      "Epoch 1 Step 5/36 Loss: 5.6822\n",
      "Epoch 1 Step 6/36 Loss: 5.4979\n",
      "Epoch 1 Step 7/36 Loss: 6.0366\n",
      "Epoch 1 Step 8/36 Loss: 5.2019\n",
      "Epoch 1 Step 9/36 Loss: 4.6537\n",
      "Epoch 1 Step 10/36 Loss: 5.2053\n",
      "Epoch 1 Step 11/36 Loss: 5.4988\n",
      "Epoch 1 Step 12/36 Loss: 5.5990\n",
      "Epoch 1 Step 13/36 Loss: 5.5374\n",
      "Epoch 1 Step 14/36 Loss: 3.8314\n",
      "Epoch 1 Step 15/36 Loss: 5.9381\n",
      "Epoch 1 Step 16/36 Loss: 5.2765\n",
      "Epoch 1 Step 17/36 Loss: 4.8398\n",
      "Epoch 1 Step 18/36 Loss: 5.2187\n",
      "Epoch 1 Step 19/36 Loss: 5.6362\n",
      "Epoch 1 Step 20/36 Loss: 5.7391\n",
      "Epoch 1 Step 21/36 Loss: 5.2164\n",
      "Epoch 1 Step 22/36 Loss: 5.7127\n",
      "Epoch 1 Step 23/36 Loss: 6.3992\n",
      "Epoch 1 Step 24/36 Loss: 5.8231\n",
      "Epoch 1 Step 25/36 Loss: 4.0724\n",
      "Epoch 1 Step 26/36 Loss: 5.9625\n",
      "Epoch 1 Step 27/36 Loss: 5.5929\n",
      "Epoch 1 Step 28/36 Loss: 6.3695\n",
      "Epoch 1 Step 29/36 Loss: 6.1914\n",
      "Epoch 1 Step 30/36 Loss: 4.2798\n",
      "Epoch 1 Step 31/36 Loss: 4.4258\n",
      "Epoch 1 Step 32/36 Loss: 3.9918\n",
      "Epoch 1 Step 33/36 Loss: 5.3025\n",
      "Epoch 1 Step 34/36 Loss: 5.7353\n",
      "Epoch 1 Step 35/36 Loss: 5.9401\n",
      "Epoch 1 Step 36/36 Loss: 5.5716\n",
      "Epoch 1 Step 1/36 Loss: 4.7575\n",
      "Epoch 1 Step 2/36 Loss: 5.7600\n",
      "Epoch 1 Step 3/36 Loss: 5.3499\n",
      "Epoch 1 Step 4/36 Loss: 4.6697\n",
      "Epoch 1 Step 5/36 Loss: 5.5858\n",
      "Epoch 1 Step 6/36 Loss: 4.6622\n",
      "Epoch 1 Step 7/36 Loss: 5.5076\n",
      "Epoch 1 Step 8/36 Loss: 5.2548\n",
      "Epoch 1 Step 9/36 Loss: 4.6422\n",
      "Epoch 1 Step 10/36 Loss: 6.2596\n",
      "Epoch 1 Step 11/36 Loss: 6.3427\n",
      "Epoch 1 Step 12/36 Loss: 4.4134\n",
      "Epoch 1 Step 13/36 Loss: 5.4263\n",
      "Epoch 1 Step 14/36 Loss: 6.5455\n",
      "Epoch 1 Step 15/36 Loss: 5.0392\n",
      "Epoch 1 Step 16/36 Loss: 4.5621\n",
      "Epoch 1 Step 17/36 Loss: 5.1380\n",
      "Epoch 1 Step 18/36 Loss: 6.3396\n",
      "Epoch 1 Step 19/36 Loss: 4.5380\n",
      "Epoch 1 Step 20/36 Loss: 7.0227\n",
      "Epoch 1 Step 21/36 Loss: 5.4311\n",
      "Epoch 1 Step 22/36 Loss: 6.1667\n",
      "Epoch 1 Step 23/36 Loss: 5.9485\n",
      "Epoch 1 Step 24/36 Loss: 5.3059\n",
      "Epoch 1 Step 25/36 Loss: 6.0522\n",
      "Epoch 1 Step 26/36 Loss: 6.5552\n",
      "Epoch 1 Step 27/36 Loss: 5.3354\n",
      "Epoch 1 Step 28/36 Loss: 7.1589\n",
      "Epoch 1 Step 29/36 Loss: 6.6558\n",
      "Epoch 1 Step 30/36 Loss: 4.6201\n",
      "Epoch 1 Step 31/36 Loss: 4.2796\n",
      "Epoch 1 Step 32/36 Loss: 5.1599\n",
      "Epoch 1 Step 33/36 Loss: 4.6914\n",
      "Epoch 1 Step 34/36 Loss: 4.3482\n",
      "Epoch 1 Step 35/36 Loss: 6.7499\n",
      "Epoch 1 Step 36/36 Loss: 4.4767\n",
      "Epoch 1 Step 1/36 Loss: 6.2955\n",
      "Epoch 1 Step 2/36 Loss: 4.5270\n",
      "Epoch 1 Step 3/36 Loss: 4.7191\n",
      "Epoch 1 Step 4/36 Loss: 6.5523\n",
      "Epoch 1 Step 5/36 Loss: 4.8383\n",
      "Epoch 1 Step 6/36 Loss: 5.8131\n",
      "Epoch 1 Step 7/36 Loss: 4.5878\n",
      "Epoch 1 Step 8/36 Loss: 6.2579\n",
      "Epoch 1 Step 9/36 Loss: 5.6531\n",
      "Epoch 1 Step 10/36 Loss: 5.4504\n",
      "Epoch 1 Step 11/36 Loss: 6.8525\n",
      "Epoch 1 Step 12/36 Loss: 5.9931\n",
      "Epoch 1 Step 13/36 Loss: 5.8720\n",
      "Epoch 1 Step 14/36 Loss: 4.8591\n",
      "Epoch 1 Step 15/36 Loss: 3.7690\n",
      "Epoch 1 Step 16/36 Loss: 6.0324\n",
      "Epoch 1 Step 17/36 Loss: 5.5979\n",
      "Epoch 1 Step 18/36 Loss: 4.3431\n",
      "Epoch 1 Step 19/36 Loss: 5.3259\n",
      "Epoch 1 Step 20/36 Loss: 6.3073\n",
      "Epoch 1 Step 21/36 Loss: 4.7268\n",
      "Epoch 1 Step 22/36 Loss: 5.0423\n",
      "Epoch 1 Step 23/36 Loss: 5.0846\n",
      "Epoch 1 Step 24/36 Loss: 4.9659\n",
      "Epoch 1 Step 25/36 Loss: 5.0097\n",
      "Epoch 1 Step 26/36 Loss: 6.1061\n",
      "Epoch 1 Step 27/36 Loss: 5.5870\n",
      "Epoch 1 Step 28/36 Loss: 5.5574\n",
      "Epoch 1 Step 29/36 Loss: 6.0254\n",
      "Epoch 1 Step 30/36 Loss: 5.3881\n",
      "Epoch 1 Step 31/36 Loss: 4.7846\n",
      "Epoch 1 Step 32/36 Loss: 5.2682\n",
      "Epoch 1 Step 33/36 Loss: 4.8543\n",
      "Epoch 1 Step 34/36 Loss: 5.1971\n",
      "Epoch 1 Step 35/36 Loss: 4.8885\n",
      "Epoch 1 Step 36/36 Loss: 6.6017\n",
      "Epoch 1 Step 1/36 Loss: 5.7053\n",
      "Epoch 1 Step 2/36 Loss: 4.9815\n",
      "Epoch 1 Step 3/36 Loss: 5.2445\n",
      "Epoch 1 Step 4/36 Loss: 5.0471\n",
      "Epoch 1 Step 5/36 Loss: 5.9853\n",
      "Epoch 1 Step 6/36 Loss: 5.0375\n",
      "Epoch 1 Step 7/36 Loss: 5.1309\n",
      "Epoch 1 Step 8/36 Loss: 6.6596\n",
      "Epoch 1 Step 9/36 Loss: 4.3589\n",
      "Epoch 1 Step 10/36 Loss: 4.8589\n",
      "Epoch 1 Step 11/36 Loss: 5.6066\n",
      "Epoch 1 Step 12/36 Loss: 4.5049\n",
      "Epoch 1 Step 13/36 Loss: 5.3529\n",
      "Epoch 1 Step 14/36 Loss: 4.5891\n",
      "Epoch 1 Step 15/36 Loss: 6.2983\n",
      "Epoch 1 Step 16/36 Loss: 7.4715\n",
      "Epoch 1 Step 17/36 Loss: 5.2941\n",
      "Epoch 1 Step 18/36 Loss: 5.0307\n",
      "Epoch 1 Step 19/36 Loss: 6.0833\n",
      "Epoch 1 Step 20/36 Loss: 4.1435\n",
      "Epoch 1 Step 21/36 Loss: 5.8841\n",
      "Epoch 1 Step 22/36 Loss: 5.3388\n",
      "Epoch 1 Step 23/36 Loss: 5.4817\n",
      "Epoch 1 Step 24/36 Loss: 4.6401\n",
      "Epoch 1 Step 25/36 Loss: 5.0921\n",
      "Epoch 1 Step 26/36 Loss: 4.8406\n",
      "Epoch 1 Step 27/36 Loss: 5.4518\n",
      "Epoch 1 Step 28/36 Loss: 5.6057\n",
      "Epoch 1 Step 29/36 Loss: 4.6800\n",
      "Epoch 1 Step 30/36 Loss: 4.9431\n",
      "Epoch 1 Step 31/36 Loss: 6.0736\n",
      "Epoch 1 Step 32/36 Loss: 4.5573\n",
      "Epoch 1 Step 33/36 Loss: 6.3112\n",
      "Epoch 1 Step 34/36 Loss: 4.7901\n",
      "Epoch 1 Step 35/36 Loss: 5.9072\n",
      "Epoch 1 Step 36/36 Loss: 5.4870\n",
      "Epoch 1 Step 1/36 Loss: 5.1234\n",
      "Epoch 1 Step 2/36 Loss: 4.9580\n",
      "Epoch 1 Step 3/36 Loss: 4.6159\n",
      "Epoch 1 Step 4/36 Loss: 5.7399\n",
      "Epoch 1 Step 5/36 Loss: 5.0114\n",
      "Epoch 1 Step 6/36 Loss: 6.2207\n",
      "Epoch 1 Step 7/36 Loss: 4.6173\n",
      "Epoch 1 Step 8/36 Loss: 4.6532\n",
      "Epoch 1 Step 9/36 Loss: 5.2332\n",
      "Epoch 1 Step 10/36 Loss: 4.3242\n",
      "Epoch 1 Step 11/36 Loss: 6.0236\n",
      "Epoch 1 Step 12/36 Loss: 5.5535\n",
      "Epoch 1 Step 13/36 Loss: 5.0860\n",
      "Epoch 1 Step 14/36 Loss: 5.3170\n",
      "Epoch 1 Step 15/36 Loss: 5.1685\n",
      "Epoch 1 Step 16/36 Loss: 5.0691\n",
      "Epoch 1 Step 17/36 Loss: 5.8746\n",
      "Epoch 1 Step 18/36 Loss: 5.4489\n",
      "Epoch 1 Step 19/36 Loss: 6.6014\n",
      "Epoch 1 Step 20/36 Loss: 5.0733\n",
      "Epoch 1 Step 21/36 Loss: 6.2746\n",
      "Epoch 1 Step 22/36 Loss: 6.1876\n",
      "Epoch 1 Step 23/36 Loss: 6.4141\n",
      "Epoch 1 Step 24/36 Loss: 6.1124\n",
      "Epoch 1 Step 25/36 Loss: 4.4966\n",
      "Epoch 1 Step 26/36 Loss: 6.2692\n",
      "Epoch 1 Step 27/36 Loss: 5.1197\n",
      "Epoch 1 Step 28/36 Loss: 5.5552\n",
      "Epoch 1 Step 29/36 Loss: 4.2883\n",
      "Epoch 1 Step 30/36 Loss: 4.3382\n",
      "Epoch 1 Step 31/36 Loss: 5.2074\n",
      "Epoch 1 Step 32/36 Loss: 6.1715\n",
      "Epoch 1 Step 33/36 Loss: 4.4295\n",
      "Epoch 1 Step 34/36 Loss: 6.5878\n",
      "Epoch 1 Step 35/36 Loss: 6.6487\n",
      "Epoch 1 Step 36/36 Loss: 5.8351\n",
      "Epoch 1 Step 1/36 Loss: 3.8347\n",
      "Epoch 1 Step 2/36 Loss: 6.1978\n",
      "Epoch 1 Step 3/36 Loss: 5.0302\n",
      "Epoch 1 Step 4/36 Loss: 5.6835\n",
      "Epoch 1 Step 5/36 Loss: 5.1702\n",
      "Epoch 1 Step 6/36 Loss: 4.6932\n",
      "Epoch 1 Step 7/36 Loss: 4.4143\n",
      "Epoch 1 Step 8/36 Loss: 4.9643\n",
      "Epoch 1 Step 9/36 Loss: 5.8685\n",
      "Epoch 1 Step 10/36 Loss: 5.3573\n",
      "Epoch 1 Step 11/36 Loss: 4.2697\n",
      "Epoch 1 Step 12/36 Loss: 4.9657\n",
      "Epoch 1 Step 13/36 Loss: 5.1885\n",
      "Epoch 1 Step 14/36 Loss: 5.4158\n",
      "Epoch 1 Step 15/36 Loss: 5.3217\n",
      "Epoch 1 Step 16/36 Loss: 5.1950\n",
      "Epoch 1 Step 17/36 Loss: 5.7863\n",
      "Epoch 1 Step 18/36 Loss: 5.8850\n",
      "Epoch 1 Step 19/36 Loss: 6.3287\n",
      "Epoch 1 Step 20/36 Loss: 6.1509\n",
      "Epoch 1 Step 21/36 Loss: 4.6377\n",
      "Epoch 1 Step 22/36 Loss: 6.4776\n",
      "Epoch 1 Step 23/36 Loss: 5.3162\n",
      "Epoch 1 Step 24/36 Loss: 5.7786\n",
      "Epoch 1 Step 25/36 Loss: 4.3944\n",
      "Epoch 1 Step 26/36 Loss: 5.5765\n",
      "Epoch 1 Step 27/36 Loss: 6.4934\n",
      "Epoch 1 Step 28/36 Loss: 4.9201\n",
      "Epoch 1 Step 29/36 Loss: 4.4945\n",
      "Epoch 1 Step 30/36 Loss: 5.5278\n",
      "Epoch 1 Step 31/36 Loss: 6.6130\n",
      "Epoch 1 Step 32/36 Loss: 6.7789\n",
      "Epoch 1 Step 33/36 Loss: 4.6893\n",
      "Epoch 1 Step 34/36 Loss: 5.4118\n",
      "Epoch 1 Step 35/36 Loss: 5.7355\n",
      "Epoch 1 Step 36/36 Loss: 4.9504\n",
      "Epoch 1 Step 1/36 Loss: 5.6291\n",
      "Epoch 1 Step 2/36 Loss: 5.6100\n",
      "Epoch 1 Step 3/36 Loss: 5.1678\n",
      "Epoch 1 Step 4/36 Loss: 5.5991\n",
      "Epoch 1 Step 5/36 Loss: 5.7036\n",
      "Epoch 1 Step 6/36 Loss: 5.6384\n",
      "Epoch 1 Step 7/36 Loss: 5.4996\n",
      "Epoch 1 Step 8/36 Loss: 5.0793\n",
      "Epoch 1 Step 9/36 Loss: 6.6542\n",
      "Epoch 1 Step 10/36 Loss: 4.4090\n",
      "Epoch 1 Step 11/36 Loss: 4.2600\n",
      "Epoch 1 Step 12/36 Loss: 4.7077\n",
      "Epoch 1 Step 13/36 Loss: 4.9586\n",
      "Epoch 1 Step 14/36 Loss: 5.7074\n",
      "Epoch 1 Step 15/36 Loss: 4.6554\n",
      "Epoch 1 Step 16/36 Loss: 7.4628\n",
      "Epoch 1 Step 17/36 Loss: 5.7040\n",
      "Epoch 1 Step 18/36 Loss: 5.5084\n",
      "Epoch 1 Step 19/36 Loss: 4.9945\n",
      "Epoch 1 Step 20/36 Loss: 7.3031\n",
      "Epoch 1 Step 21/36 Loss: 6.3416\n",
      "Epoch 1 Step 22/36 Loss: 4.8594\n",
      "Epoch 1 Step 23/36 Loss: 4.4180\n",
      "Epoch 1 Step 24/36 Loss: 5.5098\n",
      "Epoch 1 Step 25/36 Loss: 5.0235\n",
      "Epoch 1 Step 26/36 Loss: 4.4222\n",
      "Epoch 1 Step 27/36 Loss: 5.5721\n",
      "Epoch 1 Step 28/36 Loss: 4.7605\n",
      "Epoch 1 Step 29/36 Loss: 4.7128\n",
      "Epoch 1 Step 30/36 Loss: 5.9625\n",
      "Epoch 1 Step 31/36 Loss: 5.9314\n",
      "Epoch 1 Step 32/36 Loss: 5.7547\n",
      "Epoch 1 Step 33/36 Loss: 6.1010\n",
      "Epoch 1 Step 34/36 Loss: 5.3391\n",
      "Epoch 1 Step 35/36 Loss: 4.9060\n",
      "Epoch 1 Step 36/36 Loss: 6.8457\n",
      "Epoch 1 Step 1/36 Loss: 4.8807\n",
      "Epoch 1 Step 2/36 Loss: 6.2871\n",
      "Epoch 1 Step 3/36 Loss: 6.2464\n",
      "Epoch 1 Step 4/36 Loss: 5.2687\n",
      "Epoch 1 Step 5/36 Loss: 6.1588\n",
      "Epoch 1 Step 6/36 Loss: 5.8991\n",
      "Epoch 1 Step 7/36 Loss: 5.5251\n",
      "Epoch 1 Step 8/36 Loss: 6.8151\n",
      "Epoch 1 Step 9/36 Loss: 6.5249\n",
      "Epoch 1 Step 10/36 Loss: 5.3306\n",
      "Epoch 1 Step 11/36 Loss: 5.9843\n",
      "Epoch 1 Step 12/36 Loss: 4.1363\n",
      "Epoch 1 Step 13/36 Loss: 5.2471\n",
      "Epoch 1 Step 14/36 Loss: 6.2846\n",
      "Epoch 1 Step 15/36 Loss: 5.0466\n",
      "Epoch 1 Step 16/36 Loss: 5.2227\n",
      "Epoch 1 Step 17/36 Loss: 6.2007\n",
      "Epoch 1 Step 18/36 Loss: 6.0201\n",
      "Epoch 1 Step 19/36 Loss: 5.8003\n",
      "Epoch 1 Step 20/36 Loss: 6.2220\n",
      "Epoch 1 Step 21/36 Loss: 4.9584\n",
      "Epoch 1 Step 22/36 Loss: 4.4532\n",
      "Epoch 1 Step 23/36 Loss: 5.7056\n",
      "Epoch 1 Step 24/36 Loss: 5.0314\n",
      "Epoch 1 Step 25/36 Loss: 4.6154\n",
      "Epoch 1 Step 26/36 Loss: 4.9015\n",
      "Epoch 1 Step 27/36 Loss: 6.4314\n",
      "Epoch 1 Step 28/36 Loss: 4.4057\n",
      "Epoch 1 Step 29/36 Loss: 5.9834\n",
      "Epoch 1 Step 30/36 Loss: 5.9304\n",
      "Epoch 1 Step 31/36 Loss: 6.0199\n",
      "Epoch 1 Step 32/36 Loss: 5.4335\n",
      "Epoch 1 Step 33/36 Loss: 5.1235\n",
      "Epoch 1 Step 34/36 Loss: 6.5804\n",
      "Epoch 1 Step 35/36 Loss: 6.1524\n",
      "Epoch 1 Step 36/36 Loss: 5.9703\n",
      "Epoch 1 Step 1/36 Loss: 5.4928\n",
      "Epoch 1 Step 2/36 Loss: 7.0327\n",
      "Epoch 1 Step 3/36 Loss: 4.8361\n",
      "Epoch 1 Step 4/36 Loss: 5.5256\n",
      "Epoch 1 Step 5/36 Loss: 5.6370\n",
      "Epoch 1 Step 6/36 Loss: 5.1672\n",
      "Epoch 1 Step 7/36 Loss: 5.7499\n",
      "Epoch 1 Step 8/36 Loss: 5.0053\n",
      "Epoch 1 Step 9/36 Loss: 6.1838\n",
      "Epoch 1 Step 10/36 Loss: 4.3272\n",
      "Epoch 1 Step 11/36 Loss: 5.6002\n",
      "Epoch 1 Step 12/36 Loss: 5.3497\n",
      "Epoch 1 Step 13/36 Loss: 5.4539\n",
      "Epoch 1 Step 14/36 Loss: 4.7612\n",
      "Epoch 1 Step 15/36 Loss: 4.3636\n",
      "Epoch 1 Step 16/36 Loss: 5.6077\n",
      "Epoch 1 Step 17/36 Loss: 4.8698\n",
      "Epoch 1 Step 18/36 Loss: 5.6532\n",
      "Epoch 1 Step 19/36 Loss: 4.2957\n",
      "Epoch 1 Step 20/36 Loss: 5.3691\n",
      "Epoch 1 Step 21/36 Loss: 5.4090\n",
      "Epoch 1 Step 22/36 Loss: 5.3024\n",
      "Epoch 1 Step 23/36 Loss: 5.3128\n",
      "Epoch 1 Step 24/36 Loss: 5.4296\n",
      "Epoch 1 Step 25/36 Loss: 6.4095\n",
      "Epoch 1 Step 26/36 Loss: 5.5175\n",
      "Epoch 1 Step 27/36 Loss: 5.5673\n",
      "Epoch 1 Step 28/36 Loss: 6.2542\n",
      "Epoch 1 Step 29/36 Loss: 5.0202\n",
      "Epoch 1 Step 30/36 Loss: 3.9665\n",
      "Epoch 1 Step 31/36 Loss: 5.6119\n",
      "Epoch 1 Step 32/36 Loss: 5.6734\n",
      "Epoch 1 Step 33/36 Loss: 5.8407\n",
      "Epoch 1 Step 34/36 Loss: 5.3183\n",
      "Epoch 1 Step 35/36 Loss: 5.2305\n",
      "Epoch 1 Step 36/36 Loss: 5.5486\n",
      "Epoch 1 Step 1/36 Loss: 5.4040\n",
      "Epoch 1 Step 2/36 Loss: 4.2763\n",
      "Epoch 1 Step 3/36 Loss: 6.2303\n",
      "Epoch 1 Step 4/36 Loss: 5.7394\n",
      "Epoch 1 Step 5/36 Loss: 4.5695\n",
      "Epoch 1 Step 6/36 Loss: 4.6852\n",
      "Epoch 1 Step 7/36 Loss: 5.5415\n",
      "Epoch 1 Step 8/36 Loss: 5.4543\n",
      "Epoch 1 Step 9/36 Loss: 5.3840\n",
      "Epoch 1 Step 10/36 Loss: 5.6914\n",
      "Epoch 1 Step 11/36 Loss: 6.4917\n",
      "Epoch 1 Step 12/36 Loss: 5.1499\n",
      "Epoch 1 Step 13/36 Loss: 4.7816\n",
      "Epoch 1 Step 14/36 Loss: 4.9877\n",
      "Epoch 1 Step 15/36 Loss: 6.3692\n",
      "Epoch 1 Step 16/36 Loss: 6.1814\n",
      "Epoch 1 Step 17/36 Loss: 3.7224\n",
      "Epoch 1 Step 18/36 Loss: 4.9921\n",
      "Epoch 1 Step 19/36 Loss: 4.9409\n",
      "Epoch 1 Step 20/36 Loss: 4.8410\n",
      "Epoch 1 Step 21/36 Loss: 7.4145\n",
      "Epoch 1 Step 22/36 Loss: 4.9906\n",
      "Epoch 1 Step 23/36 Loss: 4.8377\n",
      "Epoch 1 Step 24/36 Loss: 4.2235\n",
      "Epoch 1 Step 25/36 Loss: 5.4363\n",
      "Epoch 1 Step 26/36 Loss: 4.7645\n",
      "Epoch 1 Step 27/36 Loss: 5.4952\n",
      "Epoch 1 Step 28/36 Loss: 5.2365\n",
      "Epoch 1 Step 29/36 Loss: 5.8988\n",
      "Epoch 1 Step 30/36 Loss: 5.3449\n",
      "Epoch 1 Step 31/36 Loss: 5.4477\n",
      "Epoch 1 Step 32/36 Loss: 5.0000\n",
      "Epoch 1 Step 33/36 Loss: 5.1383\n",
      "Epoch 1 Step 34/36 Loss: 5.4820\n",
      "Epoch 1 Step 35/36 Loss: 6.3130\n",
      "Epoch 1 Step 36/36 Loss: 5.9918\n",
      "Epoch 1 Step 1/36 Loss: 5.1049\n",
      "Epoch 1 Step 2/36 Loss: 5.7678\n",
      "Epoch 1 Step 3/36 Loss: 5.5702\n",
      "Epoch 1 Step 4/36 Loss: 4.0190\n",
      "Epoch 1 Step 5/36 Loss: 5.4481\n",
      "Epoch 1 Step 6/36 Loss: 6.6889\n",
      "Epoch 1 Step 7/36 Loss: 6.2258\n",
      "Epoch 1 Step 8/36 Loss: 6.9008\n",
      "Epoch 1 Step 9/36 Loss: 4.6175\n",
      "Epoch 1 Step 10/36 Loss: 5.9900\n",
      "Epoch 1 Step 11/36 Loss: 5.5668\n",
      "Epoch 1 Step 12/36 Loss: 6.1425\n",
      "Epoch 1 Step 13/36 Loss: 5.0092\n",
      "Epoch 1 Step 14/36 Loss: 4.8968\n",
      "Epoch 1 Step 15/36 Loss: 5.5528\n",
      "Epoch 1 Step 16/36 Loss: 4.0715\n",
      "Epoch 1 Step 17/36 Loss: 4.4869\n",
      "Epoch 1 Step 18/36 Loss: 5.1331\n",
      "Epoch 1 Step 19/36 Loss: 5.4710\n",
      "Epoch 1 Step 20/36 Loss: 7.1172\n",
      "Epoch 1 Step 21/36 Loss: 4.9553\n",
      "Epoch 1 Step 22/36 Loss: 5.3569\n",
      "Epoch 1 Step 23/36 Loss: 6.0024\n",
      "Epoch 1 Step 24/36 Loss: 4.7855\n",
      "Epoch 1 Step 25/36 Loss: 5.4348\n",
      "Epoch 1 Step 26/36 Loss: 7.3390\n",
      "Epoch 1 Step 27/36 Loss: 6.5092\n",
      "Epoch 1 Step 28/36 Loss: 5.2374\n",
      "Epoch 1 Step 29/36 Loss: 4.3615\n",
      "Epoch 1 Step 30/36 Loss: 6.1022\n",
      "Epoch 1 Step 31/36 Loss: 5.7520\n",
      "Epoch 1 Step 32/36 Loss: 5.4796\n",
      "Epoch 1 Step 33/36 Loss: 5.9861\n",
      "Epoch 1 Step 34/36 Loss: 4.8229\n",
      "Epoch 1 Step 35/36 Loss: 5.8748\n",
      "Epoch 1 Step 36/36 Loss: 5.9791\n",
      "Epoch 1 Step 1/36 Loss: 5.7300\n",
      "Epoch 1 Step 2/36 Loss: 5.3933\n",
      "Epoch 1 Step 3/36 Loss: 5.1475\n",
      "Epoch 1 Step 4/36 Loss: 4.7429\n",
      "Epoch 1 Step 5/36 Loss: 6.5302\n",
      "Epoch 1 Step 6/36 Loss: 4.1499\n",
      "Epoch 1 Step 7/36 Loss: 5.5166\n",
      "Epoch 1 Step 8/36 Loss: 5.2299\n",
      "Epoch 1 Step 9/36 Loss: 5.2121\n",
      "Epoch 1 Step 10/36 Loss: 4.7736\n",
      "Epoch 1 Step 11/36 Loss: 4.6984\n",
      "Epoch 1 Step 12/36 Loss: 6.1427\n",
      "Epoch 1 Step 13/36 Loss: 6.2834\n",
      "Epoch 1 Step 14/36 Loss: 7.1523\n",
      "Epoch 1 Step 15/36 Loss: 3.7919\n",
      "Epoch 1 Step 16/36 Loss: 7.2724\n",
      "Epoch 1 Step 17/36 Loss: 7.4899\n",
      "Epoch 1 Step 18/36 Loss: 7.0458\n",
      "Epoch 1 Step 19/36 Loss: 5.0620\n",
      "Epoch 1 Step 20/36 Loss: 6.0934\n",
      "Epoch 1 Step 21/36 Loss: 5.1307\n",
      "Epoch 1 Step 22/36 Loss: 7.2975\n",
      "Epoch 1 Step 23/36 Loss: 6.8827\n",
      "Epoch 1 Step 24/36 Loss: 6.4537\n",
      "Epoch 1 Step 25/36 Loss: 5.1975\n",
      "Epoch 1 Step 26/36 Loss: 5.0916\n",
      "Epoch 1 Step 27/36 Loss: 5.8091\n",
      "Epoch 1 Step 28/36 Loss: 4.3596\n",
      "Epoch 1 Step 29/36 Loss: 4.7825\n",
      "Epoch 1 Step 30/36 Loss: 5.2786\n",
      "Epoch 1 Step 31/36 Loss: 4.7695\n",
      "Epoch 1 Step 32/36 Loss: 4.6489\n",
      "Epoch 1 Step 33/36 Loss: 6.2838\n",
      "Epoch 1 Step 34/36 Loss: 5.0015\n",
      "Epoch 1 Step 35/36 Loss: 4.8631\n",
      "Epoch 1 Step 36/36 Loss: 4.1529\n",
      "Epoch 1 Step 1/36 Loss: 5.9816\n",
      "Epoch 1 Step 2/36 Loss: 4.5376\n",
      "Epoch 1 Step 3/36 Loss: 7.2095\n",
      "Epoch 1 Step 4/36 Loss: 5.3229\n",
      "Epoch 1 Step 5/36 Loss: 4.9507\n",
      "Epoch 1 Step 6/36 Loss: 6.0222\n",
      "Epoch 1 Step 7/36 Loss: 6.2186\n",
      "Epoch 1 Step 8/36 Loss: 6.5312\n",
      "Epoch 1 Step 9/36 Loss: 5.5628\n",
      "Epoch 1 Step 10/36 Loss: 6.3940\n",
      "Epoch 1 Step 11/36 Loss: 5.0772\n",
      "Epoch 1 Step 12/36 Loss: 5.4991\n",
      "Epoch 1 Step 13/36 Loss: 4.3081\n",
      "Epoch 1 Step 14/36 Loss: 5.9844\n",
      "Epoch 1 Step 15/36 Loss: 4.8354\n",
      "Epoch 1 Step 16/36 Loss: 5.0736\n",
      "Epoch 1 Step 17/36 Loss: 4.5222\n",
      "Epoch 1 Step 18/36 Loss: 5.3916\n",
      "Epoch 1 Step 19/36 Loss: 5.6948\n",
      "Epoch 1 Step 20/36 Loss: 5.6626\n",
      "Epoch 1 Step 21/36 Loss: 4.7345\n",
      "Epoch 1 Step 22/36 Loss: 4.5277\n",
      "Epoch 1 Step 23/36 Loss: 4.8283\n",
      "Epoch 1 Step 24/36 Loss: 5.1883\n",
      "Epoch 1 Step 25/36 Loss: 5.7109\n",
      "Epoch 1 Step 26/36 Loss: 6.0427\n",
      "Epoch 1 Step 27/36 Loss: 5.6746\n",
      "Epoch 1 Step 28/36 Loss: 4.5893\n",
      "Epoch 1 Step 29/36 Loss: 6.8150\n",
      "Epoch 1 Step 30/36 Loss: 5.7168\n",
      "Epoch 1 Step 31/36 Loss: 5.3149\n",
      "Epoch 1 Step 32/36 Loss: 4.7273\n",
      "Epoch 1 Step 33/36 Loss: 5.6551\n",
      "Epoch 1 Step 34/36 Loss: 6.5468\n",
      "Epoch 1 Step 35/36 Loss: 6.1292\n",
      "Epoch 1 Step 36/36 Loss: 4.8655\n",
      "Epoch 1 Step 1/36 Loss: 4.9960\n",
      "Epoch 1 Step 2/36 Loss: 5.4851\n",
      "Epoch 1 Step 3/36 Loss: 5.1172\n",
      "Epoch 1 Step 4/36 Loss: 5.3492\n",
      "Epoch 1 Step 5/36 Loss: 4.4171\n",
      "Epoch 1 Step 6/36 Loss: 6.0461\n",
      "Epoch 1 Step 7/36 Loss: 7.0685\n",
      "Epoch 1 Step 8/36 Loss: 6.0727\n",
      "Epoch 1 Step 9/36 Loss: 5.2878\n",
      "Epoch 1 Step 10/36 Loss: 5.7485\n",
      "Epoch 1 Step 11/36 Loss: 5.8775\n",
      "Epoch 1 Step 12/36 Loss: 6.2344\n",
      "Epoch 1 Step 13/36 Loss: 6.0325\n",
      "Epoch 1 Step 14/36 Loss: 4.6725\n",
      "Epoch 1 Step 15/36 Loss: 5.4908\n",
      "Epoch 1 Step 16/36 Loss: 4.2824\n",
      "Epoch 1 Step 17/36 Loss: 4.9113\n",
      "Epoch 1 Step 18/36 Loss: 5.4806\n",
      "Epoch 1 Step 19/36 Loss: 6.4110\n",
      "Epoch 1 Step 20/36 Loss: 5.5653\n",
      "Epoch 1 Step 21/36 Loss: 5.4641\n",
      "Epoch 1 Step 22/36 Loss: 4.6031\n",
      "Epoch 1 Step 23/36 Loss: 4.8953\n",
      "Epoch 1 Step 24/36 Loss: 6.0911\n",
      "Epoch 1 Step 25/36 Loss: 5.5685\n",
      "Epoch 1 Step 26/36 Loss: 5.9951\n",
      "Epoch 1 Step 27/36 Loss: 6.1611\n",
      "Epoch 1 Step 28/36 Loss: 4.9516\n",
      "Epoch 1 Step 29/36 Loss: 5.5481\n",
      "Epoch 1 Step 30/36 Loss: 5.2399\n",
      "Epoch 1 Step 31/36 Loss: 5.7842\n",
      "Epoch 1 Step 32/36 Loss: 5.4594\n",
      "Epoch 1 Step 33/36 Loss: 6.2997\n",
      "Epoch 1 Step 34/36 Loss: 5.8048\n",
      "Epoch 1 Step 35/36 Loss: 4.9634\n",
      "Epoch 1 Step 36/36 Loss: 3.7617\n",
      "Epoch 1 Step 1/36 Loss: 5.0691\n",
      "Epoch 1 Step 2/36 Loss: 6.2156\n",
      "Epoch 1 Step 3/36 Loss: 5.6848\n",
      "Epoch 1 Step 4/36 Loss: 5.1154\n",
      "Epoch 1 Step 5/36 Loss: 4.6314\n",
      "Epoch 1 Step 6/36 Loss: 5.0149\n",
      "Epoch 1 Step 7/36 Loss: 4.9377\n",
      "Epoch 1 Step 8/36 Loss: 5.0576\n",
      "Epoch 1 Step 9/36 Loss: 4.5926\n",
      "Epoch 1 Step 10/36 Loss: 5.6474\n",
      "Epoch 1 Step 11/36 Loss: 5.8453\n",
      "Epoch 1 Step 12/36 Loss: 6.7860\n",
      "Epoch 1 Step 13/36 Loss: 5.5942\n",
      "Epoch 1 Step 14/36 Loss: 4.7285\n",
      "Epoch 1 Step 15/36 Loss: 6.0166\n",
      "Epoch 1 Step 16/36 Loss: 6.4640\n",
      "Epoch 1 Step 17/36 Loss: 4.6072\n",
      "Epoch 1 Step 18/36 Loss: 6.0619\n",
      "Epoch 1 Step 19/36 Loss: 4.6777\n",
      "Epoch 1 Step 20/36 Loss: 6.4299\n",
      "Epoch 1 Step 21/36 Loss: 4.4086\n",
      "Epoch 1 Step 22/36 Loss: 5.4741\n",
      "Epoch 1 Step 23/36 Loss: 4.8402\n",
      "Epoch 1 Step 24/36 Loss: 5.7241\n",
      "Epoch 1 Step 25/36 Loss: 5.7302\n",
      "Epoch 1 Step 26/36 Loss: 4.8525\n",
      "Epoch 1 Step 27/36 Loss: 5.5606\n",
      "Epoch 1 Step 28/36 Loss: 5.5708\n",
      "Epoch 1 Step 29/36 Loss: 5.8087\n",
      "Epoch 1 Step 30/36 Loss: 6.0570\n",
      "Epoch 1 Step 31/36 Loss: 5.1379\n",
      "Epoch 1 Step 32/36 Loss: 6.5332\n",
      "Epoch 1 Step 33/36 Loss: 4.8474\n",
      "Epoch 1 Step 34/36 Loss: 5.9790\n",
      "Epoch 1 Step 35/36 Loss: 6.7447\n",
      "Epoch 1 Step 36/36 Loss: 5.1168\n",
      "Epoch 1 Step 1/36 Loss: 5.0420\n",
      "Epoch 1 Step 2/36 Loss: 6.5314\n",
      "Epoch 1 Step 3/36 Loss: 6.5447\n",
      "Epoch 1 Step 4/36 Loss: 4.6021\n",
      "Epoch 1 Step 5/36 Loss: 6.2179\n",
      "Epoch 1 Step 6/36 Loss: 5.6203\n",
      "Epoch 1 Step 7/36 Loss: 5.6993\n",
      "Epoch 1 Step 8/36 Loss: 5.3772\n",
      "Epoch 1 Step 9/36 Loss: 5.1494\n",
      "Epoch 1 Step 10/36 Loss: 6.6523\n",
      "Epoch 1 Step 11/36 Loss: 4.4074\n",
      "Epoch 1 Step 12/36 Loss: 5.3071\n",
      "Epoch 1 Step 13/36 Loss: 5.6167\n",
      "Epoch 1 Step 14/36 Loss: 6.0759\n",
      "Epoch 1 Step 15/36 Loss: 5.4990\n",
      "Epoch 1 Step 16/36 Loss: 5.7002\n",
      "Epoch 1 Step 17/36 Loss: 5.8720\n",
      "Epoch 1 Step 18/36 Loss: 4.5876\n",
      "Epoch 1 Step 19/36 Loss: 4.2653\n",
      "Epoch 1 Step 20/36 Loss: 6.4775\n",
      "Epoch 1 Step 21/36 Loss: 5.1515\n",
      "Epoch 1 Step 22/36 Loss: 5.2943\n",
      "Epoch 1 Step 23/36 Loss: 4.8917\n",
      "Epoch 1 Step 24/36 Loss: 4.4380\n",
      "Epoch 1 Step 25/36 Loss: 4.9710\n",
      "Epoch 1 Step 26/36 Loss: 5.8743\n",
      "Epoch 1 Step 27/36 Loss: 4.6871\n",
      "Epoch 1 Step 28/36 Loss: 5.8927\n",
      "Epoch 1 Step 29/36 Loss: 5.2013\n",
      "Epoch 1 Step 30/36 Loss: 5.7313\n",
      "Epoch 1 Step 31/36 Loss: 5.1820\n",
      "Epoch 1 Step 32/36 Loss: 6.7818\n",
      "Epoch 1 Step 33/36 Loss: 5.6267\n",
      "Epoch 1 Step 34/36 Loss: 5.2494\n",
      "Epoch 1 Step 35/36 Loss: 4.8775\n",
      "Epoch 1 Step 36/36 Loss: 5.9471\n",
      "Epoch 1 Step 1/36 Loss: 5.4676\n",
      "Epoch 1 Step 2/36 Loss: 4.8921\n",
      "Epoch 1 Step 3/36 Loss: 5.3423\n",
      "Epoch 1 Step 4/36 Loss: 5.2859\n",
      "Epoch 1 Step 5/36 Loss: 5.6190\n",
      "Epoch 1 Step 6/36 Loss: 4.6307\n",
      "Epoch 1 Step 7/36 Loss: 5.7489\n",
      "Epoch 1 Step 8/36 Loss: 5.6442\n",
      "Epoch 1 Step 9/36 Loss: 5.1133\n",
      "Epoch 1 Step 10/36 Loss: 4.7920\n",
      "Epoch 1 Step 11/36 Loss: 5.0409\n",
      "Epoch 1 Step 12/36 Loss: 5.9096\n",
      "Epoch 1 Step 13/36 Loss: 5.0473\n",
      "Epoch 1 Step 14/36 Loss: 4.8465\n",
      "Epoch 1 Step 15/36 Loss: 7.6475\n",
      "Epoch 1 Step 16/36 Loss: 4.6928\n",
      "Epoch 1 Step 17/36 Loss: 5.1547\n",
      "Epoch 1 Step 18/36 Loss: 4.4762\n",
      "Epoch 1 Step 19/36 Loss: 6.8643\n",
      "Epoch 1 Step 20/36 Loss: 4.0572\n",
      "Epoch 1 Step 21/36 Loss: 5.8746\n",
      "Epoch 1 Step 22/36 Loss: 4.7264\n",
      "Epoch 1 Step 23/36 Loss: 5.7737\n",
      "Epoch 1 Step 24/36 Loss: 6.7934\n",
      "Epoch 1 Step 25/36 Loss: 5.0113\n",
      "Epoch 1 Step 26/36 Loss: 5.5570\n",
      "Epoch 1 Step 27/36 Loss: 4.7983\n",
      "Epoch 1 Step 28/36 Loss: 6.1423\n",
      "Epoch 1 Step 29/36 Loss: 5.4875\n",
      "Epoch 1 Step 30/36 Loss: 5.3896\n",
      "Epoch 1 Step 31/36 Loss: 5.1648\n",
      "Epoch 1 Step 32/36 Loss: 6.5483\n",
      "Epoch 1 Step 33/36 Loss: 5.3722\n",
      "Epoch 1 Step 34/36 Loss: 6.2306\n",
      "Epoch 1 Step 35/36 Loss: 5.5425\n",
      "Epoch 1 Step 36/36 Loss: 4.3471\n",
      "Epoch 1 Step 1/36 Loss: 4.9161\n",
      "Epoch 1 Step 2/36 Loss: 5.2984\n",
      "Epoch 1 Step 3/36 Loss: 5.4147\n",
      "Epoch 1 Step 4/36 Loss: 6.3661\n",
      "Epoch 1 Step 5/36 Loss: 6.2944\n",
      "Epoch 1 Step 6/36 Loss: 5.9059\n",
      "Epoch 1 Step 7/36 Loss: 5.2591\n",
      "Epoch 1 Step 8/36 Loss: 5.2477\n",
      "Epoch 1 Step 9/36 Loss: 4.4912\n",
      "Epoch 1 Step 10/36 Loss: 5.6957\n",
      "Epoch 1 Step 11/36 Loss: 6.6930\n",
      "Epoch 1 Step 12/36 Loss: 5.6084\n",
      "Epoch 1 Step 13/36 Loss: 6.2831\n",
      "Epoch 1 Step 14/36 Loss: 5.4875\n",
      "Epoch 1 Step 15/36 Loss: 5.3816\n",
      "Epoch 1 Step 16/36 Loss: 4.9169\n",
      "Epoch 1 Step 17/36 Loss: 4.5223\n",
      "Epoch 1 Step 18/36 Loss: 4.7281\n",
      "Epoch 1 Step 19/36 Loss: 4.1875\n",
      "Epoch 1 Step 20/36 Loss: 4.4708\n",
      "Epoch 1 Step 21/36 Loss: 6.3734\n",
      "Epoch 1 Step 22/36 Loss: 4.3214\n",
      "Epoch 1 Step 23/36 Loss: 6.4061\n",
      "Epoch 1 Step 24/36 Loss: 4.9837\n",
      "Epoch 1 Step 25/36 Loss: 5.5446\n",
      "Epoch 1 Step 26/36 Loss: 4.1621\n",
      "Epoch 1 Step 27/36 Loss: 6.8249\n",
      "Epoch 1 Step 28/36 Loss: 5.1357\n",
      "Epoch 1 Step 29/36 Loss: 5.5647\n",
      "Epoch 1 Step 30/36 Loss: 6.3015\n",
      "Epoch 1 Step 31/36 Loss: 4.7451\n",
      "Epoch 1 Step 32/36 Loss: 5.0971\n",
      "Epoch 1 Step 33/36 Loss: 5.7736\n",
      "Epoch 1 Step 34/36 Loss: 4.7689\n",
      "Epoch 1 Step 35/36 Loss: 5.0019\n",
      "Epoch 1 Step 36/36 Loss: 4.8787\n",
      "Epoch 1 Step 1/36 Loss: 4.9642\n",
      "Epoch 1 Step 2/36 Loss: 4.7411\n",
      "Epoch 1 Step 3/36 Loss: 3.8286\n",
      "Epoch 1 Step 4/36 Loss: 5.6889\n",
      "Epoch 1 Step 5/36 Loss: 4.6251\n",
      "Epoch 1 Step 6/36 Loss: 5.0393\n",
      "Epoch 1 Step 7/36 Loss: 5.2692\n",
      "Epoch 1 Step 8/36 Loss: 7.1205\n",
      "Epoch 1 Step 9/36 Loss: 5.9238\n",
      "Epoch 1 Step 10/36 Loss: 5.3664\n",
      "Epoch 1 Step 11/36 Loss: 6.0129\n",
      "Epoch 1 Step 12/36 Loss: 5.7695\n",
      "Epoch 1 Step 13/36 Loss: 4.9736\n",
      "Epoch 1 Step 14/36 Loss: 5.4091\n",
      "Epoch 1 Step 15/36 Loss: 5.7078\n",
      "Epoch 1 Step 16/36 Loss: 4.2808\n",
      "Epoch 1 Step 17/36 Loss: 5.8981\n",
      "Epoch 1 Step 18/36 Loss: 5.0175\n",
      "Epoch 1 Step 19/36 Loss: 5.2045\n",
      "Epoch 1 Step 20/36 Loss: 5.2672\n",
      "Epoch 1 Step 21/36 Loss: 4.4217\n",
      "Epoch 1 Step 22/36 Loss: 6.5218\n",
      "Epoch 1 Step 23/36 Loss: 6.0687\n",
      "Epoch 1 Step 24/36 Loss: 5.3474\n",
      "Epoch 1 Step 25/36 Loss: 6.7033\n",
      "Epoch 1 Step 26/36 Loss: 3.8770\n",
      "Epoch 1 Step 27/36 Loss: 6.5566\n",
      "Epoch 1 Step 28/36 Loss: 5.2214\n",
      "Epoch 1 Step 29/36 Loss: 5.2474\n",
      "Epoch 1 Step 30/36 Loss: 5.3828\n",
      "Epoch 1 Step 31/36 Loss: 4.7491\n",
      "Epoch 1 Step 32/36 Loss: 6.5171\n",
      "Epoch 1 Step 33/36 Loss: 6.9445\n",
      "Epoch 1 Step 34/36 Loss: 5.5513\n",
      "Epoch 1 Step 35/36 Loss: 4.4318\n",
      "Epoch 1 Step 36/36 Loss: 4.7157\n",
      "Epoch 1 Step 1/36 Loss: 6.1968\n",
      "Epoch 1 Step 2/36 Loss: 5.2353\n",
      "Epoch 1 Step 3/36 Loss: 6.1180\n",
      "Epoch 1 Step 4/36 Loss: 6.2975\n",
      "Epoch 1 Step 5/36 Loss: 5.7903\n",
      "Epoch 1 Step 6/36 Loss: 4.9440\n",
      "Epoch 1 Step 7/36 Loss: 4.0499\n",
      "Epoch 1 Step 8/36 Loss: 4.9976\n",
      "Epoch 1 Step 9/36 Loss: 6.2138\n",
      "Epoch 1 Step 10/36 Loss: 4.3349\n",
      "Epoch 1 Step 11/36 Loss: 4.6169\n",
      "Epoch 1 Step 12/36 Loss: 5.9549\n",
      "Epoch 1 Step 13/36 Loss: 5.9578\n",
      "Epoch 1 Step 14/36 Loss: 5.7646\n",
      "Epoch 1 Step 15/36 Loss: 4.3064\n",
      "Epoch 1 Step 16/36 Loss: 4.2411\n",
      "Epoch 1 Step 17/36 Loss: 4.6745\n",
      "Epoch 1 Step 18/36 Loss: 4.4977\n",
      "Epoch 1 Step 19/36 Loss: 4.9004\n",
      "Epoch 1 Step 20/36 Loss: 6.1500\n",
      "Epoch 1 Step 21/36 Loss: 4.5903\n",
      "Epoch 1 Step 22/36 Loss: 5.1360\n",
      "Epoch 1 Step 23/36 Loss: 5.7337\n",
      "Epoch 1 Step 24/36 Loss: 5.2683\n",
      "Epoch 1 Step 25/36 Loss: 6.4687\n",
      "Epoch 1 Step 26/36 Loss: 5.3129\n",
      "Epoch 1 Step 27/36 Loss: 5.0029\n",
      "Epoch 1 Step 28/36 Loss: 6.0300\n",
      "Epoch 1 Step 29/36 Loss: 6.2788\n",
      "Epoch 1 Step 30/36 Loss: 4.6921\n",
      "Epoch 1 Step 31/36 Loss: 6.1922\n",
      "Epoch 1 Step 32/36 Loss: 6.3056\n",
      "Epoch 1 Step 33/36 Loss: 5.2246\n",
      "Epoch 1 Step 34/36 Loss: 5.3299\n",
      "Epoch 1 Step 35/36 Loss: 4.5931\n",
      "Epoch 1 Step 36/36 Loss: 5.7509\n",
      "Epoch 1 Step 1/36 Loss: 4.2418\n",
      "Epoch 1 Step 2/36 Loss: 6.4751\n",
      "Epoch 1 Step 3/36 Loss: 5.8169\n",
      "Epoch 1 Step 4/36 Loss: 4.4682\n",
      "Epoch 1 Step 5/36 Loss: 5.0461\n",
      "Epoch 1 Step 6/36 Loss: 5.4463\n",
      "Epoch 1 Step 7/36 Loss: 4.6135\n",
      "Epoch 1 Step 8/36 Loss: 5.6040\n",
      "Epoch 1 Step 9/36 Loss: 4.5286\n",
      "Epoch 1 Step 10/36 Loss: 6.3281\n",
      "Epoch 1 Step 11/36 Loss: 6.7531\n",
      "Epoch 1 Step 12/36 Loss: 6.0606\n",
      "Epoch 1 Step 13/36 Loss: 6.3904\n",
      "Epoch 1 Step 14/36 Loss: 5.9229\n",
      "Epoch 1 Step 15/36 Loss: 4.3098\n",
      "Epoch 1 Step 16/36 Loss: 4.2225\n",
      "Epoch 1 Step 17/36 Loss: 4.8870\n",
      "Epoch 1 Step 18/36 Loss: 5.8050\n",
      "Epoch 1 Step 19/36 Loss: 4.9842\n",
      "Epoch 1 Step 20/36 Loss: 6.3586\n",
      "Epoch 1 Step 21/36 Loss: 5.6376\n",
      "Epoch 1 Step 22/36 Loss: 5.9040\n",
      "Epoch 1 Step 23/36 Loss: 4.4096\n",
      "Epoch 1 Step 24/36 Loss: 5.5879\n",
      "Epoch 1 Step 25/36 Loss: 5.9239\n",
      "Epoch 1 Step 26/36 Loss: 6.1116\n",
      "Epoch 1 Step 27/36 Loss: 7.1273\n",
      "Epoch 1 Step 28/36 Loss: 6.5630\n",
      "Epoch 1 Step 29/36 Loss: 5.5238\n",
      "Epoch 1 Step 30/36 Loss: 6.2726\n",
      "Epoch 1 Step 31/36 Loss: 5.7659\n",
      "Epoch 1 Step 32/36 Loss: 5.1132\n",
      "Epoch 1 Step 33/36 Loss: 5.5828\n",
      "Epoch 1 Step 34/36 Loss: 4.9243\n",
      "Epoch 1 Step 35/36 Loss: 5.0149\n",
      "Epoch 1 Step 36/36 Loss: 4.9082\n",
      "Epoch 1 Step 1/36 Loss: 5.0592\n",
      "Epoch 1 Step 2/36 Loss: 5.8478\n",
      "Epoch 1 Step 3/36 Loss: 4.5897\n",
      "Epoch 1 Step 4/36 Loss: 4.6780\n",
      "Epoch 1 Step 5/36 Loss: 6.0920\n",
      "Epoch 1 Step 6/36 Loss: 4.5254\n",
      "Epoch 1 Step 7/36 Loss: 4.6788\n",
      "Epoch 1 Step 8/36 Loss: 5.5841\n",
      "Epoch 1 Step 9/36 Loss: 5.9308\n",
      "Epoch 1 Step 10/36 Loss: 5.9357\n",
      "Epoch 1 Step 11/36 Loss: 4.9335\n",
      "Epoch 1 Step 12/36 Loss: 4.8640\n",
      "Epoch 1 Step 13/36 Loss: 7.0709\n",
      "Epoch 1 Step 14/36 Loss: 6.9694\n",
      "Epoch 1 Step 15/36 Loss: 5.6839\n",
      "Epoch 1 Step 16/36 Loss: 5.8843\n",
      "Epoch 1 Step 17/36 Loss: 5.5666\n",
      "Epoch 1 Step 18/36 Loss: 5.3316\n",
      "Epoch 1 Step 19/36 Loss: 6.0860\n",
      "Epoch 1 Step 20/36 Loss: 5.3998\n",
      "Epoch 1 Step 21/36 Loss: 5.7566\n",
      "Epoch 1 Step 22/36 Loss: 4.9208\n",
      "Epoch 1 Step 23/36 Loss: 5.3678\n",
      "Epoch 1 Step 24/36 Loss: 5.5396\n",
      "Epoch 1 Step 25/36 Loss: 4.9845\n",
      "Epoch 1 Step 26/36 Loss: 4.6477\n",
      "Epoch 1 Step 27/36 Loss: 6.7102\n",
      "Epoch 1 Step 28/36 Loss: 5.6564\n",
      "Epoch 1 Step 29/36 Loss: 5.7409\n",
      "Epoch 1 Step 30/36 Loss: 4.8498\n",
      "Epoch 1 Step 31/36 Loss: 5.1470\n",
      "Epoch 1 Step 32/36 Loss: 5.7416\n",
      "Epoch 1 Step 33/36 Loss: 6.4362\n",
      "Epoch 1 Step 34/36 Loss: 5.7321\n",
      "Epoch 1 Step 35/36 Loss: 5.0121\n",
      "Epoch 1 Step 36/36 Loss: 4.5491\n",
      "Epoch 1 Step 1/36 Loss: 5.4234\n",
      "Epoch 1 Step 2/36 Loss: 4.6582\n",
      "Epoch 1 Step 3/36 Loss: 6.0404\n",
      "Epoch 1 Step 4/36 Loss: 4.8274\n",
      "Epoch 1 Step 5/36 Loss: 5.9273\n",
      "Epoch 1 Step 6/36 Loss: 5.8352\n",
      "Epoch 1 Step 7/36 Loss: 5.3971\n",
      "Epoch 1 Step 8/36 Loss: 5.1000\n",
      "Epoch 1 Step 9/36 Loss: 4.8601\n",
      "Epoch 1 Step 10/36 Loss: 5.1849\n",
      "Epoch 1 Step 11/36 Loss: 5.5058\n",
      "Epoch 1 Step 12/36 Loss: 4.8912\n",
      "Epoch 1 Step 13/36 Loss: 6.4228\n",
      "Epoch 1 Step 14/36 Loss: 5.2298\n",
      "Epoch 1 Step 15/36 Loss: 6.2837\n",
      "Epoch 1 Step 16/36 Loss: 4.8565\n",
      "Epoch 1 Step 17/36 Loss: 5.5943\n",
      "Epoch 1 Step 18/36 Loss: 5.1825\n",
      "Epoch 1 Step 19/36 Loss: 5.5647\n",
      "Epoch 1 Step 20/36 Loss: 4.7309\n",
      "Epoch 1 Step 21/36 Loss: 6.1524\n",
      "Epoch 1 Step 22/36 Loss: 5.2426\n",
      "Epoch 1 Step 23/36 Loss: 5.0403\n",
      "Epoch 1 Step 24/36 Loss: 5.7617\n",
      "Epoch 1 Step 25/36 Loss: 4.9262\n",
      "Epoch 1 Step 26/36 Loss: 5.0096\n",
      "Epoch 1 Step 27/36 Loss: 5.7710\n",
      "Epoch 1 Step 28/36 Loss: 6.6265\n",
      "Epoch 1 Step 29/36 Loss: 4.0713\n",
      "Epoch 1 Step 30/36 Loss: 5.5074\n",
      "Epoch 1 Step 31/36 Loss: 5.2871\n",
      "Epoch 1 Step 32/36 Loss: 5.4312\n",
      "Epoch 1 Step 33/36 Loss: 6.2449\n",
      "Epoch 1 Step 34/36 Loss: 5.4836\n",
      "Epoch 1 Step 35/36 Loss: 6.4119\n",
      "Epoch 1 Step 36/36 Loss: 4.7836\n",
      "Epoch 1 Step 1/36 Loss: 5.7168\n",
      "Epoch 1 Step 2/36 Loss: 4.9685\n",
      "Epoch 1 Step 3/36 Loss: 5.0302\n",
      "Epoch 1 Step 4/36 Loss: 4.5200\n",
      "Epoch 1 Step 5/36 Loss: 5.4159\n",
      "Epoch 1 Step 6/36 Loss: 6.2372\n",
      "Epoch 1 Step 7/36 Loss: 5.8314\n",
      "Epoch 1 Step 8/36 Loss: 4.7590\n",
      "Epoch 1 Step 9/36 Loss: 5.8080\n",
      "Epoch 1 Step 10/36 Loss: 4.9155\n",
      "Epoch 1 Step 11/36 Loss: 5.0224\n",
      "Epoch 1 Step 12/36 Loss: 5.9167\n",
      "Epoch 1 Step 13/36 Loss: 5.1823\n",
      "Epoch 1 Step 14/36 Loss: 5.7679\n",
      "Epoch 1 Step 15/36 Loss: 5.8463\n",
      "Epoch 1 Step 16/36 Loss: 4.8569\n",
      "Epoch 1 Step 17/36 Loss: 6.4047\n",
      "Epoch 1 Step 18/36 Loss: 6.1792\n",
      "Epoch 1 Step 19/36 Loss: 5.8590\n",
      "Epoch 1 Step 20/36 Loss: 5.4598\n",
      "Epoch 1 Step 21/36 Loss: 4.2476\n",
      "Epoch 1 Step 22/36 Loss: 6.9019\n",
      "Epoch 1 Step 23/36 Loss: 5.7770\n",
      "Epoch 1 Step 24/36 Loss: 5.2447\n",
      "Epoch 1 Step 25/36 Loss: 5.4904\n",
      "Epoch 1 Step 26/36 Loss: 5.2748\n",
      "Epoch 1 Step 27/36 Loss: 4.6510\n",
      "Epoch 1 Step 28/36 Loss: 5.5253\n",
      "Epoch 1 Step 29/36 Loss: 5.6353\n",
      "Epoch 1 Step 30/36 Loss: 5.3224\n",
      "Epoch 1 Step 31/36 Loss: 5.0046\n",
      "Epoch 1 Step 32/36 Loss: 4.5356\n",
      "Epoch 1 Step 33/36 Loss: 5.3946\n",
      "Epoch 1 Step 34/36 Loss: 6.8781\n",
      "Epoch 1 Step 35/36 Loss: 4.9435\n",
      "Epoch 1 Step 36/36 Loss: 6.2846\n",
      "Epoch 1 Step 1/36 Loss: 6.7791\n",
      "Epoch 1 Step 2/36 Loss: 5.8524\n",
      "Epoch 1 Step 3/36 Loss: 7.4814\n",
      "Epoch 1 Step 4/36 Loss: 4.1262\n",
      "Epoch 1 Step 5/36 Loss: 6.6498\n",
      "Epoch 1 Step 6/36 Loss: 6.4731\n",
      "Epoch 1 Step 7/36 Loss: 4.9109\n",
      "Epoch 1 Step 8/36 Loss: 4.7760\n",
      "Epoch 1 Step 9/36 Loss: 5.0749\n",
      "Epoch 1 Step 10/36 Loss: 5.2182\n",
      "Epoch 1 Step 11/36 Loss: 5.4289\n",
      "Epoch 1 Step 12/36 Loss: 5.4003\n",
      "Epoch 1 Step 13/36 Loss: 6.2097\n",
      "Epoch 1 Step 14/36 Loss: 4.8849\n",
      "Epoch 1 Step 15/36 Loss: 3.6544\n",
      "Epoch 1 Step 16/36 Loss: 5.3513\n",
      "Epoch 1 Step 17/36 Loss: 5.9904\n",
      "Epoch 1 Step 18/36 Loss: 5.4659\n",
      "Epoch 1 Step 19/36 Loss: 5.5416\n",
      "Epoch 1 Step 20/36 Loss: 6.5737\n",
      "Epoch 1 Step 21/36 Loss: 6.6362\n",
      "Epoch 1 Step 22/36 Loss: 4.7510\n",
      "Epoch 1 Step 23/36 Loss: 5.6502\n",
      "Epoch 1 Step 24/36 Loss: 4.1548\n",
      "Epoch 1 Step 25/36 Loss: 5.6310\n",
      "Epoch 1 Step 26/36 Loss: 5.1363\n",
      "Epoch 1 Step 27/36 Loss: 5.7715\n",
      "Epoch 1 Step 28/36 Loss: 5.3703\n",
      "Epoch 1 Step 29/36 Loss: 5.6717\n",
      "Epoch 1 Step 30/36 Loss: 5.1893\n",
      "Epoch 1 Step 31/36 Loss: 5.6382\n",
      "Epoch 1 Step 32/36 Loss: 5.5086\n",
      "Epoch 1 Step 33/36 Loss: 4.8373\n",
      "Epoch 1 Step 34/36 Loss: 5.6459\n",
      "Epoch 1 Step 35/36 Loss: 5.3429\n",
      "Epoch 1 Step 36/36 Loss: 5.5954\n",
      "Epoch 1 Step 1/36 Loss: 5.9334\n",
      "Epoch 1 Step 2/36 Loss: 4.2327\n",
      "Epoch 1 Step 3/36 Loss: 5.0957\n",
      "Epoch 1 Step 4/36 Loss: 5.6877\n",
      "Epoch 1 Step 5/36 Loss: 5.1796\n",
      "Epoch 1 Step 6/36 Loss: 6.1274\n",
      "Epoch 1 Step 7/36 Loss: 4.5706\n",
      "Epoch 1 Step 8/36 Loss: 5.7060\n",
      "Epoch 1 Step 9/36 Loss: 5.3247\n",
      "Epoch 1 Step 10/36 Loss: 5.0737\n",
      "Epoch 1 Step 11/36 Loss: 7.4410\n",
      "Epoch 1 Step 12/36 Loss: 4.9630\n",
      "Epoch 1 Step 13/36 Loss: 6.1915\n",
      "Epoch 1 Step 14/36 Loss: 5.0591\n",
      "Epoch 1 Step 15/36 Loss: 5.3299\n",
      "Epoch 1 Step 16/36 Loss: 5.7190\n",
      "Epoch 1 Step 17/36 Loss: 4.0748\n",
      "Epoch 1 Step 18/36 Loss: 5.5648\n",
      "Epoch 1 Step 19/36 Loss: 5.6793\n",
      "Epoch 1 Step 20/36 Loss: 4.5731\n",
      "Epoch 1 Step 21/36 Loss: 5.4958\n",
      "Epoch 1 Step 22/36 Loss: 6.3654\n",
      "Epoch 1 Step 23/36 Loss: 7.2566\n",
      "Epoch 1 Step 24/36 Loss: 5.9321\n",
      "Epoch 1 Step 25/36 Loss: 6.6190\n",
      "Epoch 1 Step 26/36 Loss: 6.4739\n",
      "Epoch 1 Step 27/36 Loss: 4.8733\n",
      "Epoch 1 Step 28/36 Loss: 4.5740\n",
      "Epoch 1 Step 29/36 Loss: 5.0352\n",
      "Epoch 1 Step 30/36 Loss: 5.1268\n",
      "Epoch 1 Step 31/36 Loss: 5.1709\n",
      "Epoch 1 Step 32/36 Loss: 5.3339\n",
      "Epoch 1 Step 33/36 Loss: 5.0368\n",
      "Epoch 1 Step 34/36 Loss: 6.4402\n",
      "Epoch 1 Step 35/36 Loss: 5.6052\n",
      "Epoch 1 Step 36/36 Loss: 5.4894\n",
      "Epoch 1 Step 1/36 Loss: 5.6842\n",
      "Epoch 1 Step 2/36 Loss: 4.9672\n",
      "Epoch 1 Step 3/36 Loss: 4.9497\n",
      "Epoch 1 Step 4/36 Loss: 5.5058\n",
      "Epoch 1 Step 5/36 Loss: 5.5192\n",
      "Epoch 1 Step 6/36 Loss: 6.0547\n",
      "Epoch 1 Step 7/36 Loss: 5.8648\n",
      "Epoch 1 Step 8/36 Loss: 5.3220\n",
      "Epoch 1 Step 9/36 Loss: 4.4200\n",
      "Epoch 1 Step 10/36 Loss: 6.0379\n",
      "Epoch 1 Step 11/36 Loss: 5.1245\n",
      "Epoch 1 Step 12/36 Loss: 5.9616\n",
      "Epoch 1 Step 13/36 Loss: 5.6662\n",
      "Epoch 1 Step 14/36 Loss: 5.8666\n",
      "Epoch 1 Step 15/36 Loss: 6.3177\n",
      "Epoch 1 Step 16/36 Loss: 5.8319\n",
      "Epoch 1 Step 17/36 Loss: 5.6199\n",
      "Epoch 1 Step 18/36 Loss: 5.3765\n",
      "Epoch 1 Step 19/36 Loss: 5.9660\n",
      "Epoch 1 Step 20/36 Loss: 5.9048\n",
      "Epoch 1 Step 21/36 Loss: 5.2258\n",
      "Epoch 1 Step 22/36 Loss: 5.6447\n",
      "Epoch 1 Step 23/36 Loss: 5.5473\n",
      "Epoch 1 Step 24/36 Loss: 5.5536\n",
      "Epoch 1 Step 25/36 Loss: 6.4282\n",
      "Epoch 1 Step 26/36 Loss: 4.4371\n",
      "Epoch 1 Step 27/36 Loss: 4.4426\n",
      "Epoch 1 Step 28/36 Loss: 5.7584\n",
      "Epoch 1 Step 29/36 Loss: 5.6929\n",
      "Epoch 1 Step 30/36 Loss: 5.0924\n",
      "Epoch 1 Step 31/36 Loss: 6.7851\n",
      "Epoch 1 Step 32/36 Loss: 6.7891\n",
      "Epoch 1 Step 33/36 Loss: 4.3640\n",
      "Epoch 1 Step 34/36 Loss: 4.7178\n",
      "Epoch 1 Step 35/36 Loss: 6.4274\n",
      "Epoch 1 Step 36/36 Loss: 5.1406\n",
      "Epoch 1 Step 1/36 Loss: 5.4125\n",
      "Epoch 1 Step 2/36 Loss: 5.2769\n",
      "Epoch 1 Step 3/36 Loss: 4.9381\n",
      "Epoch 1 Step 4/36 Loss: 4.6720\n",
      "Epoch 1 Step 5/36 Loss: 4.7859\n",
      "Epoch 1 Step 6/36 Loss: 4.6480\n",
      "Epoch 1 Step 7/36 Loss: 5.6701\n",
      "Epoch 1 Step 8/36 Loss: 5.3025\n",
      "Epoch 1 Step 9/36 Loss: 6.2129\n",
      "Epoch 1 Step 10/36 Loss: 5.0079\n",
      "Epoch 1 Step 11/36 Loss: 4.8585\n",
      "Epoch 1 Step 12/36 Loss: 4.7370\n",
      "Epoch 1 Step 13/36 Loss: 4.6143\n",
      "Epoch 1 Step 14/36 Loss: 5.4243\n",
      "Epoch 1 Step 15/36 Loss: 5.4037\n",
      "Epoch 1 Step 16/36 Loss: 5.4590\n",
      "Epoch 1 Step 17/36 Loss: 6.1063\n",
      "Epoch 1 Step 18/36 Loss: 6.6079\n",
      "Epoch 1 Step 19/36 Loss: 4.8604\n",
      "Epoch 1 Step 20/36 Loss: 5.7653\n",
      "Epoch 1 Step 21/36 Loss: 6.3830\n",
      "Epoch 1 Step 22/36 Loss: 5.3664\n",
      "Epoch 1 Step 23/36 Loss: 5.3645\n",
      "Epoch 1 Step 24/36 Loss: 5.1330\n",
      "Epoch 1 Step 25/36 Loss: 6.2814\n",
      "Epoch 1 Step 26/36 Loss: 4.9685\n",
      "Epoch 1 Step 27/36 Loss: 5.3439\n",
      "Epoch 1 Step 28/36 Loss: 5.9505\n",
      "Epoch 1 Step 29/36 Loss: 5.7789\n",
      "Epoch 1 Step 30/36 Loss: 4.9940\n",
      "Epoch 1 Step 31/36 Loss: 5.6321\n",
      "Epoch 1 Step 32/36 Loss: 5.7162\n",
      "Epoch 1 Step 33/36 Loss: 4.6699\n",
      "Epoch 1 Step 34/36 Loss: 5.9365\n",
      "Epoch 1 Step 35/36 Loss: 7.1063\n",
      "Epoch 1 Step 36/36 Loss: 5.4823\n",
      "Epoch 1 Step 1/36 Loss: 4.0102\n",
      "Epoch 1 Step 2/36 Loss: 3.8420\n",
      "Epoch 1 Step 3/36 Loss: 4.9023\n",
      "Epoch 1 Step 4/36 Loss: 5.6936\n",
      "Epoch 1 Step 5/36 Loss: 4.8680\n",
      "Epoch 1 Step 6/36 Loss: 5.3886\n",
      "Epoch 1 Step 7/36 Loss: 6.0018\n",
      "Epoch 1 Step 8/36 Loss: 5.5639\n",
      "Epoch 1 Step 9/36 Loss: 5.0241\n",
      "Epoch 1 Step 10/36 Loss: 5.1170\n",
      "Epoch 1 Step 11/36 Loss: 4.6980\n",
      "Epoch 1 Step 12/36 Loss: 7.2842\n",
      "Epoch 1 Step 13/36 Loss: 5.1534\n",
      "Epoch 1 Step 14/36 Loss: 5.8705\n",
      "Epoch 1 Step 15/36 Loss: 6.0778\n",
      "Epoch 1 Step 16/36 Loss: 5.5121\n",
      "Epoch 1 Step 17/36 Loss: 6.1075\n",
      "Epoch 1 Step 18/36 Loss: 5.3065\n",
      "Epoch 1 Step 19/36 Loss: 6.2340\n",
      "Epoch 1 Step 20/36 Loss: 5.1784\n",
      "Epoch 1 Step 21/36 Loss: 5.1533\n",
      "Epoch 1 Step 22/36 Loss: 4.3200\n",
      "Epoch 1 Step 23/36 Loss: 5.1349\n",
      "Epoch 1 Step 24/36 Loss: 5.5314\n",
      "Epoch 1 Step 25/36 Loss: 5.6098\n",
      "Epoch 1 Step 26/36 Loss: 5.6036\n",
      "Epoch 1 Step 27/36 Loss: 4.9665\n",
      "Epoch 1 Step 28/36 Loss: 7.7181\n",
      "Epoch 1 Step 29/36 Loss: 6.2342\n",
      "Epoch 1 Step 30/36 Loss: 5.3546\n",
      "Epoch 1 Step 31/36 Loss: 5.5462\n",
      "Epoch 1 Step 32/36 Loss: 5.5117\n",
      "Epoch 1 Step 33/36 Loss: 5.1479\n",
      "Epoch 1 Step 34/36 Loss: 6.0686\n",
      "Epoch 1 Step 35/36 Loss: 5.2541\n",
      "Epoch 1 Step 36/36 Loss: 6.4300\n",
      "Epoch 1 Step 1/36 Loss: 5.1951\n",
      "Epoch 1 Step 2/36 Loss: 5.5977\n",
      "Epoch 1 Step 3/36 Loss: 4.3474\n",
      "Epoch 1 Step 4/36 Loss: 5.4461\n",
      "Epoch 1 Step 5/36 Loss: 5.8246\n",
      "Epoch 1 Step 6/36 Loss: 4.3139\n",
      "Epoch 1 Step 7/36 Loss: 7.1485\n",
      "Epoch 1 Step 8/36 Loss: 4.8306\n",
      "Epoch 1 Step 9/36 Loss: 4.8286\n",
      "Epoch 1 Step 10/36 Loss: 4.8377\n",
      "Epoch 1 Step 11/36 Loss: 4.2181\n",
      "Epoch 1 Step 12/36 Loss: 5.9993\n",
      "Epoch 1 Step 13/36 Loss: 5.4191\n",
      "Epoch 1 Step 14/36 Loss: 5.4185\n",
      "Epoch 1 Step 15/36 Loss: 5.4125\n",
      "Epoch 1 Step 16/36 Loss: 4.2171\n",
      "Epoch 1 Step 17/36 Loss: 5.5273\n",
      "Epoch 1 Step 18/36 Loss: 6.3154\n",
      "Epoch 1 Step 19/36 Loss: 5.9797\n",
      "Epoch 1 Step 20/36 Loss: 4.9074\n",
      "Epoch 1 Step 21/36 Loss: 5.3934\n",
      "Epoch 1 Step 22/36 Loss: 5.1611\n",
      "Epoch 1 Step 23/36 Loss: 6.1534\n",
      "Epoch 1 Step 24/36 Loss: 5.6355\n",
      "Epoch 1 Step 25/36 Loss: 5.7856\n",
      "Epoch 1 Step 26/36 Loss: 6.3449\n",
      "Epoch 1 Step 27/36 Loss: 5.6300\n",
      "Epoch 1 Step 28/36 Loss: 5.2401\n",
      "Epoch 1 Step 29/36 Loss: 6.4601\n",
      "Epoch 1 Step 30/36 Loss: 4.1590\n",
      "Epoch 1 Step 31/36 Loss: 5.8387\n",
      "Epoch 1 Step 32/36 Loss: 5.4768\n",
      "Epoch 1 Step 33/36 Loss: 6.7419\n",
      "Epoch 1 Step 34/36 Loss: 4.5523\n",
      "Epoch 1 Step 35/36 Loss: 5.2325\n",
      "Epoch 1 Step 36/36 Loss: 5.7742\n",
      "Epoch 1 Step 1/36 Loss: 6.1726\n",
      "Epoch 1 Step 2/36 Loss: 4.7983\n",
      "Epoch 1 Step 3/36 Loss: 6.6451\n",
      "Epoch 1 Step 4/36 Loss: 4.7035\n",
      "Epoch 1 Step 5/36 Loss: 5.6228\n",
      "Epoch 1 Step 6/36 Loss: 6.0244\n",
      "Epoch 1 Step 7/36 Loss: 5.1176\n",
      "Epoch 1 Step 8/36 Loss: 5.8090\n",
      "Epoch 1 Step 9/36 Loss: 5.4351\n",
      "Epoch 1 Step 10/36 Loss: 5.5392\n",
      "Epoch 1 Step 11/36 Loss: 4.0778\n",
      "Epoch 1 Step 12/36 Loss: 5.7396\n",
      "Epoch 1 Step 13/36 Loss: 5.7601\n",
      "Epoch 1 Step 14/36 Loss: 5.7930\n",
      "Epoch 1 Step 15/36 Loss: 4.8364\n",
      "Epoch 1 Step 16/36 Loss: 5.6777\n",
      "Epoch 1 Step 17/36 Loss: 5.9590\n",
      "Epoch 1 Step 18/36 Loss: 6.3964\n",
      "Epoch 1 Step 19/36 Loss: 6.4652\n",
      "Epoch 1 Step 20/36 Loss: 5.5909\n",
      "Epoch 1 Step 21/36 Loss: 4.5449\n",
      "Epoch 1 Step 22/36 Loss: 5.0045\n",
      "Epoch 1 Step 23/36 Loss: 4.6426\n",
      "Epoch 1 Step 24/36 Loss: 5.5687\n",
      "Epoch 1 Step 25/36 Loss: 6.1502\n",
      "Epoch 1 Step 26/36 Loss: 5.8128\n",
      "Epoch 1 Step 27/36 Loss: 4.4037\n",
      "Epoch 1 Step 28/36 Loss: 5.9039\n",
      "Epoch 1 Step 29/36 Loss: 4.5845\n",
      "Epoch 1 Step 30/36 Loss: 6.2078\n",
      "Epoch 1 Step 31/36 Loss: 6.5968\n",
      "Epoch 1 Step 32/36 Loss: 5.0560\n",
      "Epoch 1 Step 33/36 Loss: 7.1252\n",
      "Epoch 1 Step 34/36 Loss: 6.2518\n",
      "Epoch 1 Step 35/36 Loss: 4.9599\n",
      "Epoch 1 Step 36/36 Loss: 5.7621\n",
      "Epoch 1 Step 1/36 Loss: 5.1731\n",
      "Epoch 1 Step 2/36 Loss: 4.5047\n",
      "Epoch 1 Step 3/36 Loss: 5.7299\n",
      "Epoch 1 Step 4/36 Loss: 5.8211\n",
      "Epoch 1 Step 5/36 Loss: 4.7557\n",
      "Epoch 1 Step 6/36 Loss: 5.8250\n",
      "Epoch 1 Step 7/36 Loss: 6.5676\n",
      "Epoch 1 Step 8/36 Loss: 5.2623\n",
      "Epoch 1 Step 9/36 Loss: 5.0921\n",
      "Epoch 1 Step 10/36 Loss: 4.8468\n",
      "Epoch 1 Step 11/36 Loss: 6.1227\n",
      "Epoch 1 Step 12/36 Loss: 4.5854\n",
      "Epoch 1 Step 13/36 Loss: 5.1830\n",
      "Epoch 1 Step 14/36 Loss: 5.0637\n",
      "Epoch 1 Step 15/36 Loss: 4.8151\n",
      "Epoch 1 Step 16/36 Loss: 5.0837\n",
      "Epoch 1 Step 17/36 Loss: 4.6634\n",
      "Epoch 1 Step 18/36 Loss: 4.8384\n",
      "Epoch 1 Step 19/36 Loss: 5.5893\n",
      "Epoch 1 Step 20/36 Loss: 4.5749\n",
      "Epoch 1 Step 21/36 Loss: 5.0520\n",
      "Epoch 1 Step 22/36 Loss: 5.1436\n",
      "Epoch 1 Step 23/36 Loss: 5.3897\n",
      "Epoch 1 Step 24/36 Loss: 5.4326\n",
      "Epoch 1 Step 25/36 Loss: 5.5990\n",
      "Epoch 1 Step 26/36 Loss: 6.1017\n",
      "Epoch 1 Step 27/36 Loss: 7.6599\n",
      "Epoch 1 Step 28/36 Loss: 5.0783\n",
      "Epoch 1 Step 29/36 Loss: 6.0901\n",
      "Epoch 1 Step 30/36 Loss: 5.9123\n",
      "Epoch 1 Step 31/36 Loss: 4.3056\n",
      "Epoch 1 Step 32/36 Loss: 6.9837\n",
      "Epoch 1 Step 33/36 Loss: 6.5681\n",
      "Epoch 1 Step 34/36 Loss: 4.3450\n",
      "Epoch 1 Step 35/36 Loss: 5.2756\n",
      "Epoch 1 Step 36/36 Loss: 6.9986\n",
      "Epoch 1 Step 1/36 Loss: 5.1239\n",
      "Epoch 1 Step 2/36 Loss: 5.0645\n",
      "Epoch 1 Step 3/36 Loss: 5.7693\n",
      "Epoch 1 Step 4/36 Loss: 4.9744\n",
      "Epoch 1 Step 5/36 Loss: 4.4633\n",
      "Epoch 1 Step 6/36 Loss: 5.6702\n",
      "Epoch 1 Step 7/36 Loss: 6.0793\n",
      "Epoch 1 Step 8/36 Loss: 5.4881\n",
      "Epoch 1 Step 9/36 Loss: 5.6385\n",
      "Epoch 1 Step 10/36 Loss: 4.7781\n",
      "Epoch 1 Step 11/36 Loss: 4.1868\n",
      "Epoch 1 Step 12/36 Loss: 5.5158\n",
      "Epoch 1 Step 13/36 Loss: 5.9963\n",
      "Epoch 1 Step 14/36 Loss: 5.6614\n",
      "Epoch 1 Step 15/36 Loss: 5.5856\n",
      "Epoch 1 Step 16/36 Loss: 6.0134\n",
      "Epoch 1 Step 17/36 Loss: 7.1320\n",
      "Epoch 1 Step 18/36 Loss: 4.7238\n",
      "Epoch 1 Step 19/36 Loss: 5.8994\n",
      "Epoch 1 Step 20/36 Loss: 4.9952\n",
      "Epoch 1 Step 21/36 Loss: 4.8145\n",
      "Epoch 1 Step 22/36 Loss: 5.2964\n",
      "Epoch 1 Step 23/36 Loss: 5.1854\n",
      "Epoch 1 Step 24/36 Loss: 4.8756\n",
      "Epoch 1 Step 25/36 Loss: 5.3162\n",
      "Epoch 1 Step 26/36 Loss: 6.2034\n",
      "Epoch 1 Step 27/36 Loss: 5.7683\n",
      "Epoch 1 Step 28/36 Loss: 6.0907\n",
      "Epoch 1 Step 29/36 Loss: 4.7115\n",
      "Epoch 1 Step 30/36 Loss: 6.2684\n",
      "Epoch 1 Step 31/36 Loss: 6.7013\n",
      "Epoch 1 Step 32/36 Loss: 3.8165\n",
      "Epoch 1 Step 33/36 Loss: 6.1023\n",
      "Epoch 1 Step 34/36 Loss: 4.6728\n",
      "Epoch 1 Step 35/36 Loss: 6.6062\n",
      "Epoch 1 Step 36/36 Loss: 6.0061\n",
      "Epoch 1 Step 1/36 Loss: 7.4429\n",
      "Epoch 1 Step 2/36 Loss: 5.1117\n",
      "Epoch 1 Step 3/36 Loss: 6.2458\n",
      "Epoch 1 Step 4/36 Loss: 6.0783\n",
      "Epoch 1 Step 5/36 Loss: 5.6830\n",
      "Epoch 1 Step 6/36 Loss: 5.9873\n",
      "Epoch 1 Step 7/36 Loss: 5.6703\n",
      "Epoch 1 Step 8/36 Loss: 5.5137\n",
      "Epoch 1 Step 9/36 Loss: 6.6602\n",
      "Epoch 1 Step 10/36 Loss: 5.2596\n",
      "Epoch 1 Step 11/36 Loss: 5.0173\n",
      "Epoch 1 Step 12/36 Loss: 5.8593\n",
      "Epoch 1 Step 13/36 Loss: 5.2927\n",
      "Epoch 1 Step 14/36 Loss: 4.0652\n",
      "Epoch 1 Step 15/36 Loss: 5.6613\n",
      "Epoch 1 Step 16/36 Loss: 6.1195\n",
      "Epoch 1 Step 17/36 Loss: 4.3157\n",
      "Epoch 1 Step 18/36 Loss: 5.7227\n",
      "Epoch 1 Step 19/36 Loss: 4.8810\n",
      "Epoch 1 Step 20/36 Loss: 6.8751\n",
      "Epoch 1 Step 21/36 Loss: 6.0322\n",
      "Epoch 1 Step 22/36 Loss: 5.2265\n",
      "Epoch 1 Step 23/36 Loss: 5.2718\n",
      "Epoch 1 Step 24/36 Loss: 5.5149\n",
      "Epoch 1 Step 25/36 Loss: 5.8853\n",
      "Epoch 1 Step 26/36 Loss: 5.5041\n",
      "Epoch 1 Step 27/36 Loss: 3.9299\n",
      "Epoch 1 Step 28/36 Loss: 4.5936\n",
      "Epoch 1 Step 29/36 Loss: 5.9873\n",
      "Epoch 1 Step 30/36 Loss: 5.2246\n",
      "Epoch 1 Step 31/36 Loss: 5.2087\n",
      "Epoch 1 Step 32/36 Loss: 5.4717\n",
      "Epoch 1 Step 33/36 Loss: 4.7620\n",
      "Epoch 1 Step 34/36 Loss: 5.8350\n",
      "Epoch 1 Step 35/36 Loss: 5.9827\n",
      "Epoch 1 Step 36/36 Loss: 5.0066\n",
      "Epoch 1 Step 1/36 Loss: 6.3301\n",
      "Epoch 1 Step 2/36 Loss: 5.6688\n",
      "Epoch 1 Step 3/36 Loss: 5.9045\n",
      "Epoch 1 Step 4/36 Loss: 5.4421\n",
      "Epoch 1 Step 5/36 Loss: 4.8502\n",
      "Epoch 1 Step 6/36 Loss: 5.3349\n",
      "Epoch 1 Step 7/36 Loss: 5.7276\n",
      "Epoch 1 Step 8/36 Loss: 5.3307\n",
      "Epoch 1 Step 9/36 Loss: 5.3502\n",
      "Epoch 1 Step 10/36 Loss: 6.5186\n",
      "Epoch 1 Step 11/36 Loss: 5.8115\n",
      "Epoch 1 Step 12/36 Loss: 5.2507\n",
      "Epoch 1 Step 13/36 Loss: 5.9014\n",
      "Epoch 1 Step 14/36 Loss: 5.9917\n",
      "Epoch 1 Step 15/36 Loss: 6.2714\n",
      "Epoch 1 Step 16/36 Loss: 4.3866\n",
      "Epoch 1 Step 17/36 Loss: 4.8913\n",
      "Epoch 1 Step 18/36 Loss: 6.9260\n",
      "Epoch 1 Step 19/36 Loss: 4.9159\n",
      "Epoch 1 Step 20/36 Loss: 6.2039\n",
      "Epoch 1 Step 21/36 Loss: 5.1252\n",
      "Epoch 1 Step 22/36 Loss: 5.3176\n",
      "Epoch 1 Step 23/36 Loss: 5.8798\n",
      "Epoch 1 Step 24/36 Loss: 4.4964\n",
      "Epoch 1 Step 25/36 Loss: 4.0309\n",
      "Epoch 1 Step 26/36 Loss: 5.2363\n",
      "Epoch 1 Step 27/36 Loss: 5.5298\n",
      "Epoch 1 Step 28/36 Loss: 4.5269\n",
      "Epoch 1 Step 29/36 Loss: 6.4955\n",
      "Epoch 1 Step 30/36 Loss: 5.6588\n",
      "Epoch 1 Step 31/36 Loss: 5.6569\n",
      "Epoch 1 Step 32/36 Loss: 6.5802\n",
      "Epoch 1 Step 33/36 Loss: 6.0946\n",
      "Epoch 1 Step 34/36 Loss: 5.1416\n",
      "Epoch 1 Step 35/36 Loss: 6.3984\n",
      "Epoch 1 Step 36/36 Loss: 4.8709\n",
      "Epoch 1 Step 1/36 Loss: 6.4204\n",
      "Epoch 1 Step 2/36 Loss: 7.0523\n",
      "Epoch 1 Step 3/36 Loss: 4.8308\n",
      "Epoch 1 Step 4/36 Loss: 5.9470\n",
      "Epoch 1 Step 5/36 Loss: 4.4867\n",
      "Epoch 1 Step 6/36 Loss: 4.8985\n",
      "Epoch 1 Step 7/36 Loss: 5.7791\n",
      "Epoch 1 Step 8/36 Loss: 5.3549\n",
      "Epoch 1 Step 9/36 Loss: 4.9437\n",
      "Epoch 1 Step 10/36 Loss: 6.2811\n",
      "Epoch 1 Step 11/36 Loss: 5.0814\n",
      "Epoch 1 Step 12/36 Loss: 4.5353\n",
      "Epoch 1 Step 13/36 Loss: 4.7943\n",
      "Epoch 1 Step 14/36 Loss: 5.0530\n",
      "Epoch 1 Step 15/36 Loss: 4.8121\n",
      "Epoch 1 Step 16/36 Loss: 5.3324\n",
      "Epoch 1 Step 17/36 Loss: 5.7519\n",
      "Epoch 1 Step 18/36 Loss: 5.1007\n",
      "Epoch 1 Step 19/36 Loss: 6.6395\n",
      "Epoch 1 Step 20/36 Loss: 5.2712\n",
      "Epoch 1 Step 21/36 Loss: 4.7974\n",
      "Epoch 1 Step 22/36 Loss: 5.2680\n",
      "Epoch 1 Step 23/36 Loss: 4.4684\n",
      "Epoch 1 Step 24/36 Loss: 4.4364\n",
      "Epoch 1 Step 25/36 Loss: 5.7012\n",
      "Epoch 1 Step 26/36 Loss: 5.2575\n",
      "Epoch 1 Step 27/36 Loss: 5.7554\n",
      "Epoch 1 Step 28/36 Loss: 4.3596\n",
      "Epoch 1 Step 29/36 Loss: 5.7084\n",
      "Epoch 1 Step 30/36 Loss: 5.1157\n",
      "Epoch 1 Step 31/36 Loss: 5.0711\n",
      "Epoch 1 Step 32/36 Loss: 6.9508\n",
      "Epoch 1 Step 33/36 Loss: 5.0655\n",
      "Epoch 1 Step 34/36 Loss: 5.8728\n",
      "Epoch 1 Step 35/36 Loss: 5.4114\n",
      "Epoch 1 Step 36/36 Loss: 5.4627\n",
      "Epoch 1 Step 1/36 Loss: 4.3795\n",
      "Epoch 1 Step 2/36 Loss: 6.8249\n",
      "Epoch 1 Step 3/36 Loss: 6.4645\n",
      "Epoch 1 Step 4/36 Loss: 5.0982\n",
      "Epoch 1 Step 5/36 Loss: 4.7553\n",
      "Epoch 1 Step 6/36 Loss: 5.7018\n",
      "Epoch 1 Step 7/36 Loss: 5.7791\n",
      "Epoch 1 Step 8/36 Loss: 5.4813\n",
      "Epoch 1 Step 9/36 Loss: 5.4764\n",
      "Epoch 1 Step 10/36 Loss: 4.9331\n",
      "Epoch 1 Step 11/36 Loss: 5.4789\n",
      "Epoch 1 Step 12/36 Loss: 5.7322\n",
      "Epoch 1 Step 13/36 Loss: 5.1634\n",
      "Epoch 1 Step 14/36 Loss: 5.6877\n",
      "Epoch 1 Step 15/36 Loss: 5.8838\n",
      "Epoch 1 Step 16/36 Loss: 5.2083\n",
      "Epoch 1 Step 17/36 Loss: 5.8717\n",
      "Epoch 1 Step 18/36 Loss: 6.1147\n",
      "Epoch 1 Step 19/36 Loss: 5.7978\n",
      "Epoch 1 Step 20/36 Loss: 6.3407\n",
      "Epoch 1 Step 21/36 Loss: 4.8868\n",
      "Epoch 1 Step 22/36 Loss: 5.8919\n",
      "Epoch 1 Step 23/36 Loss: 6.4538\n",
      "Epoch 1 Step 24/36 Loss: 4.9250\n",
      "Epoch 1 Step 25/36 Loss: 4.8993\n",
      "Epoch 1 Step 26/36 Loss: 5.6341\n",
      "Epoch 1 Step 27/36 Loss: 5.9221\n",
      "Epoch 1 Step 28/36 Loss: 5.9201\n",
      "Epoch 1 Step 29/36 Loss: 6.1654\n",
      "Epoch 1 Step 30/36 Loss: 4.2711\n",
      "Epoch 1 Step 31/36 Loss: 7.1404\n",
      "Epoch 1 Step 32/36 Loss: 5.6126\n",
      "Epoch 1 Step 33/36 Loss: 5.3106\n",
      "Epoch 1 Step 34/36 Loss: 4.8544\n",
      "Epoch 1 Step 35/36 Loss: 7.0608\n",
      "Epoch 1 Step 36/36 Loss: 5.9909\n",
      "Epoch 1 Step 1/36 Loss: 5.2798\n",
      "Epoch 1 Step 2/36 Loss: 4.0770\n",
      "Epoch 1 Step 3/36 Loss: 6.4321\n",
      "Epoch 1 Step 4/36 Loss: 6.0012\n",
      "Epoch 1 Step 5/36 Loss: 5.1133\n",
      "Epoch 1 Step 6/36 Loss: 5.7307\n",
      "Epoch 1 Step 7/36 Loss: 6.0933\n",
      "Epoch 1 Step 8/36 Loss: 6.3277\n",
      "Epoch 1 Step 9/36 Loss: 5.9190\n",
      "Epoch 1 Step 10/36 Loss: 6.1612\n",
      "Epoch 1 Step 11/36 Loss: 5.8358\n",
      "Epoch 1 Step 12/36 Loss: 5.6992\n",
      "Epoch 1 Step 13/36 Loss: 4.9259\n",
      "Epoch 1 Step 14/36 Loss: 4.6692\n",
      "Epoch 1 Step 15/36 Loss: 4.0169\n",
      "Epoch 1 Step 16/36 Loss: 5.1339\n",
      "Epoch 1 Step 17/36 Loss: 4.7121\n",
      "Epoch 1 Step 18/36 Loss: 4.3351\n",
      "Epoch 1 Step 19/36 Loss: 5.8072\n",
      "Epoch 1 Step 20/36 Loss: 4.9624\n",
      "Epoch 1 Step 21/36 Loss: 5.4837\n",
      "Epoch 1 Step 22/36 Loss: 5.4608\n",
      "Epoch 1 Step 23/36 Loss: 3.9961\n",
      "Epoch 1 Step 24/36 Loss: 6.0457\n",
      "Epoch 1 Step 25/36 Loss: 4.4263\n",
      "Epoch 1 Step 26/36 Loss: 5.2077\n",
      "Epoch 1 Step 27/36 Loss: 6.1987\n",
      "Epoch 1 Step 28/36 Loss: 5.6636\n",
      "Epoch 1 Step 29/36 Loss: 4.5566\n",
      "Epoch 1 Step 30/36 Loss: 6.3470\n",
      "Epoch 1 Step 31/36 Loss: 3.9202\n",
      "Epoch 1 Step 32/36 Loss: 7.0083\n",
      "Epoch 1 Step 33/36 Loss: 4.0442\n",
      "Epoch 1 Step 34/36 Loss: 5.4311\n",
      "Epoch 1 Step 35/36 Loss: 4.9350\n",
      "Epoch 1 Step 36/36 Loss: 5.3883\n",
      "Epoch 1 Step 1/36 Loss: 5.3209\n",
      "Epoch 1 Step 2/36 Loss: 6.3176\n",
      "Epoch 1 Step 3/36 Loss: 5.5717\n",
      "Epoch 1 Step 4/36 Loss: 4.6053\n",
      "Epoch 1 Step 5/36 Loss: 6.4195\n",
      "Epoch 1 Step 6/36 Loss: 5.6656\n",
      "Epoch 1 Step 7/36 Loss: 4.6902\n",
      "Epoch 1 Step 8/36 Loss: 6.8826\n",
      "Epoch 1 Step 9/36 Loss: 6.0236\n",
      "Epoch 1 Step 10/36 Loss: 5.7976\n",
      "Epoch 1 Step 11/36 Loss: 4.6782\n",
      "Epoch 1 Step 12/36 Loss: 5.3552\n",
      "Epoch 1 Step 13/36 Loss: 5.6483\n",
      "Epoch 1 Step 14/36 Loss: 5.4833\n",
      "Epoch 1 Step 15/36 Loss: 4.7928\n",
      "Epoch 1 Step 16/36 Loss: 5.6528\n",
      "Epoch 1 Step 17/36 Loss: 6.1737\n",
      "Epoch 1 Step 18/36 Loss: 5.5098\n",
      "Epoch 1 Step 19/36 Loss: 5.7586\n",
      "Epoch 1 Step 20/36 Loss: 6.3248\n",
      "Epoch 1 Step 21/36 Loss: 5.1792\n",
      "Epoch 1 Step 22/36 Loss: 6.0487\n",
      "Epoch 1 Step 23/36 Loss: 4.1519\n",
      "Epoch 1 Step 24/36 Loss: 6.9547\n",
      "Epoch 1 Step 25/36 Loss: 4.8696\n",
      "Epoch 1 Step 26/36 Loss: 5.1150\n",
      "Epoch 1 Step 27/36 Loss: 5.1069\n",
      "Epoch 1 Step 28/36 Loss: 5.6001\n",
      "Epoch 1 Step 29/36 Loss: 5.5502\n",
      "Epoch 1 Step 30/36 Loss: 5.6863\n",
      "Epoch 1 Step 31/36 Loss: 4.8667\n",
      "Epoch 1 Step 32/36 Loss: 5.0327\n",
      "Epoch 1 Step 33/36 Loss: 5.9694\n",
      "Epoch 1 Step 34/36 Loss: 3.9922\n",
      "Epoch 1 Step 35/36 Loss: 5.0588\n",
      "Epoch 1 Step 36/36 Loss: 6.8037\n",
      "Epoch 1 Step 1/36 Loss: 5.1100\n",
      "Epoch 1 Step 2/36 Loss: 6.7372\n",
      "Epoch 1 Step 3/36 Loss: 5.5740\n",
      "Epoch 1 Step 4/36 Loss: 5.3765\n",
      "Epoch 1 Step 5/36 Loss: 5.1013\n",
      "Epoch 1 Step 6/36 Loss: 5.8275\n",
      "Epoch 1 Step 7/36 Loss: 5.1783\n",
      "Epoch 1 Step 8/36 Loss: 4.8070\n",
      "Epoch 1 Step 9/36 Loss: 5.1844\n",
      "Epoch 1 Step 10/36 Loss: 5.2720\n",
      "Epoch 1 Step 11/36 Loss: 6.7313\n",
      "Epoch 1 Step 12/36 Loss: 4.9173\n",
      "Epoch 1 Step 13/36 Loss: 6.4841\n",
      "Epoch 1 Step 14/36 Loss: 5.2829\n",
      "Epoch 1 Step 15/36 Loss: 6.3316\n",
      "Epoch 1 Step 16/36 Loss: 4.2033\n",
      "Epoch 1 Step 17/36 Loss: 5.6807\n",
      "Epoch 1 Step 18/36 Loss: 5.5449\n",
      "Epoch 1 Step 19/36 Loss: 5.2613\n",
      "Epoch 1 Step 20/36 Loss: 4.6714\n",
      "Epoch 1 Step 21/36 Loss: 5.5013\n",
      "Epoch 1 Step 22/36 Loss: 6.0934\n",
      "Epoch 1 Step 23/36 Loss: 4.5981\n",
      "Epoch 1 Step 24/36 Loss: 6.5044\n",
      "Epoch 1 Step 25/36 Loss: 5.5781\n",
      "Epoch 1 Step 26/36 Loss: 5.3895\n",
      "Epoch 1 Step 27/36 Loss: 4.8939\n",
      "Epoch 1 Step 28/36 Loss: 6.3956\n",
      "Epoch 1 Step 29/36 Loss: 4.9890\n",
      "Epoch 1 Step 30/36 Loss: 5.4114\n",
      "Epoch 1 Step 31/36 Loss: 3.9895\n",
      "Epoch 1 Step 32/36 Loss: 5.4160\n",
      "Epoch 1 Step 33/36 Loss: 4.6142\n",
      "Epoch 1 Step 34/36 Loss: 5.2919\n",
      "Epoch 1 Step 35/36 Loss: 4.8781\n",
      "Epoch 1 Step 36/36 Loss: 6.1933\n",
      "Epoch 1 Step 1/36 Loss: 5.7904\n",
      "Epoch 1 Step 2/36 Loss: 5.4155\n",
      "Epoch 1 Step 3/36 Loss: 5.1006\n",
      "Epoch 1 Step 4/36 Loss: 6.8571\n",
      "Epoch 1 Step 5/36 Loss: 4.9063\n",
      "Epoch 1 Step 6/36 Loss: 4.7666\n",
      "Epoch 1 Step 7/36 Loss: 5.2695\n",
      "Epoch 1 Step 8/36 Loss: 5.8444\n",
      "Epoch 1 Step 9/36 Loss: 4.6223\n",
      "Epoch 1 Step 10/36 Loss: 5.9195\n",
      "Epoch 1 Step 11/36 Loss: 6.9735\n",
      "Epoch 1 Step 12/36 Loss: 7.3342\n",
      "Epoch 1 Step 13/36 Loss: 4.6139\n",
      "Epoch 1 Step 14/36 Loss: 5.4134\n",
      "Epoch 1 Step 15/36 Loss: 6.7576\n",
      "Epoch 1 Step 16/36 Loss: 5.9385\n",
      "Epoch 1 Step 17/36 Loss: 6.8903\n",
      "Epoch 1 Step 18/36 Loss: 5.6729\n",
      "Epoch 1 Step 19/36 Loss: 5.6727\n",
      "Epoch 1 Step 20/36 Loss: 4.1583\n",
      "Epoch 1 Step 21/36 Loss: 6.8064\n",
      "Epoch 1 Step 22/36 Loss: 6.6691\n",
      "Epoch 1 Step 23/36 Loss: 5.1283\n",
      "Epoch 1 Step 24/36 Loss: 4.7581\n",
      "Epoch 1 Step 25/36 Loss: 4.7442\n",
      "Epoch 1 Step 26/36 Loss: 6.2283\n",
      "Epoch 1 Step 27/36 Loss: 4.7664\n",
      "Epoch 1 Step 28/36 Loss: 4.6336\n",
      "Epoch 1 Step 29/36 Loss: 5.4354\n",
      "Epoch 1 Step 30/36 Loss: 5.8394\n",
      "Epoch 1 Step 31/36 Loss: 6.5262\n",
      "Epoch 1 Step 32/36 Loss: 4.0596\n",
      "Epoch 1 Step 33/36 Loss: 4.8181\n",
      "Epoch 1 Step 34/36 Loss: 5.0151\n",
      "Epoch 1 Step 35/36 Loss: 5.1815\n",
      "Epoch 1 Step 36/36 Loss: 6.4414\n",
      "Epoch 1 Step 1/36 Loss: 4.6215\n",
      "Epoch 1 Step 2/36 Loss: 5.1121\n",
      "Epoch 1 Step 3/36 Loss: 5.8858\n",
      "Epoch 1 Step 4/36 Loss: 4.6631\n",
      "Epoch 1 Step 5/36 Loss: 5.2911\n",
      "Epoch 1 Step 6/36 Loss: 4.6307\n",
      "Epoch 1 Step 7/36 Loss: 5.6147\n",
      "Epoch 1 Step 8/36 Loss: 4.7846\n",
      "Epoch 1 Step 9/36 Loss: 4.9337\n",
      "Epoch 1 Step 10/36 Loss: 5.7159\n",
      "Epoch 1 Step 11/36 Loss: 6.2364\n",
      "Epoch 1 Step 12/36 Loss: 4.3437\n",
      "Epoch 1 Step 13/36 Loss: 5.5657\n",
      "Epoch 1 Step 14/36 Loss: 5.5342\n",
      "Epoch 1 Step 15/36 Loss: 6.0711\n",
      "Epoch 1 Step 16/36 Loss: 5.8090\n",
      "Epoch 1 Step 17/36 Loss: 4.7563\n",
      "Epoch 1 Step 18/36 Loss: 6.2839\n",
      "Epoch 1 Step 19/36 Loss: 5.7850\n",
      "Epoch 1 Step 20/36 Loss: 5.1057\n",
      "Epoch 1 Step 21/36 Loss: 5.5573\n",
      "Epoch 1 Step 22/36 Loss: 5.2584\n",
      "Epoch 1 Step 23/36 Loss: 4.4205\n",
      "Epoch 1 Step 24/36 Loss: 5.6005\n",
      "Epoch 1 Step 25/36 Loss: 4.8703\n",
      "Epoch 1 Step 26/36 Loss: 5.4839\n",
      "Epoch 1 Step 27/36 Loss: 6.1847\n",
      "Epoch 1 Step 28/36 Loss: 5.4806\n",
      "Epoch 1 Step 29/36 Loss: 5.6246\n",
      "Epoch 1 Step 30/36 Loss: 7.2588\n",
      "Epoch 1 Step 31/36 Loss: 7.7694\n",
      "Epoch 1 Step 32/36 Loss: 4.7690\n",
      "Epoch 1 Step 33/36 Loss: 4.8918\n",
      "Epoch 1 Step 34/36 Loss: 4.5162\n",
      "Epoch 1 Step 35/36 Loss: 5.7502\n",
      "Epoch 1 Step 36/36 Loss: 5.2539\n",
      "Epoch 1 Step 1/36 Loss: 5.3497\n",
      "Epoch 1 Step 2/36 Loss: 5.9220\n",
      "Epoch 1 Step 3/36 Loss: 4.2968\n",
      "Epoch 1 Step 4/36 Loss: 6.4740\n",
      "Epoch 1 Step 5/36 Loss: 5.1125\n",
      "Epoch 1 Step 6/36 Loss: 4.7123\n",
      "Epoch 1 Step 7/36 Loss: 4.4427\n",
      "Epoch 1 Step 8/36 Loss: 5.1827\n",
      "Epoch 1 Step 9/36 Loss: 5.4322\n",
      "Epoch 1 Step 10/36 Loss: 6.5650\n",
      "Epoch 1 Step 11/36 Loss: 4.6927\n",
      "Epoch 1 Step 12/36 Loss: 5.7975\n",
      "Epoch 1 Step 13/36 Loss: 4.5814\n",
      "Epoch 1 Step 14/36 Loss: 5.8208\n",
      "Epoch 1 Step 15/36 Loss: 5.2334\n",
      "Epoch 1 Step 16/36 Loss: 6.2339\n",
      "Epoch 1 Step 17/36 Loss: 4.8087\n",
      "Epoch 1 Step 18/36 Loss: 6.2413\n",
      "Epoch 1 Step 19/36 Loss: 5.0351\n",
      "Epoch 1 Step 20/36 Loss: 5.4632\n",
      "Epoch 1 Step 21/36 Loss: 6.4249\n",
      "Epoch 1 Step 22/36 Loss: 5.2367\n",
      "Epoch 1 Step 23/36 Loss: 5.7486\n",
      "Epoch 1 Step 24/36 Loss: 4.3435\n",
      "Epoch 1 Step 25/36 Loss: 5.8238\n",
      "Epoch 1 Step 26/36 Loss: 5.5393\n",
      "Epoch 1 Step 27/36 Loss: 4.8138\n",
      "Epoch 1 Step 28/36 Loss: 5.9882\n",
      "Epoch 1 Step 29/36 Loss: 4.7355\n",
      "Epoch 1 Step 30/36 Loss: 4.8752\n",
      "Epoch 1 Step 31/36 Loss: 4.5708\n",
      "Epoch 1 Step 32/36 Loss: 5.4491\n",
      "Epoch 1 Step 33/36 Loss: 3.8280\n",
      "Epoch 1 Step 34/36 Loss: 6.1159\n",
      "Epoch 1 Step 35/36 Loss: 6.6922\n",
      "Epoch 1 Step 36/36 Loss: 6.4392\n",
      "Epoch 1 Step 1/36 Loss: 4.2883\n",
      "Epoch 1 Step 2/36 Loss: 6.1200\n",
      "Epoch 1 Step 3/36 Loss: 4.6902\n",
      "Epoch 1 Step 4/36 Loss: 5.0774\n",
      "Epoch 1 Step 5/36 Loss: 4.6944\n",
      "Epoch 1 Step 6/36 Loss: 4.9333\n",
      "Epoch 1 Step 7/36 Loss: 4.5308\n",
      "Epoch 1 Step 8/36 Loss: 4.7374\n",
      "Epoch 1 Step 9/36 Loss: 6.5593\n",
      "Epoch 1 Step 10/36 Loss: 5.5771\n",
      "Epoch 1 Step 11/36 Loss: 6.2794\n",
      "Epoch 1 Step 12/36 Loss: 5.2162\n",
      "Epoch 1 Step 13/36 Loss: 5.5313\n",
      "Epoch 1 Step 14/36 Loss: 4.7357\n",
      "Epoch 1 Step 15/36 Loss: 5.9225\n",
      "Epoch 1 Step 16/36 Loss: 5.6493\n",
      "Epoch 1 Step 17/36 Loss: 5.3256\n",
      "Epoch 1 Step 18/36 Loss: 3.6005\n",
      "Epoch 1 Step 19/36 Loss: 5.7581\n",
      "Epoch 1 Step 20/36 Loss: 5.6298\n",
      "Epoch 1 Step 21/36 Loss: 7.2569\n",
      "Epoch 1 Step 22/36 Loss: 4.9164\n",
      "Epoch 1 Step 23/36 Loss: 4.9535\n",
      "Epoch 1 Step 24/36 Loss: 5.6743\n",
      "Epoch 1 Step 25/36 Loss: 5.1465\n",
      "Epoch 1 Step 26/36 Loss: 5.4359\n",
      "Epoch 1 Step 27/36 Loss: 5.9127\n",
      "Epoch 1 Step 28/36 Loss: 6.5498\n",
      "Epoch 1 Step 29/36 Loss: 5.3178\n",
      "Epoch 1 Step 30/36 Loss: 4.9605\n",
      "Epoch 1 Step 31/36 Loss: 5.7633\n",
      "Epoch 1 Step 32/36 Loss: 5.8513\n",
      "Epoch 1 Step 33/36 Loss: 4.8521\n",
      "Epoch 1 Step 34/36 Loss: 6.7022\n",
      "Epoch 1 Step 35/36 Loss: 5.5934\n",
      "Epoch 1 Step 36/36 Loss: 5.9305\n",
      "Epoch 1 Step 1/36 Loss: 5.9756\n",
      "Epoch 1 Step 2/36 Loss: 5.1212\n",
      "Epoch 1 Step 3/36 Loss: 5.1789\n",
      "Epoch 1 Step 4/36 Loss: 5.9902\n",
      "Epoch 1 Step 5/36 Loss: 7.0239\n",
      "Epoch 1 Step 6/36 Loss: 4.7125\n",
      "Epoch 1 Step 7/36 Loss: 5.0735\n",
      "Epoch 1 Step 8/36 Loss: 6.0261\n",
      "Epoch 1 Step 9/36 Loss: 7.1133\n",
      "Epoch 1 Step 10/36 Loss: 5.1792\n",
      "Epoch 1 Step 11/36 Loss: 5.7917\n",
      "Epoch 1 Step 12/36 Loss: 6.5118\n",
      "Epoch 1 Step 13/36 Loss: 5.1930\n",
      "Epoch 1 Step 14/36 Loss: 5.4359\n",
      "Epoch 1 Step 15/36 Loss: 5.5696\n",
      "Epoch 1 Step 16/36 Loss: 4.2545\n",
      "Epoch 1 Step 17/36 Loss: 6.5010\n",
      "Epoch 1 Step 18/36 Loss: 4.6055\n",
      "Epoch 1 Step 19/36 Loss: 5.0466\n",
      "Epoch 1 Step 20/36 Loss: 4.7831\n",
      "Epoch 1 Step 21/36 Loss: 6.1246\n",
      "Epoch 1 Step 22/36 Loss: 4.4833\n",
      "Epoch 1 Step 23/36 Loss: 6.5465\n",
      "Epoch 1 Step 24/36 Loss: 5.6140\n",
      "Epoch 1 Step 25/36 Loss: 5.9256\n",
      "Epoch 1 Step 26/36 Loss: 5.6619\n",
      "Epoch 1 Step 27/36 Loss: 4.2907\n",
      "Epoch 1 Step 28/36 Loss: 5.7572\n",
      "Epoch 1 Step 29/36 Loss: 5.4867\n",
      "Epoch 1 Step 30/36 Loss: 5.3952\n",
      "Epoch 1 Step 31/36 Loss: 5.8444\n",
      "Epoch 1 Step 32/36 Loss: 4.8448\n",
      "Epoch 1 Step 33/36 Loss: 5.7500\n",
      "Epoch 1 Step 34/36 Loss: 5.8653\n",
      "Epoch 1 Step 35/36 Loss: 5.7699\n",
      "Epoch 1 Step 36/36 Loss: 4.9438\n",
      "Epoch 1 Step 1/36 Loss: 4.7167\n",
      "Epoch 1 Step 2/36 Loss: 5.6799\n",
      "Epoch 1 Step 3/36 Loss: 6.3004\n",
      "Epoch 1 Step 4/36 Loss: 5.4190\n",
      "Epoch 1 Step 5/36 Loss: 3.6947\n",
      "Epoch 1 Step 6/36 Loss: 4.9797\n",
      "Epoch 1 Step 7/36 Loss: 4.4977\n",
      "Epoch 1 Step 8/36 Loss: 6.2004\n",
      "Epoch 1 Step 9/36 Loss: 5.6313\n",
      "Epoch 1 Step 10/36 Loss: 5.7854\n",
      "Epoch 1 Step 11/36 Loss: 5.1959\n",
      "Epoch 1 Step 12/36 Loss: 6.2080\n",
      "Epoch 1 Step 13/36 Loss: 5.9503\n",
      "Epoch 1 Step 14/36 Loss: 4.9499\n",
      "Epoch 1 Step 15/36 Loss: 4.7544\n",
      "Epoch 1 Step 16/36 Loss: 5.0517\n",
      "Epoch 1 Step 17/36 Loss: 5.8327\n",
      "Epoch 1 Step 18/36 Loss: 7.0841\n",
      "Epoch 1 Step 19/36 Loss: 5.4533\n",
      "Epoch 1 Step 20/36 Loss: 6.3727\n",
      "Epoch 1 Step 21/36 Loss: 6.7431\n",
      "Epoch 1 Step 22/36 Loss: 4.8479\n",
      "Epoch 1 Step 23/36 Loss: 5.2567\n",
      "Epoch 1 Step 24/36 Loss: 7.2381\n",
      "Epoch 1 Step 25/36 Loss: 5.5400\n",
      "Epoch 1 Step 26/36 Loss: 5.4457\n",
      "Epoch 1 Step 27/36 Loss: 5.5900\n",
      "Epoch 1 Step 28/36 Loss: 4.9968\n",
      "Epoch 1 Step 29/36 Loss: 5.7029\n",
      "Epoch 1 Step 30/36 Loss: 5.2353\n",
      "Epoch 1 Step 31/36 Loss: 4.9378\n",
      "Epoch 1 Step 32/36 Loss: 5.2646\n",
      "Epoch 1 Step 33/36 Loss: 3.9833\n",
      "Epoch 1 Step 34/36 Loss: 4.9586\n",
      "Epoch 1 Step 35/36 Loss: 5.4797\n",
      "Epoch 1 Step 36/36 Loss: 6.6156\n",
      "Epoch 1 Step 1/36 Loss: 5.5198\n",
      "Epoch 1 Step 2/36 Loss: 4.5802\n",
      "Epoch 1 Step 3/36 Loss: 4.8401\n",
      "Epoch 1 Step 4/36 Loss: 4.5898\n",
      "Epoch 1 Step 5/36 Loss: 5.0835\n",
      "Epoch 1 Step 6/36 Loss: 5.3215\n",
      "Epoch 1 Step 7/36 Loss: 6.7134\n",
      "Epoch 1 Step 8/36 Loss: 7.4336\n",
      "Epoch 1 Step 9/36 Loss: 4.5494\n",
      "Epoch 1 Step 10/36 Loss: 5.3162\n",
      "Epoch 1 Step 11/36 Loss: 7.6560\n",
      "Epoch 1 Step 12/36 Loss: 5.2453\n",
      "Epoch 1 Step 13/36 Loss: 4.9911\n",
      "Epoch 1 Step 14/36 Loss: 5.5771\n",
      "Epoch 1 Step 15/36 Loss: 6.0008\n",
      "Epoch 1 Step 16/36 Loss: 6.0315\n",
      "Epoch 1 Step 17/36 Loss: 6.9008\n",
      "Epoch 1 Step 18/36 Loss: 5.9593\n",
      "Epoch 1 Step 19/36 Loss: 5.3621\n",
      "Epoch 1 Step 20/36 Loss: 3.9471\n",
      "Epoch 1 Step 21/36 Loss: 4.0458\n",
      "Epoch 1 Step 22/36 Loss: 6.4214\n",
      "Epoch 1 Step 23/36 Loss: 5.6579\n",
      "Epoch 1 Step 24/36 Loss: 5.5110\n",
      "Epoch 1 Step 25/36 Loss: 5.9923\n",
      "Epoch 1 Step 26/36 Loss: 5.0474\n",
      "Epoch 1 Step 27/36 Loss: 5.9720\n",
      "Epoch 1 Step 28/36 Loss: 6.5986\n",
      "Epoch 1 Step 29/36 Loss: 5.1635\n",
      "Epoch 1 Step 30/36 Loss: 6.0128\n",
      "Epoch 1 Step 31/36 Loss: 4.4980\n",
      "Epoch 1 Step 32/36 Loss: 5.5937\n",
      "Epoch 1 Step 33/36 Loss: 5.5779\n",
      "Epoch 1 Step 34/36 Loss: 4.5800\n",
      "Epoch 1 Step 35/36 Loss: 5.3696\n",
      "Epoch 1 Step 36/36 Loss: 5.9756\n",
      "Epoch 1 Step 1/36 Loss: 4.5851\n",
      "Epoch 1 Step 2/36 Loss: 4.5402\n",
      "Epoch 1 Step 3/36 Loss: 6.0198\n",
      "Epoch 1 Step 4/36 Loss: 5.4685\n",
      "Epoch 1 Step 5/36 Loss: 5.2633\n",
      "Epoch 1 Step 6/36 Loss: 5.6340\n",
      "Epoch 1 Step 7/36 Loss: 7.2453\n",
      "Epoch 1 Step 8/36 Loss: 4.2613\n",
      "Epoch 1 Step 9/36 Loss: 5.9965\n",
      "Epoch 1 Step 10/36 Loss: 5.7169\n",
      "Epoch 1 Step 11/36 Loss: 5.5516\n",
      "Epoch 1 Step 12/36 Loss: 5.5740\n",
      "Epoch 1 Step 13/36 Loss: 4.6129\n",
      "Epoch 1 Step 14/36 Loss: 5.2613\n",
      "Epoch 1 Step 15/36 Loss: 5.3954\n",
      "Epoch 1 Step 16/36 Loss: 4.8646\n",
      "Epoch 1 Step 17/36 Loss: 6.4363\n",
      "Epoch 1 Step 18/36 Loss: 6.1258\n",
      "Epoch 1 Step 19/36 Loss: 4.9478\n",
      "Epoch 1 Step 20/36 Loss: 5.5513\n",
      "Epoch 1 Step 21/36 Loss: 5.3748\n",
      "Epoch 1 Step 22/36 Loss: 5.6797\n",
      "Epoch 1 Step 23/36 Loss: 5.5585\n",
      "Epoch 1 Step 24/36 Loss: 4.9799\n",
      "Epoch 1 Step 25/36 Loss: 4.5252\n",
      "Epoch 1 Step 26/36 Loss: 5.5198\n",
      "Epoch 1 Step 27/36 Loss: 3.9238\n",
      "Epoch 1 Step 28/36 Loss: 5.3520\n",
      "Epoch 1 Step 29/36 Loss: 5.6574\n",
      "Epoch 1 Step 30/36 Loss: 6.3099\n",
      "Epoch 1 Step 31/36 Loss: 5.4679\n",
      "Epoch 1 Step 32/36 Loss: 5.0073\n",
      "Epoch 1 Step 33/36 Loss: 5.3451\n",
      "Epoch 1 Step 34/36 Loss: 6.9518\n",
      "Epoch 1 Step 35/36 Loss: 3.7631\n",
      "Epoch 1 Step 36/36 Loss: 5.7075\n",
      "Epoch 1 Step 1/36 Loss: 4.7044\n",
      "Epoch 1 Step 2/36 Loss: 4.9094\n",
      "Epoch 1 Step 3/36 Loss: 4.4396\n",
      "Epoch 1 Step 4/36 Loss: 5.9692\n",
      "Epoch 1 Step 5/36 Loss: 6.8206\n",
      "Epoch 1 Step 6/36 Loss: 4.5698\n",
      "Epoch 1 Step 7/36 Loss: 5.0827\n",
      "Epoch 1 Step 8/36 Loss: 3.7796\n",
      "Epoch 1 Step 9/36 Loss: 5.1855\n",
      "Epoch 1 Step 10/36 Loss: 5.0210\n",
      "Epoch 1 Step 11/36 Loss: 5.1742\n",
      "Epoch 1 Step 12/36 Loss: 5.8164\n",
      "Epoch 1 Step 13/36 Loss: 5.2106\n",
      "Epoch 1 Step 14/36 Loss: 6.4754\n",
      "Epoch 1 Step 15/36 Loss: 5.3962\n",
      "Epoch 1 Step 16/36 Loss: 5.1989\n",
      "Epoch 1 Step 17/36 Loss: 4.7233\n",
      "Epoch 1 Step 18/36 Loss: 6.2328\n",
      "Epoch 1 Step 19/36 Loss: 4.7151\n",
      "Epoch 1 Step 20/36 Loss: 7.0862\n",
      "Epoch 1 Step 21/36 Loss: 5.5202\n",
      "Epoch 1 Step 22/36 Loss: 4.5688\n",
      "Epoch 1 Step 23/36 Loss: 4.8638\n",
      "Epoch 1 Step 24/36 Loss: 5.7496\n",
      "Epoch 1 Step 25/36 Loss: 4.9360\n",
      "Epoch 1 Step 26/36 Loss: 5.7477\n",
      "Epoch 1 Step 27/36 Loss: 3.9899\n",
      "Epoch 1 Step 28/36 Loss: 6.2944\n",
      "Epoch 1 Step 29/36 Loss: 5.7108\n",
      "Epoch 1 Step 30/36 Loss: 6.4179\n",
      "Epoch 1 Step 31/36 Loss: 5.4028\n",
      "Epoch 1 Step 32/36 Loss: 5.5267\n",
      "Epoch 1 Step 33/36 Loss: 5.0908\n",
      "Epoch 1 Step 34/36 Loss: 5.5081\n",
      "Epoch 1 Step 35/36 Loss: 6.2482\n",
      "Epoch 1 Step 36/36 Loss: 6.1459\n",
      "Epoch 1 Step 1/36 Loss: 4.5100\n",
      "Epoch 1 Step 2/36 Loss: 5.5824\n",
      "Epoch 1 Step 3/36 Loss: 4.4464\n",
      "Epoch 1 Step 4/36 Loss: 5.0777\n",
      "Epoch 1 Step 5/36 Loss: 5.3221\n",
      "Epoch 1 Step 6/36 Loss: 6.6137\n",
      "Epoch 1 Step 7/36 Loss: 5.3194\n",
      "Epoch 1 Step 8/36 Loss: 5.5863\n",
      "Epoch 1 Step 9/36 Loss: 5.4034\n",
      "Epoch 1 Step 10/36 Loss: 5.5759\n",
      "Epoch 1 Step 11/36 Loss: 5.6911\n",
      "Epoch 1 Step 12/36 Loss: 4.0416\n",
      "Epoch 1 Step 13/36 Loss: 5.4034\n",
      "Epoch 1 Step 14/36 Loss: 5.4823\n",
      "Epoch 1 Step 15/36 Loss: 5.7073\n",
      "Epoch 1 Step 16/36 Loss: 6.4216\n",
      "Epoch 1 Step 17/36 Loss: 6.2741\n",
      "Epoch 1 Step 18/36 Loss: 6.0007\n",
      "Epoch 1 Step 19/36 Loss: 4.6712\n",
      "Epoch 1 Step 20/36 Loss: 6.1747\n",
      "Epoch 1 Step 21/36 Loss: 5.6531\n",
      "Epoch 1 Step 22/36 Loss: 5.6224\n",
      "Epoch 1 Step 23/36 Loss: 5.5533\n",
      "Epoch 1 Step 24/36 Loss: 4.9730\n",
      "Epoch 1 Step 25/36 Loss: 5.4873\n",
      "Epoch 1 Step 26/36 Loss: 5.2944\n",
      "Epoch 1 Step 27/36 Loss: 4.7691\n",
      "Epoch 1 Step 28/36 Loss: 4.5604\n",
      "Epoch 1 Step 29/36 Loss: 5.5195\n",
      "Epoch 1 Step 30/36 Loss: 5.6998\n",
      "Epoch 1 Step 31/36 Loss: 4.5678\n",
      "Epoch 1 Step 32/36 Loss: 4.5512\n",
      "Epoch 1 Step 33/36 Loss: 5.5778\n",
      "Epoch 1 Step 34/36 Loss: 4.6995\n",
      "Epoch 1 Step 35/36 Loss: 7.0904\n",
      "Epoch 1 Step 36/36 Loss: 5.3714\n",
      "Epoch 1 Step 1/36 Loss: 5.1688\n",
      "Epoch 1 Step 2/36 Loss: 4.5259\n",
      "Epoch 1 Step 3/36 Loss: 7.2642\n",
      "Epoch 1 Step 4/36 Loss: 4.0502\n",
      "Epoch 1 Step 5/36 Loss: 4.1307\n",
      "Epoch 1 Step 6/36 Loss: 4.7990\n",
      "Epoch 1 Step 7/36 Loss: 5.7740\n",
      "Epoch 1 Step 8/36 Loss: 6.4405\n",
      "Epoch 1 Step 9/36 Loss: 4.9668\n",
      "Epoch 1 Step 10/36 Loss: 5.9168\n",
      "Epoch 1 Step 11/36 Loss: 5.5588\n",
      "Epoch 1 Step 12/36 Loss: 4.9222\n",
      "Epoch 1 Step 13/36 Loss: 6.5129\n",
      "Epoch 1 Step 14/36 Loss: 4.8283\n",
      "Epoch 1 Step 15/36 Loss: 5.7940\n",
      "Epoch 1 Step 16/36 Loss: 5.3938\n",
      "Epoch 1 Step 17/36 Loss: 6.2901\n",
      "Epoch 1 Step 18/36 Loss: 5.7593\n",
      "Epoch 1 Step 19/36 Loss: 5.9048\n",
      "Epoch 1 Step 20/36 Loss: 5.1863\n",
      "Epoch 1 Step 21/36 Loss: 5.4500\n",
      "Epoch 1 Step 22/36 Loss: 4.8318\n",
      "Epoch 1 Step 23/36 Loss: 5.7391\n",
      "Epoch 1 Step 24/36 Loss: 6.5205\n",
      "Epoch 1 Step 25/36 Loss: 5.0668\n",
      "Epoch 1 Step 26/36 Loss: 6.1367\n",
      "Epoch 1 Step 27/36 Loss: 5.0797\n",
      "Epoch 1 Step 28/36 Loss: 5.7272\n",
      "Epoch 1 Step 29/36 Loss: 6.7097\n",
      "Epoch 1 Step 30/36 Loss: 5.0913\n",
      "Epoch 1 Step 31/36 Loss: 5.5561\n",
      "Epoch 1 Step 32/36 Loss: 7.1519\n",
      "Epoch 1 Step 33/36 Loss: 5.3745\n",
      "Epoch 1 Step 34/36 Loss: 6.0181\n",
      "Epoch 1 Step 35/36 Loss: 5.8408\n",
      "Epoch 1 Step 36/36 Loss: 4.6587\n",
      "Epoch 1 Step 1/36 Loss: 6.0812\n",
      "Epoch 1 Step 2/36 Loss: 5.6891\n",
      "Epoch 1 Step 3/36 Loss: 4.7970\n",
      "Epoch 1 Step 4/36 Loss: 5.5530\n",
      "Epoch 1 Step 5/36 Loss: 4.6464\n",
      "Epoch 1 Step 6/36 Loss: 5.9918\n",
      "Epoch 1 Step 7/36 Loss: 5.9386\n",
      "Epoch 1 Step 8/36 Loss: 5.5517\n",
      "Epoch 1 Step 9/36 Loss: 5.9544\n",
      "Epoch 1 Step 10/36 Loss: 6.0205\n",
      "Epoch 1 Step 11/36 Loss: 6.6558\n",
      "Epoch 1 Step 12/36 Loss: 6.1458\n",
      "Epoch 1 Step 13/36 Loss: 6.5199\n",
      "Epoch 1 Step 14/36 Loss: 4.4496\n",
      "Epoch 1 Step 15/36 Loss: 6.3809\n",
      "Epoch 1 Step 16/36 Loss: 5.2702\n",
      "Epoch 1 Step 17/36 Loss: 7.1023\n",
      "Epoch 1 Step 18/36 Loss: 6.6251\n",
      "Epoch 1 Step 19/36 Loss: 6.4166\n",
      "Epoch 1 Step 20/36 Loss: 4.7167\n",
      "Epoch 1 Step 21/36 Loss: 4.6560\n",
      "Epoch 1 Step 22/36 Loss: 4.2892\n",
      "Epoch 1 Step 23/36 Loss: 5.1556\n",
      "Epoch 1 Step 24/36 Loss: 6.1767\n",
      "Epoch 1 Step 25/36 Loss: 5.6467\n",
      "Epoch 1 Step 26/36 Loss: 5.5603\n",
      "Epoch 1 Step 27/36 Loss: 5.7069\n",
      "Epoch 1 Step 28/36 Loss: 5.8450\n",
      "Epoch 1 Step 29/36 Loss: 5.0809\n",
      "Epoch 1 Step 30/36 Loss: 5.2618\n",
      "Epoch 1 Step 31/36 Loss: 6.3773\n",
      "Epoch 1 Step 32/36 Loss: 6.1520\n",
      "Epoch 1 Step 33/36 Loss: 4.9083\n",
      "Epoch 1 Step 34/36 Loss: 4.8322\n",
      "Epoch 1 Step 35/36 Loss: 4.7440\n",
      "Epoch 1 Step 36/36 Loss: 4.6140\n",
      "Epoch 1 Step 1/36 Loss: 4.0799\n",
      "Epoch 1 Step 2/36 Loss: 6.3132\n",
      "Epoch 1 Step 3/36 Loss: 4.3840\n",
      "Epoch 1 Step 4/36 Loss: 4.8279\n",
      "Epoch 1 Step 5/36 Loss: 4.7921\n",
      "Epoch 1 Step 6/36 Loss: 4.5583\n",
      "Epoch 1 Step 7/36 Loss: 5.3691\n",
      "Epoch 1 Step 8/36 Loss: 6.4839\n",
      "Epoch 1 Step 9/36 Loss: 6.1146\n",
      "Epoch 1 Step 10/36 Loss: 5.1479\n",
      "Epoch 1 Step 11/36 Loss: 5.0331\n",
      "Epoch 1 Step 12/36 Loss: 4.5622\n",
      "Epoch 1 Step 13/36 Loss: 5.0566\n",
      "Epoch 1 Step 14/36 Loss: 6.0498\n",
      "Epoch 1 Step 15/36 Loss: 6.1628\n",
      "Epoch 1 Step 16/36 Loss: 5.4728\n",
      "Epoch 1 Step 17/36 Loss: 5.6104\n",
      "Epoch 1 Step 18/36 Loss: 6.1702\n",
      "Epoch 1 Step 19/36 Loss: 5.4711\n",
      "Epoch 1 Step 20/36 Loss: 6.8938\n",
      "Epoch 1 Step 21/36 Loss: 6.7332\n",
      "Epoch 1 Step 22/36 Loss: 4.6312\n",
      "Epoch 1 Step 23/36 Loss: 4.4797\n",
      "Epoch 1 Step 24/36 Loss: 4.0345\n",
      "Epoch 1 Step 25/36 Loss: 5.1807\n",
      "Epoch 1 Step 26/36 Loss: 5.4795\n",
      "Epoch 1 Step 27/36 Loss: 5.7184\n",
      "Epoch 1 Step 28/36 Loss: 6.6254\n",
      "Epoch 1 Step 29/36 Loss: 5.5422\n",
      "Epoch 1 Step 30/36 Loss: 7.0159\n",
      "Epoch 1 Step 31/36 Loss: 5.5711\n",
      "Epoch 1 Step 32/36 Loss: 4.4731\n",
      "Epoch 1 Step 33/36 Loss: 6.7171\n",
      "Epoch 1 Step 34/36 Loss: 5.2678\n",
      "Epoch 1 Step 35/36 Loss: 5.5254\n",
      "Epoch 1 Step 36/36 Loss: 6.2692\n",
      "Epoch 1 Step 1/36 Loss: 4.4635\n",
      "Epoch 1 Step 2/36 Loss: 4.2920\n",
      "Epoch 1 Step 3/36 Loss: 5.4047\n",
      "Epoch 1 Step 4/36 Loss: 5.1995\n",
      "Epoch 1 Step 5/36 Loss: 5.3188\n",
      "Epoch 1 Step 6/36 Loss: 4.4367\n",
      "Epoch 1 Step 7/36 Loss: 6.1611\n",
      "Epoch 1 Step 8/36 Loss: 5.0843\n",
      "Epoch 1 Step 9/36 Loss: 5.1754\n",
      "Epoch 1 Step 10/36 Loss: 5.5235\n",
      "Epoch 1 Step 11/36 Loss: 5.0650\n",
      "Epoch 1 Step 12/36 Loss: 4.1312\n",
      "Epoch 1 Step 13/36 Loss: 6.7314\n",
      "Epoch 1 Step 14/36 Loss: 6.0940\n",
      "Epoch 1 Step 15/36 Loss: 4.9687\n",
      "Epoch 1 Step 16/36 Loss: 4.6865\n",
      "Epoch 1 Step 17/36 Loss: 6.6185\n",
      "Epoch 1 Step 18/36 Loss: 5.4609\n",
      "Epoch 1 Step 19/36 Loss: 5.4660\n",
      "Epoch 1 Step 20/36 Loss: 5.3243\n",
      "Epoch 1 Step 21/36 Loss: 4.6015\n",
      "Epoch 1 Step 22/36 Loss: 4.8688\n",
      "Epoch 1 Step 23/36 Loss: 5.4823\n",
      "Epoch 1 Step 24/36 Loss: 7.4038\n",
      "Epoch 1 Step 25/36 Loss: 4.4128\n",
      "Epoch 1 Step 26/36 Loss: 5.1428\n",
      "Epoch 1 Step 27/36 Loss: 4.7022\n",
      "Epoch 1 Step 28/36 Loss: 4.8144\n",
      "Epoch 1 Step 29/36 Loss: 4.8257\n",
      "Epoch 1 Step 30/36 Loss: 5.7605\n",
      "Epoch 1 Step 31/36 Loss: 5.0488\n",
      "Epoch 1 Step 32/36 Loss: 5.6046\n",
      "Epoch 1 Step 33/36 Loss: 4.4127\n",
      "Epoch 1 Step 34/36 Loss: 6.7299\n",
      "Epoch 1 Step 35/36 Loss: 4.6627\n",
      "Epoch 1 Step 36/36 Loss: 6.1061\n",
      "Epoch 1 Step 1/36 Loss: 4.6339\n",
      "Epoch 1 Step 2/36 Loss: 6.6373\n",
      "Epoch 1 Step 3/36 Loss: 5.3037\n",
      "Epoch 1 Step 4/36 Loss: 5.6667\n",
      "Epoch 1 Step 5/36 Loss: 4.2876\n",
      "Epoch 1 Step 6/36 Loss: 4.5134\n",
      "Epoch 1 Step 7/36 Loss: 4.6026\n",
      "Epoch 1 Step 8/36 Loss: 5.2458\n",
      "Epoch 1 Step 9/36 Loss: 6.2970\n",
      "Epoch 1 Step 10/36 Loss: 5.3718\n",
      "Epoch 1 Step 11/36 Loss: 5.6999\n",
      "Epoch 1 Step 12/36 Loss: 5.8357\n",
      "Epoch 1 Step 13/36 Loss: 4.8846\n",
      "Epoch 1 Step 14/36 Loss: 4.0116\n",
      "Epoch 1 Step 15/36 Loss: 4.5873\n",
      "Epoch 1 Step 16/36 Loss: 4.8211\n",
      "Epoch 1 Step 17/36 Loss: 4.4137\n",
      "Epoch 1 Step 18/36 Loss: 5.7699\n",
      "Epoch 1 Step 19/36 Loss: 6.1010\n",
      "Epoch 1 Step 20/36 Loss: 6.0830\n",
      "Epoch 1 Step 21/36 Loss: 4.9722\n",
      "Epoch 1 Step 22/36 Loss: 4.9209\n",
      "Epoch 1 Step 23/36 Loss: 6.5533\n",
      "Epoch 1 Step 24/36 Loss: 5.2766\n",
      "Epoch 1 Step 25/36 Loss: 4.1920\n",
      "Epoch 1 Step 26/36 Loss: 5.6608\n",
      "Epoch 1 Step 27/36 Loss: 4.9159\n",
      "Epoch 1 Step 28/36 Loss: 6.9191\n",
      "Epoch 1 Step 29/36 Loss: 5.2193\n",
      "Epoch 1 Step 30/36 Loss: 5.5930\n",
      "Epoch 1 Step 31/36 Loss: 5.5835\n",
      "Epoch 1 Step 32/36 Loss: 6.1576\n",
      "Epoch 1 Step 33/36 Loss: 5.9555\n",
      "Epoch 1 Step 34/36 Loss: 5.3170\n",
      "Epoch 1 Step 35/36 Loss: 6.7806\n",
      "Epoch 1 Step 36/36 Loss: 5.1063\n",
      "Epoch 1 Step 1/36 Loss: 4.6908\n",
      "Epoch 1 Step 2/36 Loss: 3.6545\n",
      "Epoch 1 Step 3/36 Loss: 6.3816\n",
      "Epoch 1 Step 4/36 Loss: 5.2177\n",
      "Epoch 1 Step 5/36 Loss: 5.6815\n",
      "Epoch 1 Step 6/36 Loss: 4.5082\n",
      "Epoch 1 Step 7/36 Loss: 5.2004\n",
      "Epoch 1 Step 8/36 Loss: 5.3283\n",
      "Epoch 1 Step 9/36 Loss: 6.6412\n",
      "Epoch 1 Step 10/36 Loss: 4.4765\n",
      "Epoch 1 Step 11/36 Loss: 6.1825\n",
      "Epoch 1 Step 12/36 Loss: 6.5240\n",
      "Epoch 1 Step 13/36 Loss: 4.3297\n",
      "Epoch 1 Step 14/36 Loss: 6.1696\n",
      "Epoch 1 Step 15/36 Loss: 6.9907\n",
      "Epoch 1 Step 16/36 Loss: 5.4003\n",
      "Epoch 1 Step 17/36 Loss: 5.4123\n",
      "Epoch 1 Step 18/36 Loss: 6.7236\n",
      "Epoch 1 Step 19/36 Loss: 5.5058\n",
      "Epoch 1 Step 20/36 Loss: 4.6738\n",
      "Epoch 1 Step 21/36 Loss: 5.6902\n",
      "Epoch 1 Step 22/36 Loss: 6.2136\n",
      "Epoch 1 Step 23/36 Loss: 6.7602\n",
      "Epoch 1 Step 24/36 Loss: 5.0019\n",
      "Epoch 1 Step 25/36 Loss: 4.6153\n",
      "Epoch 1 Step 26/36 Loss: 4.9698\n",
      "Epoch 1 Step 27/36 Loss: 6.7634\n",
      "Epoch 1 Step 28/36 Loss: 4.8241\n",
      "Epoch 1 Step 29/36 Loss: 6.4556\n",
      "Epoch 1 Step 30/36 Loss: 6.1811\n",
      "Epoch 1 Step 31/36 Loss: 5.0323\n",
      "Epoch 1 Step 32/36 Loss: 5.7963\n",
      "Epoch 1 Step 33/36 Loss: 5.6899\n",
      "Epoch 1 Step 34/36 Loss: 4.8072\n",
      "Epoch 1 Step 35/36 Loss: 4.0531\n",
      "Epoch 1 Step 36/36 Loss: 5.1423\n",
      "Epoch 1 Step 1/36 Loss: 5.6555\n",
      "Epoch 1 Step 2/36 Loss: 7.0380\n",
      "Epoch 1 Step 3/36 Loss: 4.9173\n",
      "Epoch 1 Step 4/36 Loss: 4.0348\n",
      "Epoch 1 Step 5/36 Loss: 5.7889\n",
      "Epoch 1 Step 6/36 Loss: 4.3389\n",
      "Epoch 1 Step 7/36 Loss: 5.7696\n",
      "Epoch 1 Step 8/36 Loss: 5.5217\n",
      "Epoch 1 Step 9/36 Loss: 5.0611\n",
      "Epoch 1 Step 10/36 Loss: 6.6230\n",
      "Epoch 1 Step 11/36 Loss: 5.2663\n",
      "Epoch 1 Step 12/36 Loss: 5.3421\n",
      "Epoch 1 Step 13/36 Loss: 4.8216\n",
      "Epoch 1 Step 14/36 Loss: 7.0490\n",
      "Epoch 1 Step 15/36 Loss: 5.5249\n",
      "Epoch 1 Step 16/36 Loss: 5.3211\n",
      "Epoch 1 Step 17/36 Loss: 6.6085\n",
      "Epoch 1 Step 18/36 Loss: 5.4938\n",
      "Epoch 1 Step 19/36 Loss: 5.2031\n",
      "Epoch 1 Step 20/36 Loss: 5.1343\n",
      "Epoch 1 Step 21/36 Loss: 4.1321\n",
      "Epoch 1 Step 22/36 Loss: 4.6373\n",
      "Epoch 1 Step 23/36 Loss: 3.9357\n",
      "Epoch 1 Step 24/36 Loss: 4.5478\n",
      "Epoch 1 Step 25/36 Loss: 6.3031\n",
      "Epoch 1 Step 26/36 Loss: 3.7005\n",
      "Epoch 1 Step 27/36 Loss: 6.0851\n",
      "Epoch 1 Step 28/36 Loss: 4.3944\n",
      "Epoch 1 Step 29/36 Loss: 6.1282\n",
      "Epoch 1 Step 30/36 Loss: 5.8817\n",
      "Epoch 1 Step 31/36 Loss: 5.6045\n",
      "Epoch 1 Step 32/36 Loss: 5.7933\n",
      "Epoch 1 Step 33/36 Loss: 6.2830\n",
      "Epoch 1 Step 34/36 Loss: 5.9217\n",
      "Epoch 1 Step 35/36 Loss: 4.5882\n",
      "Epoch 1 Step 36/36 Loss: 5.1777\n",
      "Epoch 1 Step 1/36 Loss: 4.5707\n",
      "Epoch 1 Step 2/36 Loss: 4.8693\n",
      "Epoch 1 Step 3/36 Loss: 4.1783\n",
      "Epoch 1 Step 4/36 Loss: 5.6936\n",
      "Epoch 1 Step 5/36 Loss: 4.7153\n",
      "Epoch 1 Step 6/36 Loss: 4.3173\n",
      "Epoch 1 Step 7/36 Loss: 6.4140\n",
      "Epoch 1 Step 8/36 Loss: 4.6364\n",
      "Epoch 1 Step 9/36 Loss: 6.4984\n",
      "Epoch 1 Step 10/36 Loss: 6.4231\n",
      "Epoch 1 Step 11/36 Loss: 5.2602\n",
      "Epoch 1 Step 12/36 Loss: 4.8913\n",
      "Epoch 1 Step 13/36 Loss: 5.5295\n",
      "Epoch 1 Step 14/36 Loss: 5.3412\n",
      "Epoch 1 Step 15/36 Loss: 5.1457\n",
      "Epoch 1 Step 16/36 Loss: 4.4491\n",
      "Epoch 1 Step 17/36 Loss: 5.6429\n",
      "Epoch 1 Step 18/36 Loss: 4.9394\n",
      "Epoch 1 Step 19/36 Loss: 5.3115\n",
      "Epoch 1 Step 20/36 Loss: 4.7199\n",
      "Epoch 1 Step 21/36 Loss: 5.4541\n",
      "Epoch 1 Step 22/36 Loss: 4.8694\n",
      "Epoch 1 Step 23/36 Loss: 5.5043\n",
      "Epoch 1 Step 24/36 Loss: 5.1673\n",
      "Epoch 1 Step 25/36 Loss: 4.9050\n",
      "Epoch 1 Step 26/36 Loss: 5.5320\n",
      "Epoch 1 Step 27/36 Loss: 4.7996\n",
      "Epoch 1 Step 28/36 Loss: 4.6969\n",
      "Epoch 1 Step 29/36 Loss: 6.0146\n",
      "Epoch 1 Step 30/36 Loss: 4.9700\n",
      "Epoch 1 Step 31/36 Loss: 5.5749\n",
      "Epoch 1 Step 32/36 Loss: 6.5552\n",
      "Epoch 1 Step 33/36 Loss: 6.0817\n",
      "Epoch 1 Step 34/36 Loss: 5.2099\n",
      "Epoch 1 Step 35/36 Loss: 6.4904\n",
      "Epoch 1 Step 36/36 Loss: 5.5819\n",
      "Epoch 1 Step 1/36 Loss: 4.2325\n",
      "Epoch 1 Step 2/36 Loss: 6.0380\n",
      "Epoch 1 Step 3/36 Loss: 5.5287\n",
      "Epoch 1 Step 4/36 Loss: 6.4156\n",
      "Epoch 1 Step 5/36 Loss: 5.0840\n",
      "Epoch 1 Step 6/36 Loss: 5.4935\n",
      "Epoch 1 Step 7/36 Loss: 7.2532\n",
      "Epoch 1 Step 8/36 Loss: 6.1553\n",
      "Epoch 1 Step 9/36 Loss: 5.1229\n",
      "Epoch 1 Step 10/36 Loss: 5.3905\n",
      "Epoch 1 Step 11/36 Loss: 5.8495\n",
      "Epoch 1 Step 12/36 Loss: 5.0732\n",
      "Epoch 1 Step 13/36 Loss: 6.1726\n",
      "Epoch 1 Step 14/36 Loss: 5.1088\n",
      "Epoch 1 Step 15/36 Loss: 6.0213\n",
      "Epoch 1 Step 16/36 Loss: 4.7492\n",
      "Epoch 1 Step 17/36 Loss: 5.7376\n",
      "Epoch 1 Step 18/36 Loss: 5.0497\n",
      "Epoch 1 Step 19/36 Loss: 5.9533\n",
      "Epoch 1 Step 20/36 Loss: 5.4406\n",
      "Epoch 1 Step 21/36 Loss: 4.9487\n",
      "Epoch 1 Step 22/36 Loss: 4.6423\n",
      "Epoch 1 Step 23/36 Loss: 4.1459\n",
      "Epoch 1 Step 24/36 Loss: 4.3261\n",
      "Epoch 1 Step 25/36 Loss: 4.3640\n",
      "Epoch 1 Step 26/36 Loss: 5.4320\n",
      "Epoch 1 Step 27/36 Loss: 7.5994\n",
      "Epoch 1 Step 28/36 Loss: 5.7970\n",
      "Epoch 1 Step 29/36 Loss: 5.7961\n",
      "Epoch 1 Step 30/36 Loss: 5.9177\n",
      "Epoch 1 Step 31/36 Loss: 3.7889\n",
      "Epoch 1 Step 32/36 Loss: 5.4768\n",
      "Epoch 1 Step 33/36 Loss: 4.9897\n",
      "Epoch 1 Step 34/36 Loss: 5.6738\n",
      "Epoch 1 Step 35/36 Loss: 6.4024\n",
      "Epoch 1 Step 36/36 Loss: 5.7736\n",
      "Epoch 1 Step 1/36 Loss: 5.7575\n",
      "Epoch 1 Step 2/36 Loss: 4.4302\n",
      "Epoch 1 Step 3/36 Loss: 4.7288\n",
      "Epoch 1 Step 4/36 Loss: 4.6406\n",
      "Epoch 1 Step 5/36 Loss: 4.9606\n",
      "Epoch 1 Step 6/36 Loss: 5.7010\n",
      "Epoch 1 Step 7/36 Loss: 6.1668\n",
      "Epoch 1 Step 8/36 Loss: 6.2205\n",
      "Epoch 1 Step 9/36 Loss: 6.0108\n",
      "Epoch 1 Step 10/36 Loss: 6.4260\n",
      "Epoch 1 Step 11/36 Loss: 3.9246\n",
      "Epoch 1 Step 12/36 Loss: 4.5178\n",
      "Epoch 1 Step 13/36 Loss: 5.6930\n",
      "Epoch 1 Step 14/36 Loss: 4.9897\n",
      "Epoch 1 Step 15/36 Loss: 6.6791\n",
      "Epoch 1 Step 16/36 Loss: 6.0484\n",
      "Epoch 1 Step 17/36 Loss: 4.6848\n",
      "Epoch 1 Step 18/36 Loss: 5.4612\n",
      "Epoch 1 Step 19/36 Loss: 5.1786\n",
      "Epoch 1 Step 20/36 Loss: 5.5050\n",
      "Epoch 1 Step 21/36 Loss: 5.2993\n",
      "Epoch 1 Step 22/36 Loss: 6.1351\n",
      "Epoch 1 Step 23/36 Loss: 4.3210\n",
      "Epoch 1 Step 24/36 Loss: 5.5236\n",
      "Epoch 1 Step 25/36 Loss: 5.1694\n",
      "Epoch 1 Step 26/36 Loss: 5.6106\n",
      "Epoch 1 Step 27/36 Loss: 7.1432\n",
      "Epoch 1 Step 28/36 Loss: 6.4466\n",
      "Epoch 1 Step 29/36 Loss: 5.6085\n",
      "Epoch 1 Step 30/36 Loss: 3.9921\n",
      "Epoch 1 Step 31/36 Loss: 5.6327\n",
      "Epoch 1 Step 32/36 Loss: 6.3154\n",
      "Epoch 1 Step 33/36 Loss: 5.5207\n",
      "Epoch 1 Step 34/36 Loss: 4.0247\n",
      "Epoch 1 Step 35/36 Loss: 5.1359\n",
      "Epoch 1 Step 36/36 Loss: 4.7027\n",
      "Epoch 1 Step 1/36 Loss: 5.1357\n",
      "Epoch 1 Step 2/36 Loss: 4.5859\n",
      "Epoch 1 Step 3/36 Loss: 5.9217\n",
      "Epoch 1 Step 4/36 Loss: 4.3590\n",
      "Epoch 1 Step 5/36 Loss: 4.1866\n",
      "Epoch 1 Step 6/36 Loss: 5.7151\n",
      "Epoch 1 Step 7/36 Loss: 4.5274\n",
      "Epoch 1 Step 8/36 Loss: 7.2160\n",
      "Epoch 1 Step 9/36 Loss: 5.6457\n",
      "Epoch 1 Step 10/36 Loss: 6.2205\n",
      "Epoch 1 Step 11/36 Loss: 5.7951\n",
      "Epoch 1 Step 12/36 Loss: 4.9670\n",
      "Epoch 1 Step 13/36 Loss: 4.6836\n",
      "Epoch 1 Step 14/36 Loss: 5.4657\n",
      "Epoch 1 Step 15/36 Loss: 5.5539\n",
      "Epoch 1 Step 16/36 Loss: 5.9584\n",
      "Epoch 1 Step 17/36 Loss: 5.6596\n",
      "Epoch 1 Step 18/36 Loss: 6.0394\n",
      "Epoch 1 Step 19/36 Loss: 5.5546\n",
      "Epoch 1 Step 20/36 Loss: 5.0484\n",
      "Epoch 1 Step 21/36 Loss: 6.1026\n",
      "Epoch 1 Step 22/36 Loss: 4.7330\n",
      "Epoch 1 Step 23/36 Loss: 4.6969\n",
      "Epoch 1 Step 24/36 Loss: 4.7364\n",
      "Epoch 1 Step 25/36 Loss: 5.4836\n",
      "Epoch 1 Step 26/36 Loss: 5.4941\n",
      "Epoch 1 Step 27/36 Loss: 5.3046\n",
      "Epoch 1 Step 28/36 Loss: 6.5854\n",
      "Epoch 1 Step 29/36 Loss: 4.9324\n",
      "Epoch 1 Step 30/36 Loss: 5.9413\n",
      "Epoch 1 Step 31/36 Loss: 5.6338\n",
      "Epoch 1 Step 32/36 Loss: 6.7462\n",
      "Epoch 1 Step 33/36 Loss: 5.9428\n",
      "Epoch 1 Step 34/36 Loss: 5.5295\n",
      "Epoch 1 Step 35/36 Loss: 5.6898\n",
      "Epoch 1 Step 36/36 Loss: 6.5676\n",
      "Epoch 1 Step 1/36 Loss: 7.0382\n",
      "Epoch 1 Step 2/36 Loss: 6.0238\n",
      "Epoch 1 Step 3/36 Loss: 5.2966\n",
      "Epoch 1 Step 4/36 Loss: 4.4225\n",
      "Epoch 1 Step 5/36 Loss: 4.5241\n",
      "Epoch 1 Step 6/36 Loss: 6.0030\n",
      "Epoch 1 Step 7/36 Loss: 7.5574\n",
      "Epoch 1 Step 8/36 Loss: 5.8010\n",
      "Epoch 1 Step 9/36 Loss: 5.1110\n",
      "Epoch 1 Step 10/36 Loss: 5.2869\n",
      "Epoch 1 Step 11/36 Loss: 5.1952\n",
      "Epoch 1 Step 12/36 Loss: 4.8843\n",
      "Epoch 1 Step 13/36 Loss: 5.7317\n",
      "Epoch 1 Step 14/36 Loss: 6.8584\n",
      "Epoch 1 Step 15/36 Loss: 6.3163\n",
      "Epoch 1 Step 16/36 Loss: 5.0370\n",
      "Epoch 1 Step 17/36 Loss: 5.0546\n",
      "Epoch 1 Step 18/36 Loss: 4.8259\n",
      "Epoch 1 Step 19/36 Loss: 4.3846\n",
      "Epoch 1 Step 20/36 Loss: 5.8170\n",
      "Epoch 1 Step 21/36 Loss: 5.3144\n",
      "Epoch 1 Step 22/36 Loss: 5.5269\n",
      "Epoch 1 Step 23/36 Loss: 4.7600\n",
      "Epoch 1 Step 24/36 Loss: 5.0974\n",
      "Epoch 1 Step 25/36 Loss: 4.9189\n",
      "Epoch 1 Step 26/36 Loss: 5.5727\n",
      "Epoch 1 Step 27/36 Loss: 5.8556\n",
      "Epoch 1 Step 28/36 Loss: 5.4654\n",
      "Epoch 1 Step 29/36 Loss: 5.6823\n",
      "Epoch 1 Step 30/36 Loss: 5.0893\n",
      "Epoch 1 Step 31/36 Loss: 5.8468\n",
      "Epoch 1 Step 32/36 Loss: 4.6316\n",
      "Epoch 1 Step 33/36 Loss: 5.5965\n",
      "Epoch 1 Step 34/36 Loss: 5.5292\n",
      "Epoch 1 Step 35/36 Loss: 5.3204\n",
      "Epoch 1 Step 36/36 Loss: 5.5680\n",
      "Epoch 1 Step 1/36 Loss: 4.8700\n",
      "Epoch 1 Step 2/36 Loss: 5.6788\n",
      "Epoch 1 Step 3/36 Loss: 5.5693\n",
      "Epoch 1 Step 4/36 Loss: 4.7100\n",
      "Epoch 1 Step 5/36 Loss: 4.4772\n",
      "Epoch 1 Step 6/36 Loss: 5.1739\n",
      "Epoch 1 Step 7/36 Loss: 5.1638\n",
      "Epoch 1 Step 8/36 Loss: 4.7671\n",
      "Epoch 1 Step 9/36 Loss: 5.5576\n",
      "Epoch 1 Step 10/36 Loss: 7.0956\n",
      "Epoch 1 Step 11/36 Loss: 4.3727\n",
      "Epoch 1 Step 12/36 Loss: 5.3104\n",
      "Epoch 1 Step 13/36 Loss: 4.9810\n",
      "Epoch 1 Step 14/36 Loss: 5.3085\n",
      "Epoch 1 Step 15/36 Loss: 5.0816\n",
      "Epoch 1 Step 16/36 Loss: 5.5106\n",
      "Epoch 1 Step 17/36 Loss: 5.5394\n",
      "Epoch 1 Step 18/36 Loss: 6.0818\n",
      "Epoch 1 Step 19/36 Loss: 5.1546\n",
      "Epoch 1 Step 20/36 Loss: 5.0860\n",
      "Epoch 1 Step 21/36 Loss: 5.6123\n",
      "Epoch 1 Step 22/36 Loss: 5.8065\n",
      "Epoch 1 Step 23/36 Loss: 4.6713\n",
      "Epoch 1 Step 24/36 Loss: 4.0932\n",
      "Epoch 1 Step 25/36 Loss: 5.2845\n",
      "Epoch 1 Step 26/36 Loss: 6.3548\n",
      "Epoch 1 Step 27/36 Loss: 6.0493\n",
      "Epoch 1 Step 28/36 Loss: 5.2780\n",
      "Epoch 1 Step 29/36 Loss: 6.1468\n",
      "Epoch 1 Step 30/36 Loss: 5.3719\n",
      "Epoch 1 Step 31/36 Loss: 5.0029\n",
      "Epoch 1 Step 32/36 Loss: 7.5781\n",
      "Epoch 1 Step 33/36 Loss: 6.3359\n",
      "Epoch 1 Step 34/36 Loss: 5.7050\n",
      "Epoch 1 Step 35/36 Loss: 4.7238\n",
      "Epoch 1 Step 36/36 Loss: 5.4473\n",
      "Epoch 1 Step 1/36 Loss: 5.5244\n",
      "Epoch 1 Step 2/36 Loss: 5.5977\n",
      "Epoch 1 Step 3/36 Loss: 5.5764\n",
      "Epoch 1 Step 4/36 Loss: 5.5109\n",
      "Epoch 1 Step 5/36 Loss: 3.8812\n",
      "Epoch 1 Step 6/36 Loss: 5.2220\n",
      "Epoch 1 Step 7/36 Loss: 5.0031\n",
      "Epoch 1 Step 8/36 Loss: 4.7599\n",
      "Epoch 1 Step 9/36 Loss: 5.5796\n",
      "Epoch 1 Step 10/36 Loss: 5.3975\n",
      "Epoch 1 Step 11/36 Loss: 5.5606\n",
      "Epoch 1 Step 12/36 Loss: 5.6618\n",
      "Epoch 1 Step 13/36 Loss: 4.9640\n",
      "Epoch 1 Step 14/36 Loss: 5.5471\n",
      "Epoch 1 Step 15/36 Loss: 7.2023\n",
      "Epoch 1 Step 16/36 Loss: 5.9973\n",
      "Epoch 1 Step 17/36 Loss: 5.2377\n",
      "Epoch 1 Step 18/36 Loss: 6.3652\n",
      "Epoch 1 Step 19/36 Loss: 4.8640\n",
      "Epoch 1 Step 20/36 Loss: 5.1010\n",
      "Epoch 1 Step 21/36 Loss: 6.3186\n",
      "Epoch 1 Step 22/36 Loss: 4.6685\n",
      "Epoch 1 Step 23/36 Loss: 7.8818\n",
      "Epoch 1 Step 24/36 Loss: 6.7403\n",
      "Epoch 1 Step 25/36 Loss: 5.9076\n",
      "Epoch 1 Step 26/36 Loss: 6.1433\n",
      "Epoch 1 Step 27/36 Loss: 5.2489\n",
      "Epoch 1 Step 28/36 Loss: 3.7391\n",
      "Epoch 1 Step 29/36 Loss: 6.4337\n",
      "Epoch 1 Step 30/36 Loss: 5.8859\n",
      "Epoch 1 Step 31/36 Loss: 5.1058\n",
      "Epoch 1 Step 32/36 Loss: 5.0521\n",
      "Epoch 1 Step 33/36 Loss: 6.3166\n",
      "Epoch 1 Step 34/36 Loss: 6.3762\n",
      "Epoch 1 Step 35/36 Loss: 6.1825\n",
      "Epoch 1 Step 36/36 Loss: 5.5130\n",
      "Epoch 1 Step 1/36 Loss: 5.5262\n",
      "Epoch 1 Step 2/36 Loss: 4.8488\n",
      "Epoch 1 Step 3/36 Loss: 7.0666\n",
      "Epoch 1 Step 4/36 Loss: 6.5191\n",
      "Epoch 1 Step 5/36 Loss: 6.4205\n",
      "Epoch 1 Step 6/36 Loss: 5.5246\n",
      "Epoch 1 Step 7/36 Loss: 6.2307\n",
      "Epoch 1 Step 8/36 Loss: 6.3233\n",
      "Epoch 1 Step 9/36 Loss: 7.1390\n",
      "Epoch 1 Step 10/36 Loss: 6.1064\n",
      "Epoch 1 Step 11/36 Loss: 7.3891\n",
      "Epoch 1 Step 12/36 Loss: 5.8326\n",
      "Epoch 1 Step 13/36 Loss: 5.5717\n",
      "Epoch 1 Step 14/36 Loss: 5.0030\n",
      "Epoch 1 Step 15/36 Loss: 4.9886\n",
      "Epoch 1 Step 16/36 Loss: 5.5472\n",
      "Epoch 1 Step 17/36 Loss: 5.7286\n",
      "Epoch 1 Step 18/36 Loss: 5.1092\n",
      "Epoch 1 Step 19/36 Loss: 5.5493\n",
      "Epoch 1 Step 20/36 Loss: 6.1263\n",
      "Epoch 1 Step 21/36 Loss: 5.5565\n",
      "Epoch 1 Step 22/36 Loss: 4.8068\n",
      "Epoch 1 Step 23/36 Loss: 5.6116\n",
      "Epoch 1 Step 24/36 Loss: 5.4100\n",
      "Epoch 1 Step 25/36 Loss: 6.0908\n",
      "Epoch 1 Step 26/36 Loss: 5.8876\n",
      "Epoch 1 Step 27/36 Loss: 5.5864\n",
      "Epoch 1 Step 28/36 Loss: 5.8691\n",
      "Epoch 1 Step 29/36 Loss: 6.2049\n",
      "Epoch 1 Step 30/36 Loss: 6.0595\n",
      "Epoch 1 Step 31/36 Loss: 4.0739\n",
      "Epoch 1 Step 32/36 Loss: 5.4585\n",
      "Epoch 1 Step 33/36 Loss: 5.7206\n",
      "Epoch 1 Step 34/36 Loss: 5.1718\n",
      "Epoch 1 Step 35/36 Loss: 5.8573\n",
      "Epoch 1 Step 36/36 Loss: 5.3969\n",
      "Epoch 1 Step 1/36 Loss: 6.2480\n",
      "Epoch 1 Step 2/36 Loss: 6.1839\n",
      "Epoch 1 Step 3/36 Loss: 5.9594\n",
      "Epoch 1 Step 4/36 Loss: 7.1522\n",
      "Epoch 1 Step 5/36 Loss: 6.7316\n",
      "Epoch 1 Step 6/36 Loss: 5.1829\n",
      "Epoch 1 Step 7/36 Loss: 5.4792\n",
      "Epoch 1 Step 8/36 Loss: 5.1853\n",
      "Epoch 1 Step 9/36 Loss: 5.0802\n",
      "Epoch 1 Step 10/36 Loss: 4.6966\n",
      "Epoch 1 Step 11/36 Loss: 4.9791\n",
      "Epoch 1 Step 12/36 Loss: 5.6274\n",
      "Epoch 1 Step 13/36 Loss: 6.8678\n",
      "Epoch 1 Step 14/36 Loss: 4.8623\n",
      "Epoch 1 Step 15/36 Loss: 6.4671\n",
      "Epoch 1 Step 16/36 Loss: 5.8312\n",
      "Epoch 1 Step 17/36 Loss: 5.0075\n",
      "Epoch 1 Step 18/36 Loss: 4.7843\n",
      "Epoch 1 Step 19/36 Loss: 6.4209\n",
      "Epoch 1 Step 20/36 Loss: 4.1089\n",
      "Epoch 1 Step 21/36 Loss: 4.5860\n",
      "Epoch 1 Step 22/36 Loss: 6.3779\n",
      "Epoch 1 Step 23/36 Loss: 4.7348\n",
      "Epoch 1 Step 24/36 Loss: 5.8829\n",
      "Epoch 1 Step 25/36 Loss: 5.5460\n",
      "Epoch 1 Step 26/36 Loss: 4.6320\n",
      "Epoch 1 Step 27/36 Loss: 5.5588\n",
      "Epoch 1 Step 28/36 Loss: 4.6365\n",
      "Epoch 1 Step 29/36 Loss: 5.8622\n",
      "Epoch 1 Step 30/36 Loss: 5.1471\n",
      "Epoch 1 Step 31/36 Loss: 4.5725\n",
      "Epoch 1 Step 32/36 Loss: 5.0823\n",
      "Epoch 1 Step 33/36 Loss: 5.5628\n",
      "Epoch 1 Step 34/36 Loss: 5.9963\n",
      "Epoch 1 Step 35/36 Loss: 4.2228\n",
      "Epoch 1 Step 36/36 Loss: 5.6118\n",
      "Epoch 1 Step 1/36 Loss: 5.3071\n",
      "Epoch 1 Step 2/36 Loss: 5.8439\n",
      "Epoch 1 Step 3/36 Loss: 4.7768\n",
      "Epoch 1 Step 4/36 Loss: 4.8348\n",
      "Epoch 1 Step 5/36 Loss: 6.2431\n",
      "Epoch 1 Step 6/36 Loss: 5.3929\n",
      "Epoch 1 Step 7/36 Loss: 4.6773\n",
      "Epoch 1 Step 8/36 Loss: 3.9019\n",
      "Epoch 1 Step 9/36 Loss: 6.9740\n",
      "Epoch 1 Step 10/36 Loss: 5.6432\n",
      "Epoch 1 Step 11/36 Loss: 6.4216\n",
      "Epoch 1 Step 12/36 Loss: 4.5553\n",
      "Epoch 1 Step 13/36 Loss: 5.3672\n",
      "Epoch 1 Step 14/36 Loss: 6.2058\n",
      "Epoch 1 Step 15/36 Loss: 5.4634\n",
      "Epoch 1 Step 16/36 Loss: 3.9899\n",
      "Epoch 1 Step 17/36 Loss: 5.6445\n",
      "Epoch 1 Step 18/36 Loss: 4.3880\n",
      "Epoch 1 Step 19/36 Loss: 6.3203\n",
      "Epoch 1 Step 20/36 Loss: 4.8684\n",
      "Epoch 1 Step 21/36 Loss: 5.2785\n",
      "Epoch 1 Step 22/36 Loss: 5.7798\n",
      "Epoch 1 Step 23/36 Loss: 5.5283\n",
      "Epoch 1 Step 24/36 Loss: 4.3773\n",
      "Epoch 1 Step 25/36 Loss: 5.5525\n",
      "Epoch 1 Step 26/36 Loss: 5.6658\n",
      "Epoch 1 Step 27/36 Loss: 5.2606\n",
      "Epoch 1 Step 28/36 Loss: 4.8172\n",
      "Epoch 1 Step 29/36 Loss: 5.7517\n",
      "Epoch 1 Step 30/36 Loss: 4.8234\n",
      "Epoch 1 Step 31/36 Loss: 4.6042\n",
      "Epoch 1 Step 32/36 Loss: 4.8097\n",
      "Epoch 1 Step 33/36 Loss: 4.5264\n",
      "Epoch 1 Step 34/36 Loss: 6.5038\n",
      "Epoch 1 Step 35/36 Loss: 5.4554\n",
      "Epoch 1 Step 36/36 Loss: 5.5933\n",
      "Epoch 1 Step 1/36 Loss: 3.7769\n",
      "Epoch 1 Step 2/36 Loss: 7.0356\n",
      "Epoch 1 Step 3/36 Loss: 5.2204\n",
      "Epoch 1 Step 4/36 Loss: 6.9708\n",
      "Epoch 1 Step 5/36 Loss: 5.8017\n",
      "Epoch 1 Step 6/36 Loss: 5.4439\n",
      "Epoch 1 Step 7/36 Loss: 5.8280\n",
      "Epoch 1 Step 8/36 Loss: 4.8862\n",
      "Epoch 1 Step 9/36 Loss: 6.5758\n",
      "Epoch 1 Step 10/36 Loss: 4.4739\n",
      "Epoch 1 Step 11/36 Loss: 6.9778\n",
      "Epoch 1 Step 12/36 Loss: 5.0141\n",
      "Epoch 1 Step 13/36 Loss: 5.3830\n",
      "Epoch 1 Step 14/36 Loss: 5.1460\n",
      "Epoch 1 Step 15/36 Loss: 4.4151\n",
      "Epoch 1 Step 16/36 Loss: 5.0930\n",
      "Epoch 1 Step 17/36 Loss: 4.9464\n",
      "Epoch 1 Step 18/36 Loss: 6.3782\n",
      "Epoch 1 Step 19/36 Loss: 6.0385\n",
      "Epoch 1 Step 20/36 Loss: 3.7605\n",
      "Epoch 1 Step 21/36 Loss: 5.9628\n",
      "Epoch 1 Step 22/36 Loss: 5.4596\n",
      "Epoch 1 Step 23/36 Loss: 6.2892\n",
      "Epoch 1 Step 24/36 Loss: 5.8504\n",
      "Epoch 1 Step 25/36 Loss: 4.7203\n",
      "Epoch 1 Step 26/36 Loss: 6.2221\n",
      "Epoch 1 Step 27/36 Loss: 4.6434\n",
      "Epoch 1 Step 28/36 Loss: 5.1700\n",
      "Epoch 1 Step 29/36 Loss: 5.8412\n",
      "Epoch 1 Step 30/36 Loss: 5.7437\n",
      "Epoch 1 Step 31/36 Loss: 6.9302\n",
      "Epoch 1 Step 32/36 Loss: 4.5285\n",
      "Epoch 1 Step 33/36 Loss: 6.4887\n",
      "Epoch 1 Step 34/36 Loss: 5.0823\n",
      "Epoch 1 Step 35/36 Loss: 5.0419\n",
      "Epoch 1 Step 36/36 Loss: 5.5860\n",
      "Epoch 1 Step 1/36 Loss: 4.8070\n",
      "Epoch 1 Step 2/36 Loss: 4.2014\n",
      "Epoch 1 Step 3/36 Loss: 5.2454\n",
      "Epoch 1 Step 4/36 Loss: 6.7885\n",
      "Epoch 1 Step 5/36 Loss: 4.8573\n",
      "Epoch 1 Step 6/36 Loss: 7.3552\n",
      "Epoch 1 Step 7/36 Loss: 5.8555\n",
      "Epoch 1 Step 8/36 Loss: 3.8618\n",
      "Epoch 1 Step 9/36 Loss: 5.1410\n",
      "Epoch 1 Step 10/36 Loss: 4.4274\n",
      "Epoch 1 Step 11/36 Loss: 5.1134\n",
      "Epoch 1 Step 12/36 Loss: 6.1966\n",
      "Epoch 1 Step 13/36 Loss: 4.0676\n",
      "Epoch 1 Step 14/36 Loss: 6.0290\n",
      "Epoch 1 Step 15/36 Loss: 5.1078\n",
      "Epoch 1 Step 16/36 Loss: 5.4791\n",
      "Epoch 1 Step 17/36 Loss: 5.5316\n",
      "Epoch 1 Step 18/36 Loss: 5.7108\n",
      "Epoch 1 Step 19/36 Loss: 6.2678\n",
      "Epoch 1 Step 20/36 Loss: 4.7382\n",
      "Epoch 1 Step 21/36 Loss: 6.2369\n",
      "Epoch 1 Step 22/36 Loss: 5.6448\n",
      "Epoch 1 Step 23/36 Loss: 4.6460\n",
      "Epoch 1 Step 24/36 Loss: 7.1522\n",
      "Epoch 1 Step 25/36 Loss: 6.1016\n",
      "Epoch 1 Step 26/36 Loss: 6.0857\n",
      "Epoch 1 Step 27/36 Loss: 4.2668\n",
      "Epoch 1 Step 28/36 Loss: 5.4435\n",
      "Epoch 1 Step 29/36 Loss: 5.1152\n",
      "Epoch 1 Step 30/36 Loss: 4.5858\n",
      "Epoch 1 Step 31/36 Loss: 5.5529\n",
      "Epoch 1 Step 32/36 Loss: 6.1643\n",
      "Epoch 1 Step 33/36 Loss: 4.8150\n",
      "Epoch 1 Step 34/36 Loss: 4.8152\n",
      "Epoch 1 Step 35/36 Loss: 5.1105\n",
      "Epoch 1 Step 36/36 Loss: 6.0050\n",
      "Epoch 1 Step 1/36 Loss: 5.5079\n",
      "Epoch 1 Step 2/36 Loss: 4.6886\n",
      "Epoch 1 Step 3/36 Loss: 5.7754\n",
      "Epoch 1 Step 4/36 Loss: 5.0634\n",
      "Epoch 1 Step 5/36 Loss: 6.4012\n",
      "Epoch 1 Step 6/36 Loss: 5.4583\n",
      "Epoch 1 Step 7/36 Loss: 4.6067\n",
      "Epoch 1 Step 8/36 Loss: 5.4395\n",
      "Epoch 1 Step 9/36 Loss: 4.6377\n",
      "Epoch 1 Step 10/36 Loss: 4.9096\n",
      "Epoch 1 Step 11/36 Loss: 5.3435\n",
      "Epoch 1 Step 12/36 Loss: 4.4240\n",
      "Epoch 1 Step 13/36 Loss: 6.6375\n",
      "Epoch 1 Step 14/36 Loss: 5.8236\n",
      "Epoch 1 Step 15/36 Loss: 7.0655\n",
      "Epoch 1 Step 16/36 Loss: 7.6643\n",
      "Epoch 1 Step 17/36 Loss: 5.2588\n",
      "Epoch 1 Step 18/36 Loss: 5.1378\n",
      "Epoch 1 Step 19/36 Loss: 4.5971\n",
      "Epoch 1 Step 20/36 Loss: 5.3443\n",
      "Epoch 1 Step 21/36 Loss: 4.6187\n",
      "Epoch 1 Step 22/36 Loss: 5.1719\n",
      "Epoch 1 Step 23/36 Loss: 3.8040\n",
      "Epoch 1 Step 24/36 Loss: 4.0085\n",
      "Epoch 1 Step 25/36 Loss: 7.5030\n",
      "Epoch 1 Step 26/36 Loss: 6.7758\n",
      "Epoch 1 Step 27/36 Loss: 4.4524\n",
      "Epoch 1 Step 28/36 Loss: 6.4090\n",
      "Epoch 1 Step 29/36 Loss: 4.9688\n",
      "Epoch 1 Step 30/36 Loss: 4.0544\n",
      "Epoch 1 Step 31/36 Loss: 5.1868\n",
      "Epoch 1 Step 32/36 Loss: 5.3298\n",
      "Epoch 1 Step 33/36 Loss: 6.1488\n",
      "Epoch 1 Step 34/36 Loss: 6.9606\n",
      "Epoch 1 Step 35/36 Loss: 7.5215\n",
      "Epoch 1 Step 36/36 Loss: 5.0504\n",
      "Epoch 1 Step 1/36 Loss: 5.3026\n",
      "Epoch 1 Step 2/36 Loss: 6.3354\n",
      "Epoch 1 Step 3/36 Loss: 4.9253\n",
      "Epoch 1 Step 4/36 Loss: 5.1477\n",
      "Epoch 1 Step 5/36 Loss: 4.8755\n",
      "Epoch 1 Step 6/36 Loss: 6.0135\n",
      "Epoch 1 Step 7/36 Loss: 5.0889\n",
      "Epoch 1 Step 8/36 Loss: 7.0684\n",
      "Epoch 1 Step 9/36 Loss: 4.8700\n",
      "Epoch 1 Step 10/36 Loss: 4.7719\n",
      "Epoch 1 Step 11/36 Loss: 5.4193\n",
      "Epoch 1 Step 12/36 Loss: 6.1670\n",
      "Epoch 1 Step 13/36 Loss: 4.5206\n",
      "Epoch 1 Step 14/36 Loss: 5.4127\n",
      "Epoch 1 Step 15/36 Loss: 4.8298\n",
      "Epoch 1 Step 16/36 Loss: 4.6679\n",
      "Epoch 1 Step 17/36 Loss: 5.2831\n",
      "Epoch 1 Step 18/36 Loss: 4.2474\n",
      "Epoch 1 Step 19/36 Loss: 5.6541\n",
      "Epoch 1 Step 20/36 Loss: 7.5072\n",
      "Epoch 1 Step 21/36 Loss: 4.5213\n",
      "Epoch 1 Step 22/36 Loss: 4.1293\n",
      "Epoch 1 Step 23/36 Loss: 8.0046\n",
      "Epoch 1 Step 24/36 Loss: 5.2277\n",
      "Epoch 1 Step 25/36 Loss: 5.8652\n",
      "Epoch 1 Step 26/36 Loss: 5.3390\n",
      "Epoch 1 Step 27/36 Loss: 5.3157\n",
      "Epoch 1 Step 28/36 Loss: 6.0668\n",
      "Epoch 1 Step 29/36 Loss: 4.7321\n",
      "Epoch 1 Step 30/36 Loss: 5.7031\n",
      "Epoch 1 Step 31/36 Loss: 4.8686\n",
      "Epoch 1 Step 32/36 Loss: 6.4526\n",
      "Epoch 1 Step 33/36 Loss: 6.5274\n",
      "Epoch 1 Step 34/36 Loss: 5.5209\n",
      "Epoch 1 Step 35/36 Loss: 5.0184\n",
      "Epoch 1 Step 36/36 Loss: 5.1024\n",
      "Epoch 1 Step 1/36 Loss: 5.4685\n",
      "Epoch 1 Step 2/36 Loss: 5.2711\n",
      "Epoch 1 Step 3/36 Loss: 7.1809\n",
      "Epoch 1 Step 4/36 Loss: 5.2275\n",
      "Epoch 1 Step 5/36 Loss: 5.3608\n",
      "Epoch 1 Step 6/36 Loss: 5.2039\n",
      "Epoch 1 Step 7/36 Loss: 4.8135\n",
      "Epoch 1 Step 8/36 Loss: 5.2717\n",
      "Epoch 1 Step 9/36 Loss: 5.1146\n",
      "Epoch 1 Step 10/36 Loss: 6.7353\n",
      "Epoch 1 Step 11/36 Loss: 5.9250\n",
      "Epoch 1 Step 12/36 Loss: 5.6680\n",
      "Epoch 1 Step 13/36 Loss: 7.4774\n",
      "Epoch 1 Step 14/36 Loss: 4.9273\n",
      "Epoch 1 Step 15/36 Loss: 6.3038\n",
      "Epoch 1 Step 16/36 Loss: 4.8782\n",
      "Epoch 1 Step 17/36 Loss: 5.2391\n",
      "Epoch 1 Step 18/36 Loss: 5.4553\n",
      "Epoch 1 Step 19/36 Loss: 4.4257\n",
      "Epoch 1 Step 20/36 Loss: 4.5290\n",
      "Epoch 1 Step 21/36 Loss: 4.8504\n",
      "Epoch 1 Step 22/36 Loss: 6.1138\n",
      "Epoch 1 Step 23/36 Loss: 4.5908\n",
      "Epoch 1 Step 24/36 Loss: 5.2348\n",
      "Epoch 1 Step 25/36 Loss: 6.2065\n",
      "Epoch 1 Step 26/36 Loss: 5.5583\n",
      "Epoch 1 Step 27/36 Loss: 6.2562\n",
      "Epoch 1 Step 28/36 Loss: 4.7029\n",
      "Epoch 1 Step 29/36 Loss: 5.3982\n",
      "Epoch 1 Step 30/36 Loss: 6.7985\n",
      "Epoch 1 Step 31/36 Loss: 4.3172\n",
      "Epoch 1 Step 32/36 Loss: 5.1868\n",
      "Epoch 1 Step 33/36 Loss: 5.0550\n",
      "Epoch 1 Step 34/36 Loss: 4.9131\n",
      "Epoch 1 Step 35/36 Loss: 4.4367\n",
      "Epoch 1 Step 36/36 Loss: 4.5917\n",
      "Epoch 1 Step 1/36 Loss: 5.9363\n",
      "Epoch 1 Step 2/36 Loss: 6.8064\n",
      "Epoch 1 Step 3/36 Loss: 5.3522\n",
      "Epoch 1 Step 4/36 Loss: 5.0789\n",
      "Epoch 1 Step 5/36 Loss: 6.6278\n",
      "Epoch 1 Step 6/36 Loss: 4.8805\n",
      "Epoch 1 Step 7/36 Loss: 6.1717\n",
      "Epoch 1 Step 8/36 Loss: 6.0524\n",
      "Epoch 1 Step 9/36 Loss: 5.9930\n",
      "Epoch 1 Step 10/36 Loss: 4.6866\n",
      "Epoch 1 Step 11/36 Loss: 5.2909\n",
      "Epoch 1 Step 12/36 Loss: 7.3354\n",
      "Epoch 1 Step 13/36 Loss: 5.8448\n",
      "Epoch 1 Step 14/36 Loss: 5.1220\n",
      "Epoch 1 Step 15/36 Loss: 5.0212\n",
      "Epoch 1 Step 16/36 Loss: 5.4978\n",
      "Epoch 1 Step 17/36 Loss: 5.2163\n",
      "Epoch 1 Step 18/36 Loss: 5.4679\n",
      "Epoch 1 Step 19/36 Loss: 5.2008\n",
      "Epoch 1 Step 20/36 Loss: 5.0753\n",
      "Epoch 1 Step 21/36 Loss: 5.8623\n",
      "Epoch 1 Step 22/36 Loss: 4.1121\n",
      "Epoch 1 Step 23/36 Loss: 4.2535\n",
      "Epoch 1 Step 24/36 Loss: 6.1558\n",
      "Epoch 1 Step 25/36 Loss: 5.3612\n",
      "Epoch 1 Step 26/36 Loss: 5.4148\n",
      "Epoch 1 Step 27/36 Loss: 4.7626\n",
      "Epoch 1 Step 28/36 Loss: 6.0235\n",
      "Epoch 1 Step 29/36 Loss: 5.8722\n",
      "Epoch 1 Step 30/36 Loss: 5.8924\n",
      "Epoch 1 Step 31/36 Loss: 5.3890\n",
      "Epoch 1 Step 32/36 Loss: 4.4493\n",
      "Epoch 1 Step 33/36 Loss: 6.5318\n",
      "Epoch 1 Step 34/36 Loss: 6.3310\n",
      "Epoch 1 Step 35/36 Loss: 5.5147\n",
      "Epoch 1 Step 36/36 Loss: 5.0433\n",
      "Epoch 1 Step 1/36 Loss: 5.5522\n",
      "Epoch 1 Step 2/36 Loss: 3.9875\n",
      "Epoch 1 Step 3/36 Loss: 4.9578\n",
      "Epoch 1 Step 4/36 Loss: 4.7734\n",
      "Epoch 1 Step 5/36 Loss: 4.7615\n",
      "Epoch 1 Step 6/36 Loss: 6.8964\n",
      "Epoch 1 Step 7/36 Loss: 4.9711\n",
      "Epoch 1 Step 8/36 Loss: 5.9518\n",
      "Epoch 1 Step 9/36 Loss: 6.7405\n",
      "Epoch 1 Step 10/36 Loss: 6.0462\n",
      "Epoch 1 Step 11/36 Loss: 6.5275\n",
      "Epoch 1 Step 12/36 Loss: 4.9801\n",
      "Epoch 1 Step 13/36 Loss: 5.0484\n",
      "Epoch 1 Step 14/36 Loss: 6.2326\n",
      "Epoch 1 Step 15/36 Loss: 5.7028\n",
      "Epoch 1 Step 16/36 Loss: 5.5879\n",
      "Epoch 1 Step 17/36 Loss: 4.7919\n",
      "Epoch 1 Step 18/36 Loss: 5.5893\n",
      "Epoch 1 Step 19/36 Loss: 5.0751\n",
      "Epoch 1 Step 20/36 Loss: 4.7496\n",
      "Epoch 1 Step 21/36 Loss: 5.8873\n",
      "Epoch 1 Step 22/36 Loss: 5.6607\n",
      "Epoch 1 Step 23/36 Loss: 6.0175\n",
      "Epoch 1 Step 24/36 Loss: 5.0688\n",
      "Epoch 1 Step 25/36 Loss: 5.6015\n",
      "Epoch 1 Step 26/36 Loss: 3.5995\n",
      "Epoch 1 Step 27/36 Loss: 6.3033\n",
      "Epoch 1 Step 28/36 Loss: 6.2155\n",
      "Epoch 1 Step 29/36 Loss: 6.0663\n",
      "Epoch 1 Step 30/36 Loss: 4.3836\n",
      "Epoch 1 Step 31/36 Loss: 5.4189\n",
      "Epoch 1 Step 32/36 Loss: 4.7733\n",
      "Epoch 1 Step 33/36 Loss: 6.3426\n",
      "Epoch 1 Step 34/36 Loss: 5.3498\n",
      "Epoch 1 Step 35/36 Loss: 5.5866\n",
      "Epoch 1 Step 36/36 Loss: 5.5443\n",
      "Epoch 1 Step 1/36 Loss: 5.8363\n",
      "Epoch 1 Step 2/36 Loss: 5.3545\n",
      "Epoch 1 Step 3/36 Loss: 6.4973\n",
      "Epoch 1 Step 4/36 Loss: 5.9914\n",
      "Epoch 1 Step 5/36 Loss: 6.0165\n",
      "Epoch 1 Step 6/36 Loss: 6.0991\n",
      "Epoch 1 Step 7/36 Loss: 5.5834\n",
      "Epoch 1 Step 8/36 Loss: 5.2237\n",
      "Epoch 1 Step 9/36 Loss: 6.3964\n",
      "Epoch 1 Step 10/36 Loss: 4.5487\n",
      "Epoch 1 Step 11/36 Loss: 4.4535\n",
      "Epoch 1 Step 12/36 Loss: 5.2927\n",
      "Epoch 1 Step 13/36 Loss: 5.9320\n",
      "Epoch 1 Step 14/36 Loss: 6.1284\n",
      "Epoch 1 Step 15/36 Loss: 5.5685\n",
      "Epoch 1 Step 16/36 Loss: 5.1599\n",
      "Epoch 1 Step 17/36 Loss: 5.5815\n",
      "Epoch 1 Step 18/36 Loss: 5.5956\n",
      "Epoch 1 Step 19/36 Loss: 5.5680\n",
      "Epoch 1 Step 20/36 Loss: 4.9200\n",
      "Epoch 1 Step 21/36 Loss: 5.6637\n",
      "Epoch 1 Step 22/36 Loss: 4.8503\n",
      "Epoch 1 Step 23/36 Loss: 5.6245\n",
      "Epoch 1 Step 24/36 Loss: 5.8100\n",
      "Epoch 1 Step 25/36 Loss: 4.6346\n",
      "Epoch 1 Step 26/36 Loss: 6.7594\n",
      "Epoch 1 Step 27/36 Loss: 5.4524\n",
      "Epoch 1 Step 28/36 Loss: 6.0115\n",
      "Epoch 1 Step 29/36 Loss: 5.8184\n",
      "Epoch 1 Step 30/36 Loss: 5.3405\n",
      "Epoch 1 Step 31/36 Loss: 5.1747\n",
      "Epoch 1 Step 32/36 Loss: 6.6262\n",
      "Epoch 1 Step 33/36 Loss: 5.7128\n",
      "Epoch 1 Step 34/36 Loss: 3.6536\n",
      "Epoch 1 Step 35/36 Loss: 5.4235\n",
      "Epoch 1 Step 36/36 Loss: 4.6218\n",
      "Epoch 1 Step 1/36 Loss: 5.9402\n",
      "Epoch 1 Step 2/36 Loss: 4.7151\n",
      "Epoch 1 Step 3/36 Loss: 4.7817\n",
      "Epoch 1 Step 4/36 Loss: 5.3383\n",
      "Epoch 1 Step 5/36 Loss: 5.4984\n",
      "Epoch 1 Step 6/36 Loss: 5.2543\n",
      "Epoch 1 Step 7/36 Loss: 4.7630\n",
      "Epoch 1 Step 8/36 Loss: 4.3912\n",
      "Epoch 1 Step 9/36 Loss: 5.9196\n",
      "Epoch 1 Step 10/36 Loss: 6.3057\n",
      "Epoch 1 Step 11/36 Loss: 5.3529\n",
      "Epoch 1 Step 12/36 Loss: 5.1593\n",
      "Epoch 1 Step 13/36 Loss: 4.7283\n",
      "Epoch 1 Step 14/36 Loss: 4.2248\n",
      "Epoch 1 Step 15/36 Loss: 4.7750\n",
      "Epoch 1 Step 16/36 Loss: 6.3684\n",
      "Epoch 1 Step 17/36 Loss: 4.9964\n",
      "Epoch 1 Step 18/36 Loss: 6.5618\n",
      "Epoch 1 Step 19/36 Loss: 5.5091\n",
      "Epoch 1 Step 20/36 Loss: 5.9223\n",
      "Epoch 1 Step 21/36 Loss: 4.3667\n",
      "Epoch 1 Step 22/36 Loss: 5.5026\n",
      "Epoch 1 Step 23/36 Loss: 5.2372\n",
      "Epoch 1 Step 24/36 Loss: 4.9756\n",
      "Epoch 1 Step 25/36 Loss: 6.6543\n",
      "Epoch 1 Step 26/36 Loss: 5.5316\n",
      "Epoch 1 Step 27/36 Loss: 6.4523\n",
      "Epoch 1 Step 28/36 Loss: 6.3333\n",
      "Epoch 1 Step 29/36 Loss: 5.5928\n",
      "Epoch 1 Step 30/36 Loss: 6.1874\n",
      "Epoch 1 Step 31/36 Loss: 5.5362\n",
      "Epoch 1 Step 32/36 Loss: 5.7929\n",
      "Epoch 1 Step 33/36 Loss: 5.8166\n",
      "Epoch 1 Step 34/36 Loss: 6.3217\n",
      "Epoch 1 Step 35/36 Loss: 4.6625\n",
      "Epoch 1 Step 36/36 Loss: 5.2416\n",
      "Epoch 1 Step 1/36 Loss: 5.8022\n",
      "Epoch 1 Step 2/36 Loss: 5.6245\n",
      "Epoch 1 Step 3/36 Loss: 5.8836\n",
      "Epoch 1 Step 4/36 Loss: 4.5265\n",
      "Epoch 1 Step 5/36 Loss: 6.2085\n",
      "Epoch 1 Step 6/36 Loss: 5.1981\n",
      "Epoch 1 Step 7/36 Loss: 5.1898\n",
      "Epoch 1 Step 8/36 Loss: 5.9039\n",
      "Epoch 1 Step 9/36 Loss: 4.7211\n",
      "Epoch 1 Step 10/36 Loss: 5.6106\n",
      "Epoch 1 Step 11/36 Loss: 6.6776\n",
      "Epoch 1 Step 12/36 Loss: 5.4400\n",
      "Epoch 1 Step 13/36 Loss: 5.5822\n",
      "Epoch 1 Step 14/36 Loss: 6.9117\n",
      "Epoch 1 Step 15/36 Loss: 5.8060\n",
      "Epoch 1 Step 16/36 Loss: 5.7554\n",
      "Epoch 1 Step 17/36 Loss: 4.9693\n",
      "Epoch 1 Step 18/36 Loss: 4.8378\n",
      "Epoch 1 Step 19/36 Loss: 6.7216\n",
      "Epoch 1 Step 20/36 Loss: 6.1186\n",
      "Epoch 1 Step 21/36 Loss: 4.2193\n",
      "Epoch 1 Step 22/36 Loss: 5.0388\n",
      "Epoch 1 Step 23/36 Loss: 4.5317\n",
      "Epoch 1 Step 24/36 Loss: 6.4078\n",
      "Epoch 1 Step 25/36 Loss: 6.2177\n",
      "Epoch 1 Step 26/36 Loss: 5.7453\n",
      "Epoch 1 Step 27/36 Loss: 5.9253\n",
      "Epoch 1 Step 28/36 Loss: 4.8918\n",
      "Epoch 1 Step 29/36 Loss: 5.4224\n",
      "Epoch 1 Step 30/36 Loss: 4.7462\n",
      "Epoch 1 Step 31/36 Loss: 5.0955\n",
      "Epoch 1 Step 32/36 Loss: 4.9424\n",
      "Epoch 1 Step 33/36 Loss: 4.3702\n",
      "Epoch 1 Step 34/36 Loss: 5.9656\n",
      "Epoch 1 Step 35/36 Loss: 6.6819\n",
      "Epoch 1 Step 36/36 Loss: 5.6419\n",
      "Epoch 1 Step 1/36 Loss: 5.3224\n",
      "Epoch 1 Step 2/36 Loss: 6.3549\n",
      "Epoch 1 Step 3/36 Loss: 4.9389\n",
      "Epoch 1 Step 4/36 Loss: 6.2430\n",
      "Epoch 1 Step 5/36 Loss: 4.6275\n",
      "Epoch 1 Step 6/36 Loss: 4.2589\n",
      "Epoch 1 Step 7/36 Loss: 5.5254\n",
      "Epoch 1 Step 8/36 Loss: 5.0349\n",
      "Epoch 1 Step 9/36 Loss: 4.9007\n",
      "Epoch 1 Step 10/36 Loss: 5.1951\n",
      "Epoch 1 Step 11/36 Loss: 5.1267\n",
      "Epoch 1 Step 12/36 Loss: 5.1721\n",
      "Epoch 1 Step 13/36 Loss: 4.1459\n",
      "Epoch 1 Step 14/36 Loss: 4.6087\n",
      "Epoch 1 Step 15/36 Loss: 5.9182\n",
      "Epoch 1 Step 16/36 Loss: 4.7655\n",
      "Epoch 1 Step 17/36 Loss: 6.4419\n",
      "Epoch 1 Step 18/36 Loss: 5.2069\n",
      "Epoch 1 Step 19/36 Loss: 5.6858\n",
      "Epoch 1 Step 20/36 Loss: 6.4359\n",
      "Epoch 1 Step 21/36 Loss: 4.8168\n",
      "Epoch 1 Step 22/36 Loss: 5.1288\n",
      "Epoch 1 Step 23/36 Loss: 5.7699\n",
      "Epoch 1 Step 24/36 Loss: 4.4921\n",
      "Epoch 1 Step 25/36 Loss: 5.2513\n",
      "Epoch 1 Step 26/36 Loss: 4.7711\n",
      "Epoch 1 Step 27/36 Loss: 5.1697\n",
      "Epoch 1 Step 28/36 Loss: 4.3697\n",
      "Epoch 1 Step 29/36 Loss: 6.0732\n",
      "Epoch 1 Step 30/36 Loss: 4.8362\n",
      "Epoch 1 Step 31/36 Loss: 4.8471\n",
      "Epoch 1 Step 32/36 Loss: 6.1464\n",
      "Epoch 1 Step 33/36 Loss: 5.2618\n",
      "Epoch 1 Step 34/36 Loss: 6.0471\n",
      "Epoch 1 Step 35/36 Loss: 8.0418\n",
      "Epoch 1 Step 36/36 Loss: 5.2932\n",
      "Epoch 1 Step 1/36 Loss: 5.1869\n",
      "Epoch 1 Step 2/36 Loss: 5.8572\n",
      "Epoch 1 Step 3/36 Loss: 5.7549\n",
      "Epoch 1 Step 4/36 Loss: 4.9890\n",
      "Epoch 1 Step 5/36 Loss: 4.7624\n",
      "Epoch 1 Step 6/36 Loss: 5.9379\n",
      "Epoch 1 Step 7/36 Loss: 5.3786\n",
      "Epoch 1 Step 8/36 Loss: 4.7240\n",
      "Epoch 1 Step 9/36 Loss: 6.1607\n",
      "Epoch 1 Step 10/36 Loss: 4.7407\n",
      "Epoch 1 Step 11/36 Loss: 5.8179\n",
      "Epoch 1 Step 12/36 Loss: 4.7096\n",
      "Epoch 1 Step 13/36 Loss: 4.7701\n",
      "Epoch 1 Step 14/36 Loss: 5.7812\n",
      "Epoch 1 Step 15/36 Loss: 5.1353\n",
      "Epoch 1 Step 16/36 Loss: 5.8510\n",
      "Epoch 1 Step 17/36 Loss: 6.4445\n",
      "Epoch 1 Step 18/36 Loss: 4.9809\n",
      "Epoch 1 Step 19/36 Loss: 6.2343\n",
      "Epoch 1 Step 20/36 Loss: 4.4595\n",
      "Epoch 1 Step 21/36 Loss: 5.9789\n",
      "Epoch 1 Step 22/36 Loss: 5.0292\n",
      "Epoch 1 Step 23/36 Loss: 4.2170\n",
      "Epoch 1 Step 24/36 Loss: 6.1056\n",
      "Epoch 1 Step 25/36 Loss: 5.5521\n",
      "Epoch 1 Step 26/36 Loss: 6.7572\n",
      "Epoch 1 Step 27/36 Loss: 5.1470\n",
      "Epoch 1 Step 28/36 Loss: 4.8856\n",
      "Epoch 1 Step 29/36 Loss: 5.4945\n",
      "Epoch 1 Step 30/36 Loss: 4.8580\n",
      "Epoch 1 Step 31/36 Loss: 6.0659\n",
      "Epoch 1 Step 32/36 Loss: 5.1024\n",
      "Epoch 1 Step 33/36 Loss: 5.7538\n",
      "Epoch 1 Step 34/36 Loss: 5.7368\n",
      "Epoch 1 Step 35/36 Loss: 6.2791\n",
      "Epoch 1 Step 36/36 Loss: 4.5220\n",
      "Epoch 1 Step 1/36 Loss: 5.8517\n",
      "Epoch 1 Step 2/36 Loss: 5.7940\n",
      "Epoch 1 Step 3/36 Loss: 5.2192\n",
      "Epoch 1 Step 4/36 Loss: 5.1676\n",
      "Epoch 1 Step 5/36 Loss: 4.7634\n",
      "Epoch 1 Step 6/36 Loss: 4.5120\n",
      "Epoch 1 Step 7/36 Loss: 4.6147\n",
      "Epoch 1 Step 8/36 Loss: 6.4414\n",
      "Epoch 1 Step 9/36 Loss: 5.1595\n",
      "Epoch 1 Step 10/36 Loss: 3.7997\n",
      "Epoch 1 Step 11/36 Loss: 5.2514\n",
      "Epoch 1 Step 12/36 Loss: 4.9269\n",
      "Epoch 1 Step 13/36 Loss: 5.0321\n",
      "Epoch 1 Step 14/36 Loss: 5.6141\n",
      "Epoch 1 Step 15/36 Loss: 4.8711\n",
      "Epoch 1 Step 16/36 Loss: 4.6032\n",
      "Epoch 1 Step 17/36 Loss: 4.5229\n",
      "Epoch 1 Step 18/36 Loss: 4.0498\n",
      "Epoch 1 Step 19/36 Loss: 4.3756\n",
      "Epoch 1 Step 20/36 Loss: 5.5735\n",
      "Epoch 1 Step 21/36 Loss: 5.4654\n",
      "Epoch 1 Step 22/36 Loss: 5.7460\n",
      "Epoch 1 Step 23/36 Loss: 4.5824\n",
      "Epoch 1 Step 24/36 Loss: 6.7308\n",
      "Epoch 1 Step 25/36 Loss: 4.4116\n",
      "Epoch 1 Step 26/36 Loss: 5.5962\n",
      "Epoch 1 Step 27/36 Loss: 5.2696\n",
      "Epoch 1 Step 28/36 Loss: 5.9497\n",
      "Epoch 1 Step 29/36 Loss: 6.8003\n",
      "Epoch 1 Step 30/36 Loss: 5.9615\n",
      "Epoch 1 Step 31/36 Loss: 4.9965\n",
      "Epoch 1 Step 32/36 Loss: 6.0278\n",
      "Epoch 1 Step 33/36 Loss: 5.7688\n",
      "Epoch 1 Step 34/36 Loss: 5.9108\n",
      "Epoch 1 Step 35/36 Loss: 4.6920\n",
      "Epoch 1 Step 36/36 Loss: 6.5365\n",
      "Epoch 1 Step 1/36 Loss: 5.9681\n",
      "Epoch 1 Step 2/36 Loss: 5.2931\n",
      "Epoch 1 Step 3/36 Loss: 5.9385\n",
      "Epoch 1 Step 4/36 Loss: 4.6673\n",
      "Epoch 1 Step 5/36 Loss: 5.4681\n",
      "Epoch 1 Step 6/36 Loss: 5.0240\n",
      "Epoch 1 Step 7/36 Loss: 5.5351\n",
      "Epoch 1 Step 8/36 Loss: 5.3272\n",
      "Epoch 1 Step 9/36 Loss: 5.2651\n",
      "Epoch 1 Step 10/36 Loss: 5.7478\n",
      "Epoch 1 Step 11/36 Loss: 5.1856\n",
      "Epoch 1 Step 12/36 Loss: 4.5200\n",
      "Epoch 1 Step 13/36 Loss: 4.5849\n",
      "Epoch 1 Step 14/36 Loss: 5.6293\n",
      "Epoch 1 Step 15/36 Loss: 4.7374\n",
      "Epoch 1 Step 16/36 Loss: 5.4131\n",
      "Epoch 1 Step 17/36 Loss: 5.3403\n",
      "Epoch 1 Step 18/36 Loss: 5.6983\n",
      "Epoch 1 Step 19/36 Loss: 5.7941\n",
      "Epoch 1 Step 20/36 Loss: 6.0069\n",
      "Epoch 1 Step 21/36 Loss: 4.4635\n",
      "Epoch 1 Step 22/36 Loss: 5.3935\n",
      "Epoch 1 Step 23/36 Loss: 6.4559\n",
      "Epoch 1 Step 24/36 Loss: 6.0005\n",
      "Epoch 1 Step 25/36 Loss: 5.7449\n",
      "Epoch 1 Step 26/36 Loss: 5.4857\n",
      "Epoch 1 Step 27/36 Loss: 6.4778\n",
      "Epoch 1 Step 28/36 Loss: 5.2394\n",
      "Epoch 1 Step 29/36 Loss: 5.7472\n",
      "Epoch 1 Step 30/36 Loss: 5.7264\n",
      "Epoch 1 Step 31/36 Loss: 5.6498\n",
      "Epoch 1 Step 32/36 Loss: 5.2933\n",
      "Epoch 1 Step 33/36 Loss: 4.3673\n",
      "Epoch 1 Step 34/36 Loss: 4.8497\n",
      "Epoch 1 Step 35/36 Loss: 6.0828\n",
      "Epoch 1 Step 36/36 Loss: 6.1257\n",
      "Epoch 1 Step 1/36 Loss: 5.3740\n",
      "Epoch 1 Step 2/36 Loss: 5.2913\n",
      "Epoch 1 Step 3/36 Loss: 6.1808\n",
      "Epoch 1 Step 4/36 Loss: 5.3130\n",
      "Epoch 1 Step 5/36 Loss: 5.8135\n",
      "Epoch 1 Step 6/36 Loss: 6.1603\n",
      "Epoch 1 Step 7/36 Loss: 6.2187\n",
      "Epoch 1 Step 8/36 Loss: 3.9132\n",
      "Epoch 1 Step 9/36 Loss: 4.4425\n",
      "Epoch 1 Step 10/36 Loss: 5.9488\n",
      "Epoch 1 Step 11/36 Loss: 5.6692\n",
      "Epoch 1 Step 12/36 Loss: 4.7409\n",
      "Epoch 1 Step 13/36 Loss: 6.2832\n",
      "Epoch 1 Step 14/36 Loss: 4.7136\n",
      "Epoch 1 Step 15/36 Loss: 4.5366\n",
      "Epoch 1 Step 16/36 Loss: 5.8843\n",
      "Epoch 1 Step 17/36 Loss: 4.9472\n",
      "Epoch 1 Step 18/36 Loss: 4.6368\n",
      "Epoch 1 Step 19/36 Loss: 5.9651\n",
      "Epoch 1 Step 20/36 Loss: 6.7856\n",
      "Epoch 1 Step 21/36 Loss: 4.7802\n",
      "Epoch 1 Step 22/36 Loss: 5.9172\n",
      "Epoch 1 Step 23/36 Loss: 4.8799\n",
      "Epoch 1 Step 24/36 Loss: 6.4922\n",
      "Epoch 1 Step 25/36 Loss: 5.4269\n",
      "Epoch 1 Step 26/36 Loss: 6.5700\n",
      "Epoch 1 Step 27/36 Loss: 4.8726\n",
      "Epoch 1 Step 28/36 Loss: 5.5533\n",
      "Epoch 1 Step 29/36 Loss: 5.4598\n",
      "Epoch 1 Step 30/36 Loss: 6.0551\n",
      "Epoch 1 Step 31/36 Loss: 5.1239\n",
      "Epoch 1 Step 32/36 Loss: 6.0664\n",
      "Epoch 1 Step 33/36 Loss: 4.9634\n",
      "Epoch 1 Step 34/36 Loss: 5.1721\n",
      "Epoch 1 Step 35/36 Loss: 4.1333\n",
      "Epoch 1 Step 36/36 Loss: 4.4387\n",
      "Epoch 1 Step 1/36 Loss: 4.9392\n",
      "Epoch 1 Step 2/36 Loss: 6.0072\n",
      "Epoch 1 Step 3/36 Loss: 5.2989\n",
      "Epoch 1 Step 4/36 Loss: 5.1616\n",
      "Epoch 1 Step 5/36 Loss: 5.1152\n",
      "Epoch 1 Step 6/36 Loss: 4.8266\n",
      "Epoch 1 Step 7/36 Loss: 4.8739\n",
      "Epoch 1 Step 8/36 Loss: 5.6196\n",
      "Epoch 1 Step 9/36 Loss: 5.4477\n",
      "Epoch 1 Step 10/36 Loss: 4.4287\n",
      "Epoch 1 Step 11/36 Loss: 6.6112\n",
      "Epoch 1 Step 12/36 Loss: 4.9113\n",
      "Epoch 1 Step 13/36 Loss: 5.3367\n",
      "Epoch 1 Step 14/36 Loss: 5.2089\n",
      "Epoch 1 Step 15/36 Loss: 6.2339\n",
      "Epoch 1 Step 16/36 Loss: 5.4184\n",
      "Epoch 1 Step 17/36 Loss: 6.3259\n",
      "Epoch 1 Step 18/36 Loss: 4.9003\n",
      "Epoch 1 Step 19/36 Loss: 5.8401\n",
      "Epoch 1 Step 20/36 Loss: 4.8609\n",
      "Epoch 1 Step 21/36 Loss: 4.8400\n",
      "Epoch 1 Step 22/36 Loss: 5.4044\n",
      "Epoch 1 Step 23/36 Loss: 5.5492\n",
      "Epoch 1 Step 24/36 Loss: 5.1739\n",
      "Epoch 1 Step 25/36 Loss: 5.6568\n",
      "Epoch 1 Step 26/36 Loss: 6.5936\n",
      "Epoch 1 Step 27/36 Loss: 4.9405\n",
      "Epoch 1 Step 28/36 Loss: 4.1764\n",
      "Epoch 1 Step 29/36 Loss: 4.8588\n",
      "Epoch 1 Step 30/36 Loss: 5.4792\n",
      "Epoch 1 Step 31/36 Loss: 5.3051\n",
      "Epoch 1 Step 32/36 Loss: 6.5411\n",
      "Epoch 1 Step 33/36 Loss: 5.1087\n",
      "Epoch 1 Step 34/36 Loss: 6.0605\n",
      "Epoch 1 Step 35/36 Loss: 6.0627\n",
      "Epoch 1 Step 36/36 Loss: 4.5180\n",
      "Epoch 1 Step 1/36 Loss: 4.9809\n",
      "Epoch 1 Step 2/36 Loss: 5.7304\n",
      "Epoch 1 Step 3/36 Loss: 5.0414\n",
      "Epoch 1 Step 4/36 Loss: 4.1706\n",
      "Epoch 1 Step 5/36 Loss: 5.3708\n",
      "Epoch 1 Step 6/36 Loss: 5.2030\n",
      "Epoch 1 Step 7/36 Loss: 6.1891\n",
      "Epoch 1 Step 8/36 Loss: 5.4180\n",
      "Epoch 1 Step 9/36 Loss: 6.3695\n",
      "Epoch 1 Step 10/36 Loss: 5.2127\n",
      "Epoch 1 Step 11/36 Loss: 5.0232\n",
      "Epoch 1 Step 12/36 Loss: 5.9106\n",
      "Epoch 1 Step 13/36 Loss: 7.2696\n",
      "Epoch 1 Step 14/36 Loss: 4.8393\n",
      "Epoch 1 Step 15/36 Loss: 5.4150\n",
      "Epoch 1 Step 16/36 Loss: 5.2050\n",
      "Epoch 1 Step 17/36 Loss: 5.3455\n",
      "Epoch 1 Step 18/36 Loss: 4.4278\n",
      "Epoch 1 Step 19/36 Loss: 6.4853\n",
      "Epoch 1 Step 20/36 Loss: 6.2845\n",
      "Epoch 1 Step 21/36 Loss: 4.7548\n",
      "Epoch 1 Step 22/36 Loss: 4.9592\n",
      "Epoch 1 Step 23/36 Loss: 6.6181\n",
      "Epoch 1 Step 24/36 Loss: 5.4546\n",
      "Epoch 1 Step 25/36 Loss: 4.7356\n",
      "Epoch 1 Step 26/36 Loss: 6.2507\n",
      "Epoch 1 Step 27/36 Loss: 5.8043\n",
      "Epoch 1 Step 28/36 Loss: 6.3436\n",
      "Epoch 1 Step 29/36 Loss: 7.0357\n",
      "Epoch 1 Step 30/36 Loss: 4.4596\n",
      "Epoch 1 Step 31/36 Loss: 5.8584\n",
      "Epoch 1 Step 32/36 Loss: 5.6835\n",
      "Epoch 1 Step 33/36 Loss: 5.2077\n",
      "Epoch 1 Step 34/36 Loss: 5.4847\n",
      "Epoch 1 Step 35/36 Loss: 6.2075\n",
      "Epoch 1 Step 36/36 Loss: 5.5640\n",
      "Epoch 1 Step 1/36 Loss: 5.6627\n",
      "Epoch 1 Step 2/36 Loss: 4.8761\n",
      "Epoch 1 Step 3/36 Loss: 5.2811\n",
      "Epoch 1 Step 4/36 Loss: 6.1775\n",
      "Epoch 1 Step 5/36 Loss: 4.5477\n",
      "Epoch 1 Step 6/36 Loss: 4.4843\n",
      "Epoch 1 Step 7/36 Loss: 4.1464\n",
      "Epoch 1 Step 8/36 Loss: 5.6501\n",
      "Epoch 1 Step 9/36 Loss: 6.0244\n",
      "Epoch 1 Step 10/36 Loss: 4.7489\n",
      "Epoch 1 Step 11/36 Loss: 4.9880\n",
      "Epoch 1 Step 12/36 Loss: 6.2156\n",
      "Epoch 1 Step 13/36 Loss: 5.5500\n",
      "Epoch 1 Step 14/36 Loss: 5.8275\n",
      "Epoch 1 Step 15/36 Loss: 5.9630\n",
      "Epoch 1 Step 16/36 Loss: 6.3957\n",
      "Epoch 1 Step 17/36 Loss: 6.0554\n",
      "Epoch 1 Step 18/36 Loss: 5.0483\n",
      "Epoch 1 Step 19/36 Loss: 4.8884\n",
      "Epoch 1 Step 20/36 Loss: 5.0803\n",
      "Epoch 1 Step 21/36 Loss: 4.8433\n",
      "Epoch 1 Step 22/36 Loss: 6.6626\n",
      "Epoch 1 Step 23/36 Loss: 5.7327\n",
      "Epoch 1 Step 24/36 Loss: 5.1993\n",
      "Epoch 1 Step 25/36 Loss: 5.0355\n",
      "Epoch 1 Step 26/36 Loss: 5.8023\n",
      "Epoch 1 Step 27/36 Loss: 5.0107\n",
      "Epoch 1 Step 28/36 Loss: 5.3406\n",
      "Epoch 1 Step 29/36 Loss: 5.7900\n",
      "Epoch 1 Step 30/36 Loss: 4.8718\n",
      "Epoch 1 Step 31/36 Loss: 4.5708\n",
      "Epoch 1 Step 32/36 Loss: 5.7468\n",
      "Epoch 1 Step 33/36 Loss: 5.3323\n",
      "Epoch 1 Step 34/36 Loss: 5.0726\n",
      "Epoch 1 Step 35/36 Loss: 6.1855\n",
      "Epoch 1 Step 36/36 Loss: 7.5898\n",
      "Epoch 1 Step 1/36 Loss: 6.4081\n",
      "Epoch 1 Step 2/36 Loss: 6.3356\n",
      "Epoch 1 Step 3/36 Loss: 5.4642\n",
      "Epoch 1 Step 4/36 Loss: 4.2736\n",
      "Epoch 1 Step 5/36 Loss: 4.9833\n",
      "Epoch 1 Step 6/36 Loss: 5.3480\n",
      "Epoch 1 Step 7/36 Loss: 6.0916\n",
      "Epoch 1 Step 8/36 Loss: 4.3256\n",
      "Epoch 1 Step 9/36 Loss: 5.1446\n",
      "Epoch 1 Step 10/36 Loss: 5.4491\n",
      "Epoch 1 Step 11/36 Loss: 6.6784\n",
      "Epoch 1 Step 12/36 Loss: 6.2607\n",
      "Epoch 1 Step 13/36 Loss: 6.1452\n",
      "Epoch 1 Step 14/36 Loss: 5.6231\n",
      "Epoch 1 Step 15/36 Loss: 4.5877\n",
      "Epoch 1 Step 16/36 Loss: 4.8239\n",
      "Epoch 1 Step 17/36 Loss: 5.4528\n",
      "Epoch 1 Step 18/36 Loss: 5.0160\n",
      "Epoch 1 Step 19/36 Loss: 6.3499\n",
      "Epoch 1 Step 20/36 Loss: 4.8305\n",
      "Epoch 1 Step 21/36 Loss: 5.6728\n",
      "Epoch 1 Step 22/36 Loss: 5.4917\n",
      "Epoch 1 Step 23/36 Loss: 4.5440\n",
      "Epoch 1 Step 24/36 Loss: 5.8122\n",
      "Epoch 1 Step 25/36 Loss: 6.5301\n",
      "Epoch 1 Step 26/36 Loss: 5.6003\n",
      "Epoch 1 Step 27/36 Loss: 5.9717\n",
      "Epoch 1 Step 28/36 Loss: 6.4285\n",
      "Epoch 1 Step 29/36 Loss: 5.3519\n",
      "Epoch 1 Step 30/36 Loss: 4.7233\n",
      "Epoch 1 Step 31/36 Loss: 6.2889\n",
      "Epoch 1 Step 32/36 Loss: 5.1423\n",
      "Epoch 1 Step 33/36 Loss: 3.7225\n",
      "Epoch 1 Step 34/36 Loss: 4.1764\n",
      "Epoch 1 Step 35/36 Loss: 5.5080\n",
      "Epoch 1 Step 36/36 Loss: 6.3946\n",
      "Epoch 1 completed. Average Loss: nan\n",
      "Epoch 2 Step 1/36 Loss: 4.6562\n",
      "Epoch 2 Step 2/36 Loss: 5.1217\n",
      "Epoch 2 Step 3/36 Loss: 5.7403\n",
      "Epoch 2 Step 4/36 Loss: 5.7207\n",
      "Epoch 2 Step 5/36 Loss: 4.8156\n",
      "Epoch 2 Step 6/36 Loss: 6.2029\n",
      "Epoch 2 Step 7/36 Loss: 6.6006\n",
      "Epoch 2 Step 8/36 Loss: 4.3647\n",
      "Epoch 2 Step 9/36 Loss: 5.0585\n",
      "Epoch 2 Step 10/36 Loss: 6.1813\n",
      "Epoch 2 Step 11/36 Loss: 5.0621\n",
      "Epoch 2 Step 12/36 Loss: 5.5419\n",
      "Epoch 2 Step 13/36 Loss: 4.7735\n",
      "Epoch 2 Step 14/36 Loss: 5.9709\n",
      "Epoch 2 Step 15/36 Loss: 5.5738\n",
      "Epoch 2 Step 16/36 Loss: 4.0686\n",
      "Epoch 2 Step 17/36 Loss: 4.6216\n",
      "Epoch 2 Step 18/36 Loss: 6.2630\n",
      "Epoch 2 Step 19/36 Loss: 5.0311\n",
      "Epoch 2 Step 20/36 Loss: 6.3963\n",
      "Epoch 2 Step 21/36 Loss: 5.0437\n",
      "Epoch 2 Step 22/36 Loss: 5.6275\n",
      "Epoch 2 Step 23/36 Loss: 6.0246\n",
      "Epoch 2 Step 24/36 Loss: 5.5493\n",
      "Epoch 2 Step 25/36 Loss: 6.4144\n",
      "Epoch 2 Step 26/36 Loss: 5.2349\n",
      "Epoch 2 Step 27/36 Loss: 4.8429\n",
      "Epoch 2 Step 28/36 Loss: 6.2114\n",
      "Epoch 2 Step 29/36 Loss: 5.2262\n",
      "Epoch 2 Step 30/36 Loss: 5.1952\n",
      "Epoch 2 Step 31/36 Loss: 5.0778\n",
      "Epoch 2 Step 32/36 Loss: 7.5192\n",
      "Epoch 2 Step 33/36 Loss: 4.9670\n",
      "Epoch 2 Step 34/36 Loss: 5.3938\n",
      "Epoch 2 Step 35/36 Loss: 6.5627\n",
      "Epoch 2 Step 36/36 Loss: 7.1470\n",
      "Epoch 2 Step 1/36 Loss: 5.5819\n",
      "Epoch 2 Step 2/36 Loss: 5.2126\n",
      "Epoch 2 Step 3/36 Loss: 4.1726\n",
      "Epoch 2 Step 4/36 Loss: 4.7723\n",
      "Epoch 2 Step 5/36 Loss: 6.2052\n",
      "Epoch 2 Step 6/36 Loss: 5.5768\n",
      "Epoch 2 Step 7/36 Loss: 5.0622\n",
      "Epoch 2 Step 8/36 Loss: 5.3638\n",
      "Epoch 2 Step 9/36 Loss: 4.6993\n",
      "Epoch 2 Step 10/36 Loss: 6.3095\n",
      "Epoch 2 Step 11/36 Loss: 5.8146\n",
      "Epoch 2 Step 12/36 Loss: 5.4885\n",
      "Epoch 2 Step 13/36 Loss: 6.8650\n",
      "Epoch 2 Step 14/36 Loss: 4.1306\n",
      "Epoch 2 Step 15/36 Loss: 5.3406\n",
      "Epoch 2 Step 16/36 Loss: 4.6083\n",
      "Epoch 2 Step 17/36 Loss: 5.4583\n",
      "Epoch 2 Step 18/36 Loss: 5.0388\n",
      "Epoch 2 Step 19/36 Loss: 4.9567\n",
      "Epoch 2 Step 20/36 Loss: 4.5603\n",
      "Epoch 2 Step 21/36 Loss: 5.2591\n",
      "Epoch 2 Step 22/36 Loss: 6.3163\n",
      "Epoch 2 Step 23/36 Loss: 6.3398\n",
      "Epoch 2 Step 24/36 Loss: 5.4327\n",
      "Epoch 2 Step 25/36 Loss: 7.2954\n",
      "Epoch 2 Step 26/36 Loss: 5.8491\n",
      "Epoch 2 Step 27/36 Loss: 5.9016\n",
      "Epoch 2 Step 28/36 Loss: 5.4714\n",
      "Epoch 2 Step 29/36 Loss: 5.9444\n",
      "Epoch 2 Step 30/36 Loss: 4.7635\n",
      "Epoch 2 Step 31/36 Loss: 5.9367\n",
      "Epoch 2 Step 32/36 Loss: 4.8797\n",
      "Epoch 2 Step 33/36 Loss: 6.1170\n",
      "Epoch 2 Step 34/36 Loss: 6.7145\n",
      "Epoch 2 Step 35/36 Loss: 5.0792\n",
      "Epoch 2 Step 36/36 Loss: 4.6822\n",
      "Epoch 2 Step 1/36 Loss: 4.9495\n",
      "Epoch 2 Step 2/36 Loss: 5.9085\n",
      "Epoch 2 Step 3/36 Loss: 5.8494\n",
      "Epoch 2 Step 4/36 Loss: 5.5095\n",
      "Epoch 2 Step 5/36 Loss: 5.3875\n",
      "Epoch 2 Step 6/36 Loss: 5.3792\n",
      "Epoch 2 Step 7/36 Loss: 4.6409\n",
      "Epoch 2 Step 8/36 Loss: 4.8905\n",
      "Epoch 2 Step 9/36 Loss: 6.1262\n",
      "Epoch 2 Step 10/36 Loss: 5.0787\n",
      "Epoch 2 Step 11/36 Loss: 6.0161\n",
      "Epoch 2 Step 12/36 Loss: 5.3248\n",
      "Epoch 2 Step 13/36 Loss: 4.1676\n",
      "Epoch 2 Step 14/36 Loss: 5.7982\n",
      "Epoch 2 Step 15/36 Loss: 4.3801\n",
      "Epoch 2 Step 16/36 Loss: 6.0229\n",
      "Epoch 2 Step 17/36 Loss: 5.7168\n",
      "Epoch 2 Step 18/36 Loss: 4.9026\n",
      "Epoch 2 Step 19/36 Loss: 5.3351\n",
      "Epoch 2 Step 20/36 Loss: 6.3260\n",
      "Epoch 2 Step 21/36 Loss: 5.4637\n",
      "Epoch 2 Step 22/36 Loss: 5.0837\n",
      "Epoch 2 Step 23/36 Loss: 6.3592\n",
      "Epoch 2 Step 24/36 Loss: 5.6840\n",
      "Epoch 2 Step 25/36 Loss: 5.2411\n",
      "Epoch 2 Step 26/36 Loss: 4.6548\n",
      "Epoch 2 Step 27/36 Loss: 5.9633\n",
      "Epoch 2 Step 28/36 Loss: 6.0342\n",
      "Epoch 2 Step 29/36 Loss: 4.6627\n",
      "Epoch 2 Step 30/36 Loss: 4.7388\n",
      "Epoch 2 Step 31/36 Loss: 6.0566\n",
      "Epoch 2 Step 32/36 Loss: 6.8049\n",
      "Epoch 2 Step 33/36 Loss: 4.8357\n",
      "Epoch 2 Step 34/36 Loss: 5.5803\n",
      "Epoch 2 Step 35/36 Loss: 6.0855\n",
      "Epoch 2 Step 36/36 Loss: 5.9025\n",
      "Epoch 2 Step 1/36 Loss: 5.2079\n",
      "Epoch 2 Step 2/36 Loss: 5.5106\n",
      "Epoch 2 Step 3/36 Loss: 5.6593\n",
      "Epoch 2 Step 4/36 Loss: 6.9460\n",
      "Epoch 2 Step 5/36 Loss: 6.1832\n",
      "Epoch 2 Step 6/36 Loss: 4.3526\n",
      "Epoch 2 Step 7/36 Loss: 5.3838\n",
      "Epoch 2 Step 8/36 Loss: 5.1113\n",
      "Epoch 2 Step 9/36 Loss: 5.1801\n",
      "Epoch 2 Step 10/36 Loss: 5.6926\n",
      "Epoch 2 Step 11/36 Loss: 5.2710\n",
      "Epoch 2 Step 12/36 Loss: 5.7735\n",
      "Epoch 2 Step 13/36 Loss: 6.3327\n",
      "Epoch 2 Step 14/36 Loss: 6.0903\n",
      "Epoch 2 Step 15/36 Loss: 5.0735\n",
      "Epoch 2 Step 16/36 Loss: 3.9827\n",
      "Epoch 2 Step 17/36 Loss: 5.1449\n",
      "Epoch 2 Step 18/36 Loss: 5.2698\n",
      "Epoch 2 Step 19/36 Loss: 4.6590\n",
      "Epoch 2 Step 20/36 Loss: 4.6904\n",
      "Epoch 2 Step 21/36 Loss: 5.7785\n",
      "Epoch 2 Step 22/36 Loss: 5.3431\n",
      "Epoch 2 Step 23/36 Loss: 5.1726\n",
      "Epoch 2 Step 24/36 Loss: 5.5269\n",
      "Epoch 2 Step 25/36 Loss: 4.7403\n",
      "Epoch 2 Step 26/36 Loss: 5.8806\n",
      "Epoch 2 Step 27/36 Loss: 6.2818\n",
      "Epoch 2 Step 28/36 Loss: 6.5093\n",
      "Epoch 2 Step 29/36 Loss: 4.5066\n",
      "Epoch 2 Step 30/36 Loss: 5.2387\n",
      "Epoch 2 Step 31/36 Loss: 4.6256\n",
      "Epoch 2 Step 32/36 Loss: 4.4033\n",
      "Epoch 2 Step 33/36 Loss: 6.2023\n",
      "Epoch 2 Step 34/36 Loss: 5.9645\n",
      "Epoch 2 Step 35/36 Loss: 4.9746\n",
      "Epoch 2 Step 36/36 Loss: 4.5739\n",
      "Epoch 2 Step 1/36 Loss: 4.9138\n",
      "Epoch 2 Step 2/36 Loss: 6.0870\n",
      "Epoch 2 Step 3/36 Loss: 5.5456\n",
      "Epoch 2 Step 4/36 Loss: 5.5404\n",
      "Epoch 2 Step 5/36 Loss: 5.4511\n",
      "Epoch 2 Step 6/36 Loss: 4.9149\n",
      "Epoch 2 Step 7/36 Loss: 6.4287\n",
      "Epoch 2 Step 8/36 Loss: 5.8364\n",
      "Epoch 2 Step 9/36 Loss: 6.7323\n",
      "Epoch 2 Step 10/36 Loss: 5.5562\n",
      "Epoch 2 Step 11/36 Loss: 5.3542\n",
      "Epoch 2 Step 12/36 Loss: 5.2466\n",
      "Epoch 2 Step 13/36 Loss: 4.7295\n",
      "Epoch 2 Step 14/36 Loss: 5.4118\n",
      "Epoch 2 Step 15/36 Loss: 4.9440\n",
      "Epoch 2 Step 16/36 Loss: 6.0519\n",
      "Epoch 2 Step 17/36 Loss: 4.2474\n",
      "Epoch 2 Step 18/36 Loss: 5.5878\n",
      "Epoch 2 Step 19/36 Loss: 5.0026\n",
      "Epoch 2 Step 20/36 Loss: 5.2575\n",
      "Epoch 2 Step 21/36 Loss: 5.4056\n",
      "Epoch 2 Step 22/36 Loss: 6.5016\n",
      "Epoch 2 Step 23/36 Loss: 5.0471\n",
      "Epoch 2 Step 24/36 Loss: 5.4956\n",
      "Epoch 2 Step 25/36 Loss: 4.4346\n",
      "Epoch 2 Step 26/36 Loss: 4.7144\n",
      "Epoch 2 Step 27/36 Loss: 5.3801\n",
      "Epoch 2 Step 28/36 Loss: 6.0626\n",
      "Epoch 2 Step 29/36 Loss: 5.0793\n",
      "Epoch 2 Step 30/36 Loss: 5.7232\n",
      "Epoch 2 Step 31/36 Loss: 4.9924\n",
      "Epoch 2 Step 32/36 Loss: 5.6089\n",
      "Epoch 2 Step 33/36 Loss: 5.4797\n",
      "Epoch 2 Step 34/36 Loss: 5.3956\n",
      "Epoch 2 Step 35/36 Loss: 6.4053\n",
      "Epoch 2 Step 36/36 Loss: 4.6089\n",
      "Epoch 2 Step 1/36 Loss: 5.1506\n",
      "Epoch 2 Step 2/36 Loss: 5.7078\n",
      "Epoch 2 Step 3/36 Loss: 7.0578\n",
      "Epoch 2 Step 4/36 Loss: 6.0591\n",
      "Epoch 2 Step 5/36 Loss: 5.8242\n",
      "Epoch 2 Step 6/36 Loss: 4.9815\n",
      "Epoch 2 Step 7/36 Loss: 6.2915\n",
      "Epoch 2 Step 8/36 Loss: 3.9714\n",
      "Epoch 2 Step 9/36 Loss: 6.8663\n",
      "Epoch 2 Step 10/36 Loss: 5.1460\n",
      "Epoch 2 Step 11/36 Loss: 4.4245\n",
      "Epoch 2 Step 12/36 Loss: 5.0974\n",
      "Epoch 2 Step 13/36 Loss: 5.1060\n",
      "Epoch 2 Step 14/36 Loss: 5.7612\n",
      "Epoch 2 Step 15/36 Loss: 4.9987\n",
      "Epoch 2 Step 16/36 Loss: 4.0993\n",
      "Epoch 2 Step 17/36 Loss: 4.9103\n",
      "Epoch 2 Step 18/36 Loss: 5.5254\n",
      "Epoch 2 Step 19/36 Loss: 5.5724\n",
      "Epoch 2 Step 20/36 Loss: 6.4908\n",
      "Epoch 2 Step 21/36 Loss: 5.3782\n",
      "Epoch 2 Step 22/36 Loss: 5.2679\n",
      "Epoch 2 Step 23/36 Loss: 5.3052\n",
      "Epoch 2 Step 24/36 Loss: 6.1765\n",
      "Epoch 2 Step 25/36 Loss: 4.9402\n",
      "Epoch 2 Step 26/36 Loss: 5.4991\n",
      "Epoch 2 Step 27/36 Loss: 5.2012\n",
      "Epoch 2 Step 28/36 Loss: 5.8334\n",
      "Epoch 2 Step 29/36 Loss: 4.5739\n",
      "Epoch 2 Step 30/36 Loss: 5.9965\n",
      "Epoch 2 Step 31/36 Loss: 6.3408\n",
      "Epoch 2 Step 32/36 Loss: 5.4331\n",
      "Epoch 2 Step 33/36 Loss: 6.6822\n",
      "Epoch 2 Step 34/36 Loss: 6.5509\n",
      "Epoch 2 Step 35/36 Loss: 5.6773\n",
      "Epoch 2 Step 36/36 Loss: 4.8848\n",
      "Epoch 2 Step 1/36 Loss: 5.7170\n",
      "Epoch 2 Step 2/36 Loss: 5.9798\n",
      "Epoch 2 Step 3/36 Loss: 5.5944\n",
      "Epoch 2 Step 4/36 Loss: 5.1342\n",
      "Epoch 2 Step 5/36 Loss: 7.0783\n",
      "Epoch 2 Step 6/36 Loss: 4.7616\n",
      "Epoch 2 Step 7/36 Loss: 4.5693\n",
      "Epoch 2 Step 8/36 Loss: 4.9454\n",
      "Epoch 2 Step 9/36 Loss: 5.6381\n",
      "Epoch 2 Step 10/36 Loss: 6.2410\n",
      "Epoch 2 Step 11/36 Loss: 5.9327\n",
      "Epoch 2 Step 12/36 Loss: 7.1924\n",
      "Epoch 2 Step 13/36 Loss: 4.7282\n",
      "Epoch 2 Step 14/36 Loss: 4.5207\n",
      "Epoch 2 Step 15/36 Loss: 5.3558\n",
      "Epoch 2 Step 16/36 Loss: 5.1643\n",
      "Epoch 2 Step 17/36 Loss: 5.6138\n",
      "Epoch 2 Step 18/36 Loss: 4.9401\n",
      "Epoch 2 Step 19/36 Loss: 4.9486\n",
      "Epoch 2 Step 20/36 Loss: 6.7810\n",
      "Epoch 2 Step 21/36 Loss: 4.3753\n",
      "Epoch 2 Step 22/36 Loss: 6.2678\n",
      "Epoch 2 Step 23/36 Loss: 5.7344\n",
      "Epoch 2 Step 24/36 Loss: 5.4937\n",
      "Epoch 2 Step 25/36 Loss: 4.9614\n",
      "Epoch 2 Step 26/36 Loss: 6.2202\n",
      "Epoch 2 Step 27/36 Loss: 5.8663\n",
      "Epoch 2 Step 28/36 Loss: 5.8196\n",
      "Epoch 2 Step 29/36 Loss: 5.7032\n",
      "Epoch 2 Step 30/36 Loss: 5.6134\n",
      "Epoch 2 Step 31/36 Loss: 5.8830\n",
      "Epoch 2 Step 32/36 Loss: 4.8077\n",
      "Epoch 2 Step 33/36 Loss: 5.1269\n",
      "Epoch 2 Step 34/36 Loss: 5.4939\n",
      "Epoch 2 Step 35/36 Loss: 5.5807\n",
      "Epoch 2 Step 36/36 Loss: 5.6135\n",
      "Epoch 2 Step 1/36 Loss: 5.2991\n",
      "Epoch 2 Step 2/36 Loss: 5.3168\n",
      "Epoch 2 Step 3/36 Loss: 5.7588\n",
      "Epoch 2 Step 4/36 Loss: 4.0177\n",
      "Epoch 2 Step 5/36 Loss: 4.4948\n",
      "Epoch 2 Step 6/36 Loss: 5.9254\n",
      "Epoch 2 Step 7/36 Loss: 5.6108\n",
      "Epoch 2 Step 8/36 Loss: 6.4999\n",
      "Epoch 2 Step 9/36 Loss: 5.6374\n",
      "Epoch 2 Step 10/36 Loss: 4.5718\n",
      "Epoch 2 Step 11/36 Loss: 3.9079\n",
      "Epoch 2 Step 12/36 Loss: 5.4768\n",
      "Epoch 2 Step 13/36 Loss: 5.1253\n",
      "Epoch 2 Step 14/36 Loss: 5.7522\n",
      "Epoch 2 Step 15/36 Loss: 4.6681\n",
      "Epoch 2 Step 16/36 Loss: 5.0414\n",
      "Epoch 2 Step 17/36 Loss: 4.9265\n",
      "Epoch 2 Step 18/36 Loss: 4.1959\n",
      "Epoch 2 Step 19/36 Loss: 6.5349\n",
      "Epoch 2 Step 20/36 Loss: 6.0634\n",
      "Epoch 2 Step 21/36 Loss: 5.9131\n",
      "Epoch 2 Step 22/36 Loss: 5.7594\n",
      "Epoch 2 Step 23/36 Loss: 4.6561\n",
      "Epoch 2 Step 24/36 Loss: 5.1962\n",
      "Epoch 2 Step 25/36 Loss: 6.1835\n",
      "Epoch 2 Step 26/36 Loss: 5.7892\n",
      "Epoch 2 Step 27/36 Loss: 4.9182\n",
      "Epoch 2 Step 28/36 Loss: 5.4718\n",
      "Epoch 2 Step 29/36 Loss: 6.0267\n",
      "Epoch 2 Step 30/36 Loss: 4.7713\n",
      "Epoch 2 Step 31/36 Loss: 5.8122\n",
      "Epoch 2 Step 32/36 Loss: 4.6241\n",
      "Epoch 2 Step 33/36 Loss: 6.8111\n",
      "Epoch 2 Step 34/36 Loss: 5.8140\n",
      "Epoch 2 Step 35/36 Loss: 5.7023\n",
      "Epoch 2 Step 36/36 Loss: 6.0573\n",
      "Epoch 2 Step 1/36 Loss: 4.8231\n",
      "Epoch 2 Step 2/36 Loss: 5.8102\n",
      "Epoch 2 Step 3/36 Loss: 5.3558\n",
      "Epoch 2 Step 4/36 Loss: 4.7570\n",
      "Epoch 2 Step 5/36 Loss: 5.6643\n",
      "Epoch 2 Step 6/36 Loss: 6.5313\n",
      "Epoch 2 Step 7/36 Loss: 5.5625\n",
      "Epoch 2 Step 8/36 Loss: 5.9429\n",
      "Epoch 2 Step 9/36 Loss: 6.6668\n",
      "Epoch 2 Step 10/36 Loss: 5.9592\n",
      "Epoch 2 Step 11/36 Loss: 5.6095\n",
      "Epoch 2 Step 12/36 Loss: 4.2302\n",
      "Epoch 2 Step 13/36 Loss: 5.1833\n",
      "Epoch 2 Step 14/36 Loss: 5.1515\n",
      "Epoch 2 Step 15/36 Loss: 5.3964\n",
      "Epoch 2 Step 16/36 Loss: 4.9802\n",
      "Epoch 2 Step 17/36 Loss: 4.8348\n",
      "Epoch 2 Step 18/36 Loss: 6.4348\n",
      "Epoch 2 Step 19/36 Loss: 6.2175\n",
      "Epoch 2 Step 20/36 Loss: 4.1815\n",
      "Epoch 2 Step 21/36 Loss: 6.3338\n",
      "Epoch 2 Step 22/36 Loss: 6.4108\n",
      "Epoch 2 Step 23/36 Loss: 5.8415\n",
      "Epoch 2 Step 24/36 Loss: 6.1475\n",
      "Epoch 2 Step 25/36 Loss: 6.8087\n",
      "Epoch 2 Step 26/36 Loss: 6.6989\n",
      "Epoch 2 Step 27/36 Loss: 4.4997\n",
      "Epoch 2 Step 28/36 Loss: 5.3703\n",
      "Epoch 2 Step 29/36 Loss: 4.8997\n",
      "Epoch 2 Step 30/36 Loss: 5.0958\n",
      "Epoch 2 Step 31/36 Loss: 5.2540\n",
      "Epoch 2 Step 32/36 Loss: 5.6599\n",
      "Epoch 2 Step 33/36 Loss: 6.3203\n",
      "Epoch 2 Step 34/36 Loss: 6.3944\n",
      "Epoch 2 Step 35/36 Loss: 4.9413\n",
      "Epoch 2 Step 36/36 Loss: 5.4373\n",
      "Epoch 2 Step 1/36 Loss: 6.3269\n",
      "Epoch 2 Step 2/36 Loss: 4.9132\n",
      "Epoch 2 Step 3/36 Loss: 3.9486\n",
      "Epoch 2 Step 4/36 Loss: 5.1100\n",
      "Epoch 2 Step 5/36 Loss: 7.2707\n",
      "Epoch 2 Step 6/36 Loss: 6.1307\n",
      "Epoch 2 Step 7/36 Loss: 6.9852\n",
      "Epoch 2 Step 8/36 Loss: 4.8318\n",
      "Epoch 2 Step 9/36 Loss: 6.1080\n",
      "Epoch 2 Step 10/36 Loss: 6.4699\n",
      "Epoch 2 Step 11/36 Loss: 4.8080\n",
      "Epoch 2 Step 12/36 Loss: 5.3042\n",
      "Epoch 2 Step 13/36 Loss: 5.5443\n",
      "Epoch 2 Step 14/36 Loss: 4.6343\n",
      "Epoch 2 Step 15/36 Loss: 5.4086\n",
      "Epoch 2 Step 16/36 Loss: 5.3471\n",
      "Epoch 2 Step 17/36 Loss: 6.3315\n",
      "Epoch 2 Step 18/36 Loss: 5.3792\n",
      "Epoch 2 Step 19/36 Loss: 4.7921\n",
      "Epoch 2 Step 20/36 Loss: 4.5936\n",
      "Epoch 2 Step 21/36 Loss: 4.9890\n",
      "Epoch 2 Step 22/36 Loss: 5.9692\n",
      "Epoch 2 Step 23/36 Loss: 5.4841\n",
      "Epoch 2 Step 24/36 Loss: 5.2233\n",
      "Epoch 2 Step 25/36 Loss: 6.2330\n",
      "Epoch 2 Step 26/36 Loss: 5.8390\n",
      "Epoch 2 Step 27/36 Loss: 4.5163\n",
      "Epoch 2 Step 28/36 Loss: 4.6526\n",
      "Epoch 2 Step 29/36 Loss: 6.6161\n",
      "Epoch 2 Step 30/36 Loss: 6.1018\n",
      "Epoch 2 Step 31/36 Loss: 5.3020\n",
      "Epoch 2 Step 32/36 Loss: 4.7729\n",
      "Epoch 2 Step 33/36 Loss: 4.3230\n",
      "Epoch 2 Step 34/36 Loss: 4.6769\n",
      "Epoch 2 Step 35/36 Loss: 6.2053\n",
      "Epoch 2 Step 36/36 Loss: 5.2556\n",
      "Epoch 2 Step 1/36 Loss: 5.8658\n",
      "Epoch 2 Step 2/36 Loss: 4.7636\n",
      "Epoch 2 Step 3/36 Loss: 5.5359\n",
      "Epoch 2 Step 4/36 Loss: 5.8559\n",
      "Epoch 2 Step 5/36 Loss: 5.2752\n",
      "Epoch 2 Step 6/36 Loss: 5.3150\n",
      "Epoch 2 Step 7/36 Loss: 4.9450\n",
      "Epoch 2 Step 8/36 Loss: 5.8235\n",
      "Epoch 2 Step 9/36 Loss: 5.6294\n",
      "Epoch 2 Step 10/36 Loss: 4.7848\n",
      "Epoch 2 Step 11/36 Loss: 5.2553\n",
      "Epoch 2 Step 12/36 Loss: 4.2614\n",
      "Epoch 2 Step 13/36 Loss: 4.9263\n",
      "Epoch 2 Step 14/36 Loss: 7.0886\n",
      "Epoch 2 Step 15/36 Loss: 5.5975\n",
      "Epoch 2 Step 16/36 Loss: 5.6304\n",
      "Epoch 2 Step 17/36 Loss: 6.7888\n",
      "Epoch 2 Step 18/36 Loss: 4.1755\n",
      "Epoch 2 Step 19/36 Loss: 4.5720\n",
      "Epoch 2 Step 20/36 Loss: 5.1811\n",
      "Epoch 2 Step 21/36 Loss: 6.0449\n",
      "Epoch 2 Step 22/36 Loss: 7.4062\n",
      "Epoch 2 Step 23/36 Loss: 6.1448\n",
      "Epoch 2 Step 24/36 Loss: 5.8572\n",
      "Epoch 2 Step 25/36 Loss: 6.4881\n",
      "Epoch 2 Step 26/36 Loss: 4.9312\n",
      "Epoch 2 Step 27/36 Loss: 5.6596\n",
      "Epoch 2 Step 28/36 Loss: 4.5279\n",
      "Epoch 2 Step 29/36 Loss: 6.8773\n",
      "Epoch 2 Step 30/36 Loss: 4.8036\n",
      "Epoch 2 Step 31/36 Loss: 5.4104\n",
      "Epoch 2 Step 32/36 Loss: 4.9341\n",
      "Epoch 2 Step 33/36 Loss: 6.1962\n",
      "Epoch 2 Step 34/36 Loss: 4.7492\n",
      "Epoch 2 Step 35/36 Loss: 5.0776\n",
      "Epoch 2 Step 36/36 Loss: 4.1549\n",
      "Epoch 2 Step 1/36 Loss: 7.9194\n",
      "Epoch 2 Step 2/36 Loss: 6.2666\n",
      "Epoch 2 Step 3/36 Loss: 6.6756\n",
      "Epoch 2 Step 4/36 Loss: 4.5487\n",
      "Epoch 2 Step 5/36 Loss: 5.7195\n",
      "Epoch 2 Step 6/36 Loss: 4.5872\n",
      "Epoch 2 Step 7/36 Loss: 7.0689\n",
      "Epoch 2 Step 8/36 Loss: 5.8866\n",
      "Epoch 2 Step 9/36 Loss: 4.7978\n",
      "Epoch 2 Step 10/36 Loss: 4.2049\n",
      "Epoch 2 Step 11/36 Loss: 4.8245\n",
      "Epoch 2 Step 12/36 Loss: 5.1823\n",
      "Epoch 2 Step 13/36 Loss: 4.7059\n",
      "Epoch 2 Step 14/36 Loss: 4.9239\n",
      "Epoch 2 Step 15/36 Loss: 5.6238\n",
      "Epoch 2 Step 16/36 Loss: 5.8288\n",
      "Epoch 2 Step 17/36 Loss: 4.9387\n",
      "Epoch 2 Step 18/36 Loss: 4.3603\n",
      "Epoch 2 Step 19/36 Loss: 6.8360\n",
      "Epoch 2 Step 20/36 Loss: 5.8593\n",
      "Epoch 2 Step 21/36 Loss: 4.0444\n",
      "Epoch 2 Step 22/36 Loss: 6.3548\n",
      "Epoch 2 Step 23/36 Loss: 5.1069\n",
      "Epoch 2 Step 24/36 Loss: 6.2140\n",
      "Epoch 2 Step 25/36 Loss: 5.3963\n",
      "Epoch 2 Step 26/36 Loss: 4.1124\n",
      "Epoch 2 Step 27/36 Loss: 5.7910\n",
      "Epoch 2 Step 28/36 Loss: 5.8899\n",
      "Epoch 2 Step 29/36 Loss: 4.9462\n",
      "Epoch 2 Step 30/36 Loss: 4.3846\n",
      "Epoch 2 Step 31/36 Loss: 5.5523\n",
      "Epoch 2 Step 32/36 Loss: 4.4931\n",
      "Epoch 2 Step 33/36 Loss: 5.1366\n",
      "Epoch 2 Step 34/36 Loss: 6.8322\n",
      "Epoch 2 Step 35/36 Loss: 6.3412\n",
      "Epoch 2 Step 36/36 Loss: 5.6217\n",
      "Epoch 2 Step 1/36 Loss: 4.0667\n",
      "Epoch 2 Step 2/36 Loss: 4.8426\n",
      "Epoch 2 Step 3/36 Loss: 4.7184\n",
      "Epoch 2 Step 4/36 Loss: 5.9571\n",
      "Epoch 2 Step 5/36 Loss: 5.7106\n",
      "Epoch 2 Step 6/36 Loss: 6.3248\n",
      "Epoch 2 Step 7/36 Loss: 5.7284\n",
      "Epoch 2 Step 8/36 Loss: 6.1422\n",
      "Epoch 2 Step 9/36 Loss: 6.1160\n",
      "Epoch 2 Step 10/36 Loss: 4.8876\n",
      "Epoch 2 Step 11/36 Loss: 5.8160\n",
      "Epoch 2 Step 12/36 Loss: 4.3944\n",
      "Epoch 2 Step 13/36 Loss: 4.8078\n",
      "Epoch 2 Step 14/36 Loss: 6.1777\n",
      "Epoch 2 Step 15/36 Loss: 5.7703\n",
      "Epoch 2 Step 16/36 Loss: 4.7359\n",
      "Epoch 2 Step 17/36 Loss: 6.0752\n",
      "Epoch 2 Step 18/36 Loss: 5.3357\n",
      "Epoch 2 Step 19/36 Loss: 5.7593\n",
      "Epoch 2 Step 20/36 Loss: 4.1891\n",
      "Epoch 2 Step 21/36 Loss: 5.4906\n",
      "Epoch 2 Step 22/36 Loss: 5.1501\n",
      "Epoch 2 Step 23/36 Loss: 5.1840\n",
      "Epoch 2 Step 24/36 Loss: 5.6747\n",
      "Epoch 2 Step 25/36 Loss: 4.5671\n",
      "Epoch 2 Step 26/36 Loss: 5.0269\n",
      "Epoch 2 Step 27/36 Loss: 4.2966\n",
      "Epoch 2 Step 28/36 Loss: 5.2639\n",
      "Epoch 2 Step 29/36 Loss: 5.4295\n",
      "Epoch 2 Step 30/36 Loss: 5.1677\n",
      "Epoch 2 Step 31/36 Loss: 4.3597\n",
      "Epoch 2 Step 32/36 Loss: 5.7962\n",
      "Epoch 2 Step 33/36 Loss: 5.5603\n",
      "Epoch 2 Step 34/36 Loss: 5.0671\n",
      "Epoch 2 Step 35/36 Loss: 4.3794\n",
      "Epoch 2 Step 36/36 Loss: 5.1432\n",
      "Epoch 2 Step 1/36 Loss: 5.3138\n",
      "Epoch 2 Step 2/36 Loss: 5.6723\n",
      "Epoch 2 Step 3/36 Loss: 4.7059\n",
      "Epoch 2 Step 4/36 Loss: 6.0389\n",
      "Epoch 2 Step 5/36 Loss: 5.2551\n",
      "Epoch 2 Step 6/36 Loss: 4.5200\n",
      "Epoch 2 Step 7/36 Loss: 6.2537\n",
      "Epoch 2 Step 8/36 Loss: 4.5173\n",
      "Epoch 2 Step 9/36 Loss: 6.6844\n",
      "Epoch 2 Step 10/36 Loss: 5.3371\n",
      "Epoch 2 Step 11/36 Loss: 4.6915\n",
      "Epoch 2 Step 12/36 Loss: 4.6374\n",
      "Epoch 2 Step 13/36 Loss: 4.7516\n",
      "Epoch 2 Step 14/36 Loss: 6.3776\n",
      "Epoch 2 Step 15/36 Loss: 4.6378\n",
      "Epoch 2 Step 16/36 Loss: 5.7772\n",
      "Epoch 2 Step 17/36 Loss: 5.6246\n",
      "Epoch 2 Step 18/36 Loss: 4.6355\n",
      "Epoch 2 Step 19/36 Loss: 6.6998\n",
      "Epoch 2 Step 20/36 Loss: 5.8013\n",
      "Epoch 2 Step 21/36 Loss: 5.2412\n",
      "Epoch 2 Step 22/36 Loss: 5.2988\n",
      "Epoch 2 Step 23/36 Loss: 4.7449\n",
      "Epoch 2 Step 24/36 Loss: 5.5485\n",
      "Epoch 2 Step 25/36 Loss: 4.3688\n",
      "Epoch 2 Step 26/36 Loss: 7.1059\n",
      "Epoch 2 Step 27/36 Loss: 4.9241\n",
      "Epoch 2 Step 28/36 Loss: 5.0533\n",
      "Epoch 2 Step 29/36 Loss: 5.0871\n",
      "Epoch 2 Step 30/36 Loss: 4.7546\n",
      "Epoch 2 Step 31/36 Loss: 5.3195\n",
      "Epoch 2 Step 32/36 Loss: 5.3376\n",
      "Epoch 2 Step 33/36 Loss: 5.0050\n",
      "Epoch 2 Step 34/36 Loss: 4.4367\n",
      "Epoch 2 Step 35/36 Loss: 4.3842\n",
      "Epoch 2 Step 36/36 Loss: 5.3264\n",
      "Epoch 2 Step 1/36 Loss: 4.7053\n",
      "Epoch 2 Step 2/36 Loss: 5.5945\n",
      "Epoch 2 Step 3/36 Loss: 5.2582\n",
      "Epoch 2 Step 4/36 Loss: 5.0915\n",
      "Epoch 2 Step 5/36 Loss: 4.5242\n",
      "Epoch 2 Step 6/36 Loss: 5.6978\n",
      "Epoch 2 Step 7/36 Loss: 5.9235\n",
      "Epoch 2 Step 8/36 Loss: 5.6163\n",
      "Epoch 2 Step 9/36 Loss: 5.3591\n",
      "Epoch 2 Step 10/36 Loss: 5.7046\n",
      "Epoch 2 Step 11/36 Loss: 5.6245\n",
      "Epoch 2 Step 12/36 Loss: 5.6827\n",
      "Epoch 2 Step 13/36 Loss: 5.7631\n",
      "Epoch 2 Step 14/36 Loss: 5.7064\n",
      "Epoch 2 Step 15/36 Loss: 5.2696\n",
      "Epoch 2 Step 16/36 Loss: 5.4108\n",
      "Epoch 2 Step 17/36 Loss: 5.5160\n",
      "Epoch 2 Step 18/36 Loss: 5.5483\n",
      "Epoch 2 Step 19/36 Loss: 5.9866\n",
      "Epoch 2 Step 20/36 Loss: 5.6981\n",
      "Epoch 2 Step 21/36 Loss: 4.8688\n",
      "Epoch 2 Step 22/36 Loss: 6.1628\n",
      "Epoch 2 Step 23/36 Loss: 4.6322\n",
      "Epoch 2 Step 24/36 Loss: 5.7580\n",
      "Epoch 2 Step 25/36 Loss: 6.5368\n",
      "Epoch 2 Step 26/36 Loss: 4.8587\n",
      "Epoch 2 Step 27/36 Loss: 5.7253\n",
      "Epoch 2 Step 28/36 Loss: 5.6446\n",
      "Epoch 2 Step 29/36 Loss: 4.7987\n",
      "Epoch 2 Step 30/36 Loss: 5.5140\n",
      "Epoch 2 Step 31/36 Loss: 5.5321\n",
      "Epoch 2 Step 32/36 Loss: 7.6213\n",
      "Epoch 2 Step 33/36 Loss: 5.1110\n",
      "Epoch 2 Step 34/36 Loss: 5.9630\n",
      "Epoch 2 Step 35/36 Loss: 6.0090\n",
      "Epoch 2 Step 36/36 Loss: 5.1758\n",
      "Epoch 2 Step 1/36 Loss: 5.3628\n",
      "Epoch 2 Step 2/36 Loss: 6.1884\n",
      "Epoch 2 Step 3/36 Loss: 5.0141\n",
      "Epoch 2 Step 4/36 Loss: 4.8729\n",
      "Epoch 2 Step 5/36 Loss: 5.5934\n",
      "Epoch 2 Step 6/36 Loss: 6.9170\n",
      "Epoch 2 Step 7/36 Loss: 6.0871\n",
      "Epoch 2 Step 8/36 Loss: 5.6371\n",
      "Epoch 2 Step 9/36 Loss: 6.5045\n",
      "Epoch 2 Step 10/36 Loss: 5.0650\n",
      "Epoch 2 Step 11/36 Loss: 6.9662\n",
      "Epoch 2 Step 12/36 Loss: 6.0072\n",
      "Epoch 2 Step 13/36 Loss: 5.4382\n",
      "Epoch 2 Step 14/36 Loss: 5.5926\n",
      "Epoch 2 Step 15/36 Loss: 4.5104\n",
      "Epoch 2 Step 16/36 Loss: 5.4883\n",
      "Epoch 2 Step 17/36 Loss: 5.9918\n",
      "Epoch 2 Step 18/36 Loss: 5.3069\n",
      "Epoch 2 Step 19/36 Loss: 5.4251\n",
      "Epoch 2 Step 20/36 Loss: 5.1679\n",
      "Epoch 2 Step 21/36 Loss: 6.0302\n",
      "Epoch 2 Step 22/36 Loss: 5.9459\n",
      "Epoch 2 Step 23/36 Loss: 4.5538\n",
      "Epoch 2 Step 24/36 Loss: 5.9608\n",
      "Epoch 2 Step 25/36 Loss: 4.2500\n",
      "Epoch 2 Step 26/36 Loss: 6.0053\n",
      "Epoch 2 Step 27/36 Loss: 5.0612\n",
      "Epoch 2 Step 28/36 Loss: 4.8539\n",
      "Epoch 2 Step 29/36 Loss: 5.1510\n",
      "Epoch 2 Step 30/36 Loss: 5.4036\n",
      "Epoch 2 Step 31/36 Loss: 4.7472\n",
      "Epoch 2 Step 32/36 Loss: 6.3372\n",
      "Epoch 2 Step 33/36 Loss: 6.4708\n",
      "Epoch 2 Step 34/36 Loss: 5.3212\n",
      "Epoch 2 Step 35/36 Loss: 4.7258\n",
      "Epoch 2 Step 36/36 Loss: 5.0731\n",
      "Epoch 2 Step 1/36 Loss: 5.1454\n",
      "Epoch 2 Step 2/36 Loss: 6.6734\n",
      "Epoch 2 Step 3/36 Loss: 4.9078\n",
      "Epoch 2 Step 4/36 Loss: 4.6921\n",
      "Epoch 2 Step 5/36 Loss: 5.2231\n",
      "Epoch 2 Step 6/36 Loss: 5.0884\n",
      "Epoch 2 Step 7/36 Loss: 4.9634\n",
      "Epoch 2 Step 8/36 Loss: 6.3878\n",
      "Epoch 2 Step 9/36 Loss: 5.4615\n",
      "Epoch 2 Step 10/36 Loss: 5.0462\n",
      "Epoch 2 Step 11/36 Loss: 5.3380\n",
      "Epoch 2 Step 12/36 Loss: 5.8493\n",
      "Epoch 2 Step 13/36 Loss: 4.9476\n",
      "Epoch 2 Step 14/36 Loss: 5.5518\n",
      "Epoch 2 Step 15/36 Loss: 5.6661\n",
      "Epoch 2 Step 16/36 Loss: 4.9873\n",
      "Epoch 2 Step 17/36 Loss: 6.4374\n",
      "Epoch 2 Step 18/36 Loss: 5.6086\n",
      "Epoch 2 Step 19/36 Loss: 6.0800\n",
      "Epoch 2 Step 20/36 Loss: 5.4327\n",
      "Epoch 2 Step 21/36 Loss: 4.9968\n",
      "Epoch 2 Step 22/36 Loss: 6.2235\n",
      "Epoch 2 Step 23/36 Loss: 5.6022\n",
      "Epoch 2 Step 24/36 Loss: 4.5958\n",
      "Epoch 2 Step 25/36 Loss: 5.1875\n",
      "Epoch 2 Step 26/36 Loss: 4.4802\n",
      "Epoch 2 Step 27/36 Loss: 6.4607\n",
      "Epoch 2 Step 28/36 Loss: 4.7558\n",
      "Epoch 2 Step 29/36 Loss: 5.7676\n",
      "Epoch 2 Step 30/36 Loss: 5.1147\n",
      "Epoch 2 Step 31/36 Loss: 4.8446\n",
      "Epoch 2 Step 32/36 Loss: 5.0857\n",
      "Epoch 2 Step 33/36 Loss: 5.6859\n",
      "Epoch 2 Step 34/36 Loss: 6.0347\n",
      "Epoch 2 Step 35/36 Loss: 5.8085\n",
      "Epoch 2 Step 36/36 Loss: 7.0249\n",
      "Epoch 2 Step 1/36 Loss: 5.6805\n",
      "Epoch 2 Step 2/36 Loss: 6.6330\n",
      "Epoch 2 Step 3/36 Loss: 4.8607\n",
      "Epoch 2 Step 4/36 Loss: 4.9356\n",
      "Epoch 2 Step 5/36 Loss: 5.0014\n",
      "Epoch 2 Step 6/36 Loss: 6.0913\n",
      "Epoch 2 Step 7/36 Loss: 5.4166\n",
      "Epoch 2 Step 8/36 Loss: 5.8894\n",
      "Epoch 2 Step 9/36 Loss: 6.5443\n",
      "Epoch 2 Step 10/36 Loss: 4.5044\n",
      "Epoch 2 Step 11/36 Loss: 6.1015\n",
      "Epoch 2 Step 12/36 Loss: 5.5476\n",
      "Epoch 2 Step 13/36 Loss: 6.7067\n",
      "Epoch 2 Step 14/36 Loss: 4.7640\n",
      "Epoch 2 Step 15/36 Loss: 4.7148\n",
      "Epoch 2 Step 16/36 Loss: 5.5854\n",
      "Epoch 2 Step 17/36 Loss: 5.1144\n",
      "Epoch 2 Step 18/36 Loss: 5.5818\n",
      "Epoch 2 Step 19/36 Loss: 5.1780\n",
      "Epoch 2 Step 20/36 Loss: 4.0490\n",
      "Epoch 2 Step 21/36 Loss: 5.6789\n",
      "Epoch 2 Step 22/36 Loss: 5.6175\n",
      "Epoch 2 Step 23/36 Loss: 5.5909\n",
      "Epoch 2 Step 24/36 Loss: 5.0882\n",
      "Epoch 2 Step 25/36 Loss: 4.5924\n",
      "Epoch 2 Step 26/36 Loss: 5.7577\n",
      "Epoch 2 Step 27/36 Loss: 5.2055\n",
      "Epoch 2 Step 28/36 Loss: 5.4854\n",
      "Epoch 2 Step 29/36 Loss: 7.4563\n",
      "Epoch 2 Step 30/36 Loss: 5.5050\n",
      "Epoch 2 Step 31/36 Loss: 4.8826\n",
      "Epoch 2 Step 32/36 Loss: 4.5936\n",
      "Epoch 2 Step 33/36 Loss: 5.0375\n",
      "Epoch 2 Step 34/36 Loss: 4.2324\n",
      "Epoch 2 Step 35/36 Loss: 5.5958\n",
      "Epoch 2 Step 36/36 Loss: 4.6337\n",
      "Epoch 2 Step 1/36 Loss: 4.5150\n",
      "Epoch 2 Step 2/36 Loss: 5.0321\n",
      "Epoch 2 Step 3/36 Loss: 7.1338\n",
      "Epoch 2 Step 4/36 Loss: 4.2951\n",
      "Epoch 2 Step 5/36 Loss: 4.5715\n",
      "Epoch 2 Step 6/36 Loss: 4.8887\n",
      "Epoch 2 Step 7/36 Loss: 6.1419\n",
      "Epoch 2 Step 8/36 Loss: 6.4548\n",
      "Epoch 2 Step 9/36 Loss: 5.4408\n",
      "Epoch 2 Step 10/36 Loss: 4.7203\n",
      "Epoch 2 Step 11/36 Loss: 7.4640\n",
      "Epoch 2 Step 12/36 Loss: 5.9246\n",
      "Epoch 2 Step 13/36 Loss: 5.1057\n",
      "Epoch 2 Step 14/36 Loss: 6.8485\n",
      "Epoch 2 Step 15/36 Loss: 4.9574\n",
      "Epoch 2 Step 16/36 Loss: 6.1710\n",
      "Epoch 2 Step 17/36 Loss: 5.5529\n",
      "Epoch 2 Step 18/36 Loss: 4.4939\n",
      "Epoch 2 Step 19/36 Loss: 6.4562\n",
      "Epoch 2 Step 20/36 Loss: 4.9478\n",
      "Epoch 2 Step 21/36 Loss: 6.6701\n",
      "Epoch 2 Step 22/36 Loss: 5.8808\n",
      "Epoch 2 Step 23/36 Loss: 4.1739\n",
      "Epoch 2 Step 24/36 Loss: 6.0469\n",
      "Epoch 2 Step 25/36 Loss: 6.5687\n",
      "Epoch 2 Step 26/36 Loss: 5.5330\n",
      "Epoch 2 Step 27/36 Loss: 5.1889\n",
      "Epoch 2 Step 28/36 Loss: 5.0636\n",
      "Epoch 2 Step 29/36 Loss: 5.8586\n",
      "Epoch 2 Step 30/36 Loss: 5.6316\n",
      "Epoch 2 Step 31/36 Loss: 5.2615\n",
      "Epoch 2 Step 32/36 Loss: 6.3080\n",
      "Epoch 2 Step 33/36 Loss: 4.6531\n",
      "Epoch 2 Step 34/36 Loss: 6.5133\n",
      "Epoch 2 Step 35/36 Loss: 5.9818\n",
      "Epoch 2 Step 36/36 Loss: 5.4003\n",
      "Epoch 2 Step 1/36 Loss: 6.4189\n",
      "Epoch 2 Step 2/36 Loss: 5.1690\n",
      "Epoch 2 Step 3/36 Loss: 6.4772\n",
      "Epoch 2 Step 4/36 Loss: 3.9842\n",
      "Epoch 2 Step 5/36 Loss: 4.8539\n",
      "Epoch 2 Step 6/36 Loss: 5.9257\n",
      "Epoch 2 Step 7/36 Loss: 3.8723\n",
      "Epoch 2 Step 8/36 Loss: 5.2982\n",
      "Epoch 2 Step 9/36 Loss: 6.0194\n",
      "Epoch 2 Step 10/36 Loss: 5.6624\n",
      "Epoch 2 Step 11/36 Loss: 5.4161\n",
      "Epoch 2 Step 12/36 Loss: 6.6610\n",
      "Epoch 2 Step 13/36 Loss: 6.2387\n",
      "Epoch 2 Step 14/36 Loss: 5.7419\n",
      "Epoch 2 Step 15/36 Loss: 6.4206\n",
      "Epoch 2 Step 16/36 Loss: 5.3414\n",
      "Epoch 2 Step 17/36 Loss: 6.0922\n",
      "Epoch 2 Step 18/36 Loss: 4.9899\n",
      "Epoch 2 Step 19/36 Loss: 5.3684\n",
      "Epoch 2 Step 20/36 Loss: 4.8272\n",
      "Epoch 2 Step 21/36 Loss: 5.6093\n",
      "Epoch 2 Step 22/36 Loss: 5.5541\n",
      "Epoch 2 Step 23/36 Loss: 5.9889\n",
      "Epoch 2 Step 24/36 Loss: 5.2340\n",
      "Epoch 2 Step 25/36 Loss: 4.5684\n",
      "Epoch 2 Step 26/36 Loss: 6.6619\n",
      "Epoch 2 Step 27/36 Loss: 5.0473\n",
      "Epoch 2 Step 28/36 Loss: 4.8563\n",
      "Epoch 2 Step 29/36 Loss: 4.8691\n",
      "Epoch 2 Step 30/36 Loss: 4.8498\n",
      "Epoch 2 Step 31/36 Loss: 5.7131\n",
      "Epoch 2 Step 32/36 Loss: 4.9539\n",
      "Epoch 2 Step 33/36 Loss: 5.9164\n",
      "Epoch 2 Step 34/36 Loss: 6.2995\n",
      "Epoch 2 Step 35/36 Loss: 5.9444\n",
      "Epoch 2 Step 36/36 Loss: 5.6078\n",
      "Epoch 2 Step 1/36 Loss: 6.6551\n",
      "Epoch 2 Step 2/36 Loss: 4.8053\n",
      "Epoch 2 Step 3/36 Loss: 6.3113\n",
      "Epoch 2 Step 4/36 Loss: 5.2634\n",
      "Epoch 2 Step 5/36 Loss: 4.4690\n",
      "Epoch 2 Step 6/36 Loss: 5.7738\n",
      "Epoch 2 Step 7/36 Loss: 4.8573\n",
      "Epoch 2 Step 8/36 Loss: 5.3028\n",
      "Epoch 2 Step 9/36 Loss: 4.7948\n",
      "Epoch 2 Step 10/36 Loss: 6.5591\n",
      "Epoch 2 Step 11/36 Loss: 4.7713\n",
      "Epoch 2 Step 12/36 Loss: 4.9507\n",
      "Epoch 2 Step 13/36 Loss: 5.3267\n",
      "Epoch 2 Step 14/36 Loss: 7.2080\n",
      "Epoch 2 Step 15/36 Loss: 4.8392\n",
      "Epoch 2 Step 16/36 Loss: 4.5847\n",
      "Epoch 2 Step 17/36 Loss: 4.5404\n",
      "Epoch 2 Step 18/36 Loss: 5.4008\n",
      "Epoch 2 Step 19/36 Loss: 5.9400\n",
      "Epoch 2 Step 20/36 Loss: 4.4897\n",
      "Epoch 2 Step 21/36 Loss: 5.3220\n",
      "Epoch 2 Step 22/36 Loss: 4.9882\n",
      "Epoch 2 Step 23/36 Loss: 5.6825\n",
      "Epoch 2 Step 24/36 Loss: 4.6518\n",
      "Epoch 2 Step 25/36 Loss: 4.5800\n",
      "Epoch 2 Step 26/36 Loss: 5.2912\n",
      "Epoch 2 Step 27/36 Loss: 5.6192\n",
      "Epoch 2 Step 28/36 Loss: 5.6688\n",
      "Epoch 2 Step 29/36 Loss: 5.8977\n",
      "Epoch 2 Step 30/36 Loss: 6.6526\n",
      "Epoch 2 Step 31/36 Loss: 5.5527\n",
      "Epoch 2 Step 32/36 Loss: 5.4789\n",
      "Epoch 2 Step 33/36 Loss: 5.9860\n",
      "Epoch 2 Step 34/36 Loss: 4.7907\n",
      "Epoch 2 Step 35/36 Loss: 4.5758\n",
      "Epoch 2 Step 36/36 Loss: 5.5581\n",
      "Epoch 2 Step 1/36 Loss: 4.5113\n",
      "Epoch 2 Step 2/36 Loss: 5.6153\n",
      "Epoch 2 Step 3/36 Loss: 5.0356\n",
      "Epoch 2 Step 4/36 Loss: 5.6707\n",
      "Epoch 2 Step 5/36 Loss: 6.0463\n",
      "Epoch 2 Step 6/36 Loss: 5.4069\n",
      "Epoch 2 Step 7/36 Loss: 6.1869\n",
      "Epoch 2 Step 8/36 Loss: 6.1291\n",
      "Epoch 2 Step 9/36 Loss: 5.4774\n",
      "Epoch 2 Step 10/36 Loss: 4.9910\n",
      "Epoch 2 Step 11/36 Loss: 4.6723\n",
      "Epoch 2 Step 12/36 Loss: 5.7629\n",
      "Epoch 2 Step 13/36 Loss: 5.4828\n",
      "Epoch 2 Step 14/36 Loss: 5.4778\n",
      "Epoch 2 Step 15/36 Loss: 5.4427\n",
      "Epoch 2 Step 16/36 Loss: 6.6929\n",
      "Epoch 2 Step 17/36 Loss: 5.1583\n",
      "Epoch 2 Step 18/36 Loss: 4.6761\n",
      "Epoch 2 Step 19/36 Loss: 5.0207\n",
      "Epoch 2 Step 20/36 Loss: 3.9437\n",
      "Epoch 2 Step 21/36 Loss: 4.3647\n",
      "Epoch 2 Step 22/36 Loss: 5.2098\n",
      "Epoch 2 Step 23/36 Loss: 5.6909\n",
      "Epoch 2 Step 24/36 Loss: 4.4321\n",
      "Epoch 2 Step 25/36 Loss: 4.8651\n",
      "Epoch 2 Step 26/36 Loss: 5.3127\n",
      "Epoch 2 Step 27/36 Loss: 5.6711\n",
      "Epoch 2 Step 28/36 Loss: 5.8539\n",
      "Epoch 2 Step 29/36 Loss: 5.9811\n",
      "Epoch 2 Step 30/36 Loss: 6.1820\n",
      "Epoch 2 Step 31/36 Loss: 5.7652\n",
      "Epoch 2 Step 32/36 Loss: 5.0148\n",
      "Epoch 2 Step 33/36 Loss: 5.3649\n",
      "Epoch 2 Step 34/36 Loss: 4.7685\n",
      "Epoch 2 Step 35/36 Loss: 5.1962\n",
      "Epoch 2 Step 36/36 Loss: 4.9686\n",
      "Epoch 2 Step 1/36 Loss: 5.4683\n",
      "Epoch 2 Step 2/36 Loss: 7.6357\n",
      "Epoch 2 Step 3/36 Loss: 5.2707\n",
      "Epoch 2 Step 4/36 Loss: 5.2158\n",
      "Epoch 2 Step 5/36 Loss: 5.2207\n",
      "Epoch 2 Step 6/36 Loss: 5.4974\n",
      "Epoch 2 Step 7/36 Loss: 4.6338\n",
      "Epoch 2 Step 8/36 Loss: 3.8410\n",
      "Epoch 2 Step 9/36 Loss: 5.1797\n",
      "Epoch 2 Step 10/36 Loss: 5.5664\n",
      "Epoch 2 Step 11/36 Loss: 5.8962\n",
      "Epoch 2 Step 12/36 Loss: 5.9419\n",
      "Epoch 2 Step 13/36 Loss: 4.6093\n",
      "Epoch 2 Step 14/36 Loss: 6.7549\n",
      "Epoch 2 Step 15/36 Loss: 5.5669\n",
      "Epoch 2 Step 16/36 Loss: 5.7728\n",
      "Epoch 2 Step 17/36 Loss: 5.0502\n",
      "Epoch 2 Step 18/36 Loss: 5.0604\n",
      "Epoch 2 Step 19/36 Loss: 5.1090\n",
      "Epoch 2 Step 20/36 Loss: 5.2847\n",
      "Epoch 2 Step 21/36 Loss: 5.1216\n",
      "Epoch 2 Step 22/36 Loss: 6.1001\n",
      "Epoch 2 Step 23/36 Loss: 5.0170\n",
      "Epoch 2 Step 24/36 Loss: 6.0463\n",
      "Epoch 2 Step 25/36 Loss: 4.7827\n",
      "Epoch 2 Step 26/36 Loss: 5.0185\n",
      "Epoch 2 Step 27/36 Loss: 4.4119\n",
      "Epoch 2 Step 28/36 Loss: 6.2704\n",
      "Epoch 2 Step 29/36 Loss: 5.6918\n",
      "Epoch 2 Step 30/36 Loss: 6.8161\n",
      "Epoch 2 Step 31/36 Loss: 5.9882\n",
      "Epoch 2 Step 32/36 Loss: 5.2668\n",
      "Epoch 2 Step 33/36 Loss: 5.5172\n",
      "Epoch 2 Step 34/36 Loss: 5.9489\n",
      "Epoch 2 Step 35/36 Loss: 5.5199\n",
      "Epoch 2 Step 36/36 Loss: 5.8059\n",
      "Epoch 2 Step 1/36 Loss: 4.9653\n",
      "Epoch 2 Step 2/36 Loss: 4.5647\n",
      "Epoch 2 Step 3/36 Loss: 5.4442\n",
      "Epoch 2 Step 4/36 Loss: 5.7539\n",
      "Epoch 2 Step 5/36 Loss: 5.8975\n",
      "Epoch 2 Step 6/36 Loss: 6.0837\n",
      "Epoch 2 Step 7/36 Loss: 4.6955\n",
      "Epoch 2 Step 8/36 Loss: 5.0138\n",
      "Epoch 2 Step 9/36 Loss: 5.9426\n",
      "Epoch 2 Step 10/36 Loss: 5.5111\n",
      "Epoch 2 Step 11/36 Loss: 5.3551\n",
      "Epoch 2 Step 12/36 Loss: 4.3354\n",
      "Epoch 2 Step 13/36 Loss: 4.2288\n",
      "Epoch 2 Step 14/36 Loss: 5.0893\n",
      "Epoch 2 Step 15/36 Loss: 5.0454\n",
      "Epoch 2 Step 16/36 Loss: 5.0330\n",
      "Epoch 2 Step 17/36 Loss: 5.3554\n",
      "Epoch 2 Step 18/36 Loss: 5.3940\n",
      "Epoch 2 Step 19/36 Loss: 4.2730\n",
      "Epoch 2 Step 20/36 Loss: 6.2022\n",
      "Epoch 2 Step 21/36 Loss: 5.3365\n",
      "Epoch 2 Step 22/36 Loss: 5.7159\n",
      "Epoch 2 Step 23/36 Loss: 5.6198\n",
      "Epoch 2 Step 24/36 Loss: 5.4254\n",
      "Epoch 2 Step 25/36 Loss: 5.8442\n",
      "Epoch 2 Step 26/36 Loss: 5.0036\n",
      "Epoch 2 Step 27/36 Loss: 4.9431\n",
      "Epoch 2 Step 28/36 Loss: 4.5123\n",
      "Epoch 2 Step 29/36 Loss: 4.7349\n",
      "Epoch 2 Step 30/36 Loss: 5.9354\n",
      "Epoch 2 Step 31/36 Loss: 5.2950\n",
      "Epoch 2 Step 32/36 Loss: 5.8275\n",
      "Epoch 2 Step 33/36 Loss: 5.9778\n",
      "Epoch 2 Step 34/36 Loss: 5.0136\n",
      "Epoch 2 Step 35/36 Loss: 7.5260\n",
      "Epoch 2 Step 36/36 Loss: 6.4949\n",
      "Epoch 2 Step 1/36 Loss: 5.7225\n",
      "Epoch 2 Step 2/36 Loss: 4.7807\n",
      "Epoch 2 Step 3/36 Loss: 6.2856\n",
      "Epoch 2 Step 4/36 Loss: 4.6559\n",
      "Epoch 2 Step 5/36 Loss: 4.3714\n",
      "Epoch 2 Step 6/36 Loss: 5.9331\n",
      "Epoch 2 Step 7/36 Loss: 5.5762\n",
      "Epoch 2 Step 8/36 Loss: 4.8012\n",
      "Epoch 2 Step 9/36 Loss: 5.5948\n",
      "Epoch 2 Step 10/36 Loss: 4.7902\n",
      "Epoch 2 Step 11/36 Loss: 6.5179\n",
      "Epoch 2 Step 12/36 Loss: 4.9917\n",
      "Epoch 2 Step 13/36 Loss: 6.5289\n",
      "Epoch 2 Step 14/36 Loss: 5.5984\n",
      "Epoch 2 Step 15/36 Loss: 4.1700\n",
      "Epoch 2 Step 16/36 Loss: 5.7965\n",
      "Epoch 2 Step 17/36 Loss: 6.2156\n",
      "Epoch 2 Step 18/36 Loss: 5.9582\n",
      "Epoch 2 Step 19/36 Loss: 5.5492\n",
      "Epoch 2 Step 20/36 Loss: 5.5200\n",
      "Epoch 2 Step 21/36 Loss: 5.2626\n",
      "Epoch 2 Step 22/36 Loss: 5.1079\n",
      "Epoch 2 Step 23/36 Loss: 4.0713\n",
      "Epoch 2 Step 24/36 Loss: 5.8870\n",
      "Epoch 2 Step 25/36 Loss: 6.7046\n",
      "Epoch 2 Step 26/36 Loss: 5.7322\n",
      "Epoch 2 Step 27/36 Loss: 4.8807\n",
      "Epoch 2 Step 28/36 Loss: 4.9490\n",
      "Epoch 2 Step 29/36 Loss: 5.7792\n",
      "Epoch 2 Step 30/36 Loss: 4.4152\n",
      "Epoch 2 Step 31/36 Loss: 5.2373\n",
      "Epoch 2 Step 32/36 Loss: 5.6301\n",
      "Epoch 2 Step 33/36 Loss: 5.5372\n",
      "Epoch 2 Step 34/36 Loss: 4.2199\n",
      "Epoch 2 Step 35/36 Loss: 7.1241\n",
      "Epoch 2 Step 36/36 Loss: 5.9903\n",
      "Epoch 2 Step 1/36 Loss: 5.8734\n",
      "Epoch 2 Step 2/36 Loss: 4.7823\n",
      "Epoch 2 Step 3/36 Loss: 5.8620\n",
      "Epoch 2 Step 4/36 Loss: 6.1298\n",
      "Epoch 2 Step 5/36 Loss: 5.2699\n",
      "Epoch 2 Step 6/36 Loss: 4.7861\n",
      "Epoch 2 Step 7/36 Loss: 7.4145\n",
      "Epoch 2 Step 8/36 Loss: 3.8778\n",
      "Epoch 2 Step 9/36 Loss: 5.4186\n",
      "Epoch 2 Step 10/36 Loss: 6.7775\n",
      "Epoch 2 Step 11/36 Loss: 4.0938\n",
      "Epoch 2 Step 12/36 Loss: 6.2253\n",
      "Epoch 2 Step 13/36 Loss: 5.1879\n",
      "Epoch 2 Step 14/36 Loss: 4.7630\n",
      "Epoch 2 Step 15/36 Loss: 6.0637\n",
      "Epoch 2 Step 16/36 Loss: 6.1124\n",
      "Epoch 2 Step 17/36 Loss: 6.2779\n",
      "Epoch 2 Step 18/36 Loss: 5.3715\n",
      "Epoch 2 Step 19/36 Loss: 5.2046\n",
      "Epoch 2 Step 20/36 Loss: 4.8769\n",
      "Epoch 2 Step 21/36 Loss: 5.1773\n",
      "Epoch 2 Step 22/36 Loss: 4.6571\n",
      "Epoch 2 Step 23/36 Loss: 4.8820\n",
      "Epoch 2 Step 24/36 Loss: 4.8649\n",
      "Epoch 2 Step 25/36 Loss: 4.8997\n",
      "Epoch 2 Step 26/36 Loss: 5.1909\n",
      "Epoch 2 Step 27/36 Loss: 4.3772\n",
      "Epoch 2 Step 28/36 Loss: 4.8657\n",
      "Epoch 2 Step 29/36 Loss: 5.7895\n",
      "Epoch 2 Step 30/36 Loss: 4.1378\n",
      "Epoch 2 Step 31/36 Loss: 6.2236\n",
      "Epoch 2 Step 32/36 Loss: 6.3575\n",
      "Epoch 2 Step 33/36 Loss: 4.7617\n",
      "Epoch 2 Step 34/36 Loss: 5.1110\n",
      "Epoch 2 Step 35/36 Loss: 5.2813\n",
      "Epoch 2 Step 36/36 Loss: 5.3953\n",
      "Epoch 2 Step 1/36 Loss: 5.5564\n",
      "Epoch 2 Step 2/36 Loss: 5.9046\n",
      "Epoch 2 Step 3/36 Loss: 4.3179\n",
      "Epoch 2 Step 4/36 Loss: 5.9983\n",
      "Epoch 2 Step 5/36 Loss: 4.1915\n",
      "Epoch 2 Step 6/36 Loss: 6.3155\n",
      "Epoch 2 Step 7/36 Loss: 6.5001\n",
      "Epoch 2 Step 8/36 Loss: 7.5484\n",
      "Epoch 2 Step 9/36 Loss: 4.9231\n",
      "Epoch 2 Step 10/36 Loss: 6.6077\n",
      "Epoch 2 Step 11/36 Loss: 4.8085\n",
      "Epoch 2 Step 12/36 Loss: 5.7721\n",
      "Epoch 2 Step 13/36 Loss: 6.6221\n",
      "Epoch 2 Step 14/36 Loss: 4.6972\n",
      "Epoch 2 Step 15/36 Loss: 6.4281\n",
      "Epoch 2 Step 16/36 Loss: 6.4994\n",
      "Epoch 2 Step 17/36 Loss: 5.5581\n",
      "Epoch 2 Step 18/36 Loss: 4.4730\n",
      "Epoch 2 Step 19/36 Loss: 4.8001\n",
      "Epoch 2 Step 20/36 Loss: 5.6643\n",
      "Epoch 2 Step 21/36 Loss: 4.5158\n",
      "Epoch 2 Step 22/36 Loss: 4.9560\n",
      "Epoch 2 Step 23/36 Loss: 5.0269\n",
      "Epoch 2 Step 24/36 Loss: 6.0815\n",
      "Epoch 2 Step 25/36 Loss: 4.5837\n",
      "Epoch 2 Step 26/36 Loss: 5.3830\n",
      "Epoch 2 Step 27/36 Loss: 4.8306\n",
      "Epoch 2 Step 28/36 Loss: 6.3088\n",
      "Epoch 2 Step 29/36 Loss: 6.3523\n",
      "Epoch 2 Step 30/36 Loss: 4.1963\n",
      "Epoch 2 Step 31/36 Loss: 5.3642\n",
      "Epoch 2 Step 32/36 Loss: 5.6367\n",
      "Epoch 2 Step 33/36 Loss: 5.6521\n",
      "Epoch 2 Step 34/36 Loss: 4.8958\n",
      "Epoch 2 Step 35/36 Loss: 6.2564\n",
      "Epoch 2 Step 36/36 Loss: 6.2627\n",
      "Epoch 2 Step 1/36 Loss: 5.5343\n",
      "Epoch 2 Step 2/36 Loss: 6.6284\n",
      "Epoch 2 Step 3/36 Loss: 4.7927\n",
      "Epoch 2 Step 4/36 Loss: 5.7236\n",
      "Epoch 2 Step 5/36 Loss: 6.6822\n",
      "Epoch 2 Step 6/36 Loss: 4.8734\n",
      "Epoch 2 Step 7/36 Loss: 5.9702\n",
      "Epoch 2 Step 8/36 Loss: 5.0822\n",
      "Epoch 2 Step 9/36 Loss: 4.6374\n",
      "Epoch 2 Step 10/36 Loss: 6.0793\n",
      "Epoch 2 Step 11/36 Loss: 6.0719\n",
      "Epoch 2 Step 12/36 Loss: 6.0159\n",
      "Epoch 2 Step 13/36 Loss: 5.2756\n",
      "Epoch 2 Step 14/36 Loss: 5.6662\n",
      "Epoch 2 Step 15/36 Loss: 5.8887\n",
      "Epoch 2 Step 16/36 Loss: 5.5307\n",
      "Epoch 2 Step 17/36 Loss: 6.0427\n",
      "Epoch 2 Step 18/36 Loss: 4.5415\n",
      "Epoch 2 Step 19/36 Loss: 6.0042\n",
      "Epoch 2 Step 20/36 Loss: 5.1274\n",
      "Epoch 2 Step 21/36 Loss: 5.4208\n",
      "Epoch 2 Step 22/36 Loss: 4.6161\n",
      "Epoch 2 Step 23/36 Loss: 5.3680\n",
      "Epoch 2 Step 24/36 Loss: 5.2984\n",
      "Epoch 2 Step 25/36 Loss: 5.3526\n",
      "Epoch 2 Step 26/36 Loss: 4.5568\n",
      "Epoch 2 Step 27/36 Loss: 5.6572\n",
      "Epoch 2 Step 28/36 Loss: 5.2371\n",
      "Epoch 2 Step 29/36 Loss: 5.0905\n",
      "Epoch 2 Step 30/36 Loss: 5.4534\n",
      "Epoch 2 Step 31/36 Loss: 5.9874\n",
      "Epoch 2 Step 32/36 Loss: 4.2947\n",
      "Epoch 2 Step 33/36 Loss: 5.6531\n",
      "Epoch 2 Step 34/36 Loss: 6.4805\n",
      "Epoch 2 Step 35/36 Loss: 5.2825\n",
      "Epoch 2 Step 36/36 Loss: 4.4863\n",
      "Epoch 2 Step 1/36 Loss: 5.6207\n",
      "Epoch 2 Step 2/36 Loss: 6.4540\n",
      "Epoch 2 Step 3/36 Loss: 5.5799\n",
      "Epoch 2 Step 4/36 Loss: 4.6047\n",
      "Epoch 2 Step 5/36 Loss: 5.1512\n",
      "Epoch 2 Step 6/36 Loss: 5.5537\n",
      "Epoch 2 Step 7/36 Loss: 4.6550\n",
      "Epoch 2 Step 8/36 Loss: 5.7115\n",
      "Epoch 2 Step 9/36 Loss: 4.3748\n",
      "Epoch 2 Step 10/36 Loss: 5.6064\n",
      "Epoch 2 Step 11/36 Loss: 5.8120\n",
      "Epoch 2 Step 12/36 Loss: 4.6771\n",
      "Epoch 2 Step 13/36 Loss: 6.3582\n",
      "Epoch 2 Step 14/36 Loss: 6.5684\n",
      "Epoch 2 Step 15/36 Loss: 5.2419\n",
      "Epoch 2 Step 16/36 Loss: 5.9062\n",
      "Epoch 2 Step 17/36 Loss: 5.3013\n",
      "Epoch 2 Step 18/36 Loss: 6.6961\n",
      "Epoch 2 Step 19/36 Loss: 6.4454\n",
      "Epoch 2 Step 20/36 Loss: 5.4986\n",
      "Epoch 2 Step 21/36 Loss: 6.7678\n",
      "Epoch 2 Step 22/36 Loss: 4.8289\n",
      "Epoch 2 Step 23/36 Loss: 5.1381\n",
      "Epoch 2 Step 24/36 Loss: 5.6589\n",
      "Epoch 2 Step 25/36 Loss: 4.6759\n",
      "Epoch 2 Step 26/36 Loss: 5.4357\n",
      "Epoch 2 Step 27/36 Loss: 5.8642\n",
      "Epoch 2 Step 28/36 Loss: 4.6546\n",
      "Epoch 2 Step 29/36 Loss: 6.1850\n",
      "Epoch 2 Step 30/36 Loss: 4.2602\n",
      "Epoch 2 Step 31/36 Loss: 4.5675\n",
      "Epoch 2 Step 32/36 Loss: 5.9032\n",
      "Epoch 2 Step 33/36 Loss: 5.8179\n",
      "Epoch 2 Step 34/36 Loss: 4.8364\n",
      "Epoch 2 Step 35/36 Loss: 4.9871\n",
      "Epoch 2 Step 36/36 Loss: 6.2510\n",
      "Epoch 2 Step 1/36 Loss: 6.0181\n",
      "Epoch 2 Step 2/36 Loss: 6.3855\n",
      "Epoch 2 Step 3/36 Loss: 5.1205\n",
      "Epoch 2 Step 4/36 Loss: 5.7928\n",
      "Epoch 2 Step 5/36 Loss: 5.2589\n",
      "Epoch 2 Step 6/36 Loss: 5.3371\n",
      "Epoch 2 Step 7/36 Loss: 5.6089\n",
      "Epoch 2 Step 8/36 Loss: 5.4367\n",
      "Epoch 2 Step 9/36 Loss: 5.3952\n",
      "Epoch 2 Step 10/36 Loss: 4.0491\n",
      "Epoch 2 Step 11/36 Loss: 5.2822\n",
      "Epoch 2 Step 12/36 Loss: 5.0534\n",
      "Epoch 2 Step 13/36 Loss: 5.6115\n",
      "Epoch 2 Step 14/36 Loss: 6.1805\n",
      "Epoch 2 Step 15/36 Loss: 6.0440\n",
      "Epoch 2 Step 16/36 Loss: 5.3608\n",
      "Epoch 2 Step 17/36 Loss: 6.0789\n",
      "Epoch 2 Step 18/36 Loss: 5.9483\n",
      "Epoch 2 Step 19/36 Loss: 6.3599\n",
      "Epoch 2 Step 20/36 Loss: 5.1379\n",
      "Epoch 2 Step 21/36 Loss: 4.9197\n",
      "Epoch 2 Step 22/36 Loss: 6.2686\n",
      "Epoch 2 Step 23/36 Loss: 5.5533\n",
      "Epoch 2 Step 24/36 Loss: 4.0242\n",
      "Epoch 2 Step 25/36 Loss: 5.6637\n",
      "Epoch 2 Step 26/36 Loss: 5.0335\n",
      "Epoch 2 Step 27/36 Loss: 5.1250\n",
      "Epoch 2 Step 28/36 Loss: 5.1918\n",
      "Epoch 2 Step 29/36 Loss: 6.0108\n",
      "Epoch 2 Step 30/36 Loss: 6.4962\n",
      "Epoch 2 Step 31/36 Loss: 4.5207\n",
      "Epoch 2 Step 32/36 Loss: 5.5853\n",
      "Epoch 2 Step 33/36 Loss: 5.8176\n",
      "Epoch 2 Step 34/36 Loss: 4.9894\n",
      "Epoch 2 Step 35/36 Loss: 5.0336\n",
      "Epoch 2 Step 36/36 Loss: 6.7719\n",
      "Epoch 2 Step 1/36 Loss: 6.0423\n",
      "Epoch 2 Step 2/36 Loss: 6.4759\n",
      "Epoch 2 Step 3/36 Loss: 4.8907\n",
      "Epoch 2 Step 4/36 Loss: 3.5438\n",
      "Epoch 2 Step 5/36 Loss: 6.1418\n",
      "Epoch 2 Step 6/36 Loss: 5.6237\n",
      "Epoch 2 Step 7/36 Loss: 5.2509\n",
      "Epoch 2 Step 8/36 Loss: 5.3376\n",
      "Epoch 2 Step 9/36 Loss: 4.7927\n",
      "Epoch 2 Step 10/36 Loss: 4.8379\n",
      "Epoch 2 Step 11/36 Loss: 6.1632\n",
      "Epoch 2 Step 12/36 Loss: 4.9460\n",
      "Epoch 2 Step 13/36 Loss: 5.5281\n",
      "Epoch 2 Step 14/36 Loss: 5.2110\n",
      "Epoch 2 Step 15/36 Loss: 5.0296\n",
      "Epoch 2 Step 16/36 Loss: 5.3774\n",
      "Epoch 2 Step 17/36 Loss: 5.6383\n",
      "Epoch 2 Step 18/36 Loss: 5.1597\n",
      "Epoch 2 Step 19/36 Loss: 6.4771\n",
      "Epoch 2 Step 20/36 Loss: 5.0265\n",
      "Epoch 2 Step 21/36 Loss: 5.3920\n",
      "Epoch 2 Step 22/36 Loss: 5.6304\n",
      "Epoch 2 Step 23/36 Loss: 5.1852\n",
      "Epoch 2 Step 24/36 Loss: 5.6579\n",
      "Epoch 2 Step 25/36 Loss: 5.1250\n",
      "Epoch 2 Step 26/36 Loss: 5.3275\n",
      "Epoch 2 Step 27/36 Loss: 6.3664\n",
      "Epoch 2 Step 28/36 Loss: 5.2743\n",
      "Epoch 2 Step 29/36 Loss: 5.8932\n",
      "Epoch 2 Step 30/36 Loss: 5.6470\n",
      "Epoch 2 Step 31/36 Loss: 5.3875\n",
      "Epoch 2 Step 32/36 Loss: 6.4699\n",
      "Epoch 2 Step 33/36 Loss: 6.3966\n",
      "Epoch 2 Step 34/36 Loss: 5.6107\n",
      "Epoch 2 Step 35/36 Loss: 4.8786\n",
      "Epoch 2 Step 36/36 Loss: 5.4969\n",
      "Epoch 2 Step 1/36 Loss: 4.6836\n",
      "Epoch 2 Step 2/36 Loss: 4.7131\n",
      "Epoch 2 Step 3/36 Loss: 5.1104\n",
      "Epoch 2 Step 4/36 Loss: 5.6168\n",
      "Epoch 2 Step 5/36 Loss: 4.8109\n",
      "Epoch 2 Step 6/36 Loss: 4.5984\n",
      "Epoch 2 Step 7/36 Loss: 4.8116\n",
      "Epoch 2 Step 8/36 Loss: 6.2681\n",
      "Epoch 2 Step 9/36 Loss: 6.1594\n",
      "Epoch 2 Step 10/36 Loss: 5.5091\n",
      "Epoch 2 Step 11/36 Loss: 6.3121\n",
      "Epoch 2 Step 12/36 Loss: 5.2935\n",
      "Epoch 2 Step 13/36 Loss: 6.9012\n",
      "Epoch 2 Step 14/36 Loss: 4.5640\n",
      "Epoch 2 Step 15/36 Loss: 5.5947\n",
      "Epoch 2 Step 16/36 Loss: 5.6720\n",
      "Epoch 2 Step 17/36 Loss: 4.9032\n",
      "Epoch 2 Step 18/36 Loss: 5.0991\n",
      "Epoch 2 Step 19/36 Loss: 3.9560\n",
      "Epoch 2 Step 20/36 Loss: 5.3564\n",
      "Epoch 2 Step 21/36 Loss: 6.1995\n",
      "Epoch 2 Step 22/36 Loss: 5.4277\n",
      "Epoch 2 Step 23/36 Loss: 4.2535\n",
      "Epoch 2 Step 24/36 Loss: 5.6442\n",
      "Epoch 2 Step 25/36 Loss: 6.0011\n",
      "Epoch 2 Step 26/36 Loss: 5.4349\n",
      "Epoch 2 Step 27/36 Loss: 4.7747\n",
      "Epoch 2 Step 28/36 Loss: 6.1095\n",
      "Epoch 2 Step 29/36 Loss: 5.2018\n",
      "Epoch 2 Step 30/36 Loss: 4.5421\n",
      "Epoch 2 Step 31/36 Loss: 5.8864\n",
      "Epoch 2 Step 32/36 Loss: 6.0558\n",
      "Epoch 2 Step 33/36 Loss: 5.0755\n",
      "Epoch 2 Step 34/36 Loss: 4.5357\n",
      "Epoch 2 Step 35/36 Loss: 4.9375\n",
      "Epoch 2 Step 36/36 Loss: 6.0191\n",
      "Epoch 2 Step 1/36 Loss: 6.0570\n",
      "Epoch 2 Step 2/36 Loss: 6.4078\n",
      "Epoch 2 Step 3/36 Loss: 5.0854\n",
      "Epoch 2 Step 4/36 Loss: 5.9799\n",
      "Epoch 2 Step 5/36 Loss: 5.2400\n",
      "Epoch 2 Step 6/36 Loss: 6.0052\n",
      "Epoch 2 Step 7/36 Loss: 5.1829\n",
      "Epoch 2 Step 8/36 Loss: 4.6940\n",
      "Epoch 2 Step 9/36 Loss: 5.4289\n",
      "Epoch 2 Step 10/36 Loss: 4.8789\n",
      "Epoch 2 Step 11/36 Loss: 7.0088\n",
      "Epoch 2 Step 12/36 Loss: 4.6435\n",
      "Epoch 2 Step 13/36 Loss: 4.0484\n",
      "Epoch 2 Step 14/36 Loss: 4.3509\n",
      "Epoch 2 Step 15/36 Loss: 5.7667\n",
      "Epoch 2 Step 16/36 Loss: 5.5854\n",
      "Epoch 2 Step 17/36 Loss: 4.5536\n",
      "Epoch 2 Step 18/36 Loss: 5.0882\n",
      "Epoch 2 Step 19/36 Loss: 5.2903\n",
      "Epoch 2 Step 20/36 Loss: 6.4075\n",
      "Epoch 2 Step 21/36 Loss: 5.6572\n",
      "Epoch 2 Step 22/36 Loss: 5.5837\n",
      "Epoch 2 Step 23/36 Loss: 5.3368\n",
      "Epoch 2 Step 24/36 Loss: 6.0268\n",
      "Epoch 2 Step 25/36 Loss: 6.6991\n",
      "Epoch 2 Step 26/36 Loss: 7.0332\n",
      "Epoch 2 Step 27/36 Loss: 6.4813\n",
      "Epoch 2 Step 28/36 Loss: 6.0281\n",
      "Epoch 2 Step 29/36 Loss: 4.8796\n",
      "Epoch 2 Step 30/36 Loss: 6.4241\n",
      "Epoch 2 Step 31/36 Loss: 4.1660\n",
      "Epoch 2 Step 32/36 Loss: 6.1380\n",
      "Epoch 2 Step 33/36 Loss: 5.5989\n",
      "Epoch 2 Step 34/36 Loss: 4.5212\n",
      "Epoch 2 Step 35/36 Loss: 5.2851\n",
      "Epoch 2 Step 36/36 Loss: 5.0875\n",
      "Epoch 2 Step 1/36 Loss: 5.0428\n",
      "Epoch 2 Step 2/36 Loss: 5.3805\n",
      "Epoch 2 Step 3/36 Loss: 6.3705\n",
      "Epoch 2 Step 4/36 Loss: 4.9284\n",
      "Epoch 2 Step 5/36 Loss: 6.2735\n",
      "Epoch 2 Step 6/36 Loss: 5.5591\n",
      "Epoch 2 Step 7/36 Loss: 6.1296\n",
      "Epoch 2 Step 8/36 Loss: 5.1017\n",
      "Epoch 2 Step 9/36 Loss: 6.5548\n",
      "Epoch 2 Step 10/36 Loss: 5.6474\n",
      "Epoch 2 Step 11/36 Loss: 5.7786\n",
      "Epoch 2 Step 12/36 Loss: 5.1047\n",
      "Epoch 2 Step 13/36 Loss: 6.3507\n",
      "Epoch 2 Step 14/36 Loss: 5.1313\n",
      "Epoch 2 Step 15/36 Loss: 5.1977\n",
      "Epoch 2 Step 16/36 Loss: 5.6868\n",
      "Epoch 2 Step 17/36 Loss: 5.7538\n",
      "Epoch 2 Step 18/36 Loss: 5.7750\n",
      "Epoch 2 Step 19/36 Loss: 4.7264\n",
      "Epoch 2 Step 20/36 Loss: 4.9113\n",
      "Epoch 2 Step 21/36 Loss: 5.5596\n",
      "Epoch 2 Step 22/36 Loss: 5.4814\n",
      "Epoch 2 Step 23/36 Loss: 4.3725\n",
      "Epoch 2 Step 24/36 Loss: 4.5195\n",
      "Epoch 2 Step 25/36 Loss: 8.1903\n",
      "Epoch 2 Step 26/36 Loss: 7.4279\n",
      "Epoch 2 Step 27/36 Loss: 5.5590\n",
      "Epoch 2 Step 28/36 Loss: 5.1782\n",
      "Epoch 2 Step 29/36 Loss: 5.4476\n",
      "Epoch 2 Step 30/36 Loss: 5.9150\n",
      "Epoch 2 Step 31/36 Loss: 5.4402\n",
      "Epoch 2 Step 32/36 Loss: 4.7625\n",
      "Epoch 2 Step 33/36 Loss: 5.6554\n",
      "Epoch 2 Step 34/36 Loss: 5.5475\n",
      "Epoch 2 Step 35/36 Loss: 5.3988\n",
      "Epoch 2 Step 36/36 Loss: 4.8502\n",
      "Epoch 2 Step 1/36 Loss: 6.8456\n",
      "Epoch 2 Step 2/36 Loss: 4.8543\n",
      "Epoch 2 Step 3/36 Loss: 4.6612\n",
      "Epoch 2 Step 4/36 Loss: 6.1076\n",
      "Epoch 2 Step 5/36 Loss: 5.8857\n",
      "Epoch 2 Step 6/36 Loss: 4.4168\n",
      "Epoch 2 Step 7/36 Loss: 6.8432\n",
      "Epoch 2 Step 8/36 Loss: 5.6661\n",
      "Epoch 2 Step 9/36 Loss: 4.3293\n",
      "Epoch 2 Step 10/36 Loss: 4.9902\n",
      "Epoch 2 Step 11/36 Loss: 4.4172\n",
      "Epoch 2 Step 12/36 Loss: 4.5276\n",
      "Epoch 2 Step 13/36 Loss: 6.4201\n",
      "Epoch 2 Step 14/36 Loss: 5.5048\n",
      "Epoch 2 Step 15/36 Loss: 4.8572\n",
      "Epoch 2 Step 16/36 Loss: 7.4185\n",
      "Epoch 2 Step 17/36 Loss: 5.1370\n",
      "Epoch 2 Step 18/36 Loss: 5.5509\n",
      "Epoch 2 Step 19/36 Loss: 6.2882\n",
      "Epoch 2 Step 20/36 Loss: 6.4945\n",
      "Epoch 2 Step 21/36 Loss: 4.0827\n",
      "Epoch 2 Step 22/36 Loss: 6.5085\n",
      "Epoch 2 Step 23/36 Loss: 5.4910\n",
      "Epoch 2 Step 24/36 Loss: 5.2875\n",
      "Epoch 2 Step 25/36 Loss: 5.3818\n",
      "Epoch 2 Step 26/36 Loss: 5.2280\n",
      "Epoch 2 Step 27/36 Loss: 4.6842\n",
      "Epoch 2 Step 28/36 Loss: 6.0135\n",
      "Epoch 2 Step 29/36 Loss: 6.2030\n",
      "Epoch 2 Step 30/36 Loss: 7.1564\n",
      "Epoch 2 Step 31/36 Loss: 5.7680\n",
      "Epoch 2 Step 32/36 Loss: 4.6909\n",
      "Epoch 2 Step 33/36 Loss: 5.4002\n",
      "Epoch 2 Step 34/36 Loss: 4.7580\n",
      "Epoch 2 Step 35/36 Loss: 5.8059\n",
      "Epoch 2 Step 36/36 Loss: 4.4951\n",
      "Epoch 2 Step 1/36 Loss: 4.4176\n",
      "Epoch 2 Step 2/36 Loss: 5.8214\n",
      "Epoch 2 Step 3/36 Loss: 5.3080\n",
      "Epoch 2 Step 4/36 Loss: 6.2356\n",
      "Epoch 2 Step 5/36 Loss: 5.1632\n",
      "Epoch 2 Step 6/36 Loss: 4.4275\n",
      "Epoch 2 Step 7/36 Loss: 5.2421\n",
      "Epoch 2 Step 8/36 Loss: 4.7393\n",
      "Epoch 2 Step 9/36 Loss: 5.0953\n",
      "Epoch 2 Step 10/36 Loss: 5.4445\n",
      "Epoch 2 Step 11/36 Loss: 6.2704\n",
      "Epoch 2 Step 12/36 Loss: 3.9928\n",
      "Epoch 2 Step 13/36 Loss: 5.4082\n",
      "Epoch 2 Step 14/36 Loss: 5.0074\n",
      "Epoch 2 Step 15/36 Loss: 5.3033\n",
      "Epoch 2 Step 16/36 Loss: 5.5378\n",
      "Epoch 2 Step 17/36 Loss: 4.3239\n",
      "Epoch 2 Step 18/36 Loss: 5.2766\n",
      "Epoch 2 Step 19/36 Loss: 5.0845\n",
      "Epoch 2 Step 20/36 Loss: 5.3700\n",
      "Epoch 2 Step 21/36 Loss: 5.6798\n",
      "Epoch 2 Step 22/36 Loss: 3.7722\n",
      "Epoch 2 Step 23/36 Loss: 6.9728\n",
      "Epoch 2 Step 24/36 Loss: 5.8933\n",
      "Epoch 2 Step 25/36 Loss: 5.6985\n",
      "Epoch 2 Step 26/36 Loss: 5.0760\n",
      "Epoch 2 Step 27/36 Loss: 6.2132\n",
      "Epoch 2 Step 28/36 Loss: 5.3632\n",
      "Epoch 2 Step 29/36 Loss: 6.3868\n",
      "Epoch 2 Step 30/36 Loss: 5.0356\n",
      "Epoch 2 Step 31/36 Loss: 6.3618\n",
      "Epoch 2 Step 32/36 Loss: 5.5930\n",
      "Epoch 2 Step 33/36 Loss: 5.7631\n",
      "Epoch 2 Step 34/36 Loss: 4.4784\n",
      "Epoch 2 Step 35/36 Loss: 5.5980\n",
      "Epoch 2 Step 36/36 Loss: 6.9125\n",
      "Epoch 2 Step 1/36 Loss: 5.1419\n",
      "Epoch 2 Step 2/36 Loss: 4.5613\n",
      "Epoch 2 Step 3/36 Loss: 6.2403\n",
      "Epoch 2 Step 4/36 Loss: 4.9618\n",
      "Epoch 2 Step 5/36 Loss: 5.8760\n",
      "Epoch 2 Step 6/36 Loss: 5.2858\n",
      "Epoch 2 Step 7/36 Loss: 4.7964\n",
      "Epoch 2 Step 8/36 Loss: 4.7428\n",
      "Epoch 2 Step 9/36 Loss: 5.7145\n",
      "Epoch 2 Step 10/36 Loss: 5.3694\n",
      "Epoch 2 Step 11/36 Loss: 5.9601\n",
      "Epoch 2 Step 12/36 Loss: 5.0018\n",
      "Epoch 2 Step 13/36 Loss: 5.3705\n",
      "Epoch 2 Step 14/36 Loss: 4.6844\n",
      "Epoch 2 Step 15/36 Loss: 5.9674\n",
      "Epoch 2 Step 16/36 Loss: 4.4504\n",
      "Epoch 2 Step 17/36 Loss: 5.9105\n",
      "Epoch 2 Step 18/36 Loss: 5.6268\n",
      "Epoch 2 Step 19/36 Loss: 6.1057\n",
      "Epoch 2 Step 20/36 Loss: 5.7091\n",
      "Epoch 2 Step 21/36 Loss: 5.1710\n",
      "Epoch 2 Step 22/36 Loss: 5.7497\n",
      "Epoch 2 Step 23/36 Loss: 5.5419\n",
      "Epoch 2 Step 24/36 Loss: 5.4923\n",
      "Epoch 2 Step 25/36 Loss: 6.2297\n",
      "Epoch 2 Step 26/36 Loss: 5.6375\n",
      "Epoch 2 Step 27/36 Loss: 5.8822\n",
      "Epoch 2 Step 28/36 Loss: 6.1618\n",
      "Epoch 2 Step 29/36 Loss: 6.4327\n",
      "Epoch 2 Step 30/36 Loss: 4.5676\n",
      "Epoch 2 Step 31/36 Loss: 4.8647\n",
      "Epoch 2 Step 32/36 Loss: 4.0713\n",
      "Epoch 2 Step 33/36 Loss: 5.0455\n",
      "Epoch 2 Step 34/36 Loss: 4.4160\n",
      "Epoch 2 Step 35/36 Loss: 6.3940\n",
      "Epoch 2 Step 36/36 Loss: 5.4365\n",
      "Epoch 2 Step 1/36 Loss: 5.8731\n",
      "Epoch 2 Step 2/36 Loss: 5.5050\n",
      "Epoch 2 Step 3/36 Loss: 3.7874\n",
      "Epoch 2 Step 4/36 Loss: 5.2010\n",
      "Epoch 2 Step 5/36 Loss: 5.0774\n",
      "Epoch 2 Step 6/36 Loss: 5.0416\n",
      "Epoch 2 Step 7/36 Loss: 4.8482\n",
      "Epoch 2 Step 8/36 Loss: 5.6953\n",
      "Epoch 2 Step 9/36 Loss: 6.4905\n",
      "Epoch 2 Step 10/36 Loss: 6.8305\n",
      "Epoch 2 Step 11/36 Loss: 5.3178\n",
      "Epoch 2 Step 12/36 Loss: 4.4631\n",
      "Epoch 2 Step 13/36 Loss: 6.6060\n",
      "Epoch 2 Step 14/36 Loss: 5.5367\n",
      "Epoch 2 Step 15/36 Loss: 5.8406\n",
      "Epoch 2 Step 16/36 Loss: 5.6379\n",
      "Epoch 2 Step 17/36 Loss: 5.1199\n",
      "Epoch 2 Step 18/36 Loss: 5.0996\n",
      "Epoch 2 Step 19/36 Loss: 4.9077\n",
      "Epoch 2 Step 20/36 Loss: 4.9155\n",
      "Epoch 2 Step 21/36 Loss: 5.4397\n",
      "Epoch 2 Step 22/36 Loss: 5.5781\n",
      "Epoch 2 Step 23/36 Loss: 5.0593\n",
      "Epoch 2 Step 24/36 Loss: 4.2644\n",
      "Epoch 2 Step 25/36 Loss: 6.0478\n",
      "Epoch 2 Step 26/36 Loss: 4.6528\n",
      "Epoch 2 Step 27/36 Loss: 5.8133\n",
      "Epoch 2 Step 28/36 Loss: 5.5181\n",
      "Epoch 2 Step 29/36 Loss: 6.2745\n",
      "Epoch 2 Step 30/36 Loss: 5.7138\n",
      "Epoch 2 Step 31/36 Loss: 5.2636\n",
      "Epoch 2 Step 32/36 Loss: 5.1809\n",
      "Epoch 2 Step 33/36 Loss: 4.5174\n",
      "Epoch 2 Step 34/36 Loss: 6.2029\n",
      "Epoch 2 Step 35/36 Loss: 4.5766\n",
      "Epoch 2 Step 36/36 Loss: 4.6353\n",
      "Epoch 2 Step 1/36 Loss: 4.3262\n",
      "Epoch 2 Step 2/36 Loss: 4.6521\n",
      "Epoch 2 Step 3/36 Loss: 6.5310\n",
      "Epoch 2 Step 4/36 Loss: 6.0992\n",
      "Epoch 2 Step 5/36 Loss: 5.2004\n",
      "Epoch 2 Step 6/36 Loss: 5.1379\n",
      "Epoch 2 Step 7/36 Loss: 5.6440\n",
      "Epoch 2 Step 8/36 Loss: 5.4811\n",
      "Epoch 2 Step 9/36 Loss: 4.7053\n",
      "Epoch 2 Step 10/36 Loss: 4.3052\n",
      "Epoch 2 Step 11/36 Loss: 5.2499\n",
      "Epoch 2 Step 12/36 Loss: 5.6687\n",
      "Epoch 2 Step 13/36 Loss: 4.6833\n",
      "Epoch 2 Step 14/36 Loss: 4.3238\n",
      "Epoch 2 Step 15/36 Loss: 6.0763\n",
      "Epoch 2 Step 16/36 Loss: 6.0541\n",
      "Epoch 2 Step 17/36 Loss: 4.7460\n",
      "Epoch 2 Step 18/36 Loss: 5.0018\n",
      "Epoch 2 Step 19/36 Loss: 5.2616\n",
      "Epoch 2 Step 20/36 Loss: 5.1901\n",
      "Epoch 2 Step 21/36 Loss: 6.3604\n",
      "Epoch 2 Step 22/36 Loss: 5.0964\n",
      "Epoch 2 Step 23/36 Loss: 5.2531\n",
      "Epoch 2 Step 24/36 Loss: 5.1555\n",
      "Epoch 2 Step 25/36 Loss: 4.7747\n",
      "Epoch 2 Step 26/36 Loss: 6.9239\n",
      "Epoch 2 Step 27/36 Loss: 6.4112\n",
      "Epoch 2 Step 28/36 Loss: 5.2286\n",
      "Epoch 2 Step 29/36 Loss: 5.8597\n",
      "Epoch 2 Step 30/36 Loss: 4.3710\n",
      "Epoch 2 Step 31/36 Loss: 5.2817\n",
      "Epoch 2 Step 32/36 Loss: 6.3336\n",
      "Epoch 2 Step 33/36 Loss: 4.4905\n",
      "Epoch 2 Step 34/36 Loss: 5.1444\n",
      "Epoch 2 Step 35/36 Loss: 4.4453\n",
      "Epoch 2 Step 36/36 Loss: 6.1223\n",
      "Epoch 2 Step 1/36 Loss: 5.6847\n",
      "Epoch 2 Step 2/36 Loss: 7.2333\n",
      "Epoch 2 Step 3/36 Loss: 5.9830\n",
      "Epoch 2 Step 4/36 Loss: 4.7646\n",
      "Epoch 2 Step 5/36 Loss: 4.9302\n",
      "Epoch 2 Step 6/36 Loss: 5.9511\n",
      "Epoch 2 Step 7/36 Loss: 5.4282\n",
      "Epoch 2 Step 8/36 Loss: 5.7618\n",
      "Epoch 2 Step 9/36 Loss: 4.9819\n",
      "Epoch 2 Step 10/36 Loss: 4.3319\n",
      "Epoch 2 Step 11/36 Loss: 5.9180\n",
      "Epoch 2 Step 12/36 Loss: 4.9180\n",
      "Epoch 2 Step 13/36 Loss: 5.9654\n",
      "Epoch 2 Step 14/36 Loss: 4.7620\n",
      "Epoch 2 Step 15/36 Loss: 4.3446\n",
      "Epoch 2 Step 16/36 Loss: 6.6440\n",
      "Epoch 2 Step 17/36 Loss: 5.9368\n",
      "Epoch 2 Step 18/36 Loss: 5.6478\n",
      "Epoch 2 Step 19/36 Loss: 7.0221\n",
      "Epoch 2 Step 20/36 Loss: 5.5306\n",
      "Epoch 2 Step 21/36 Loss: 4.7879\n",
      "Epoch 2 Step 22/36 Loss: 6.0035\n",
      "Epoch 2 Step 23/36 Loss: 5.0298\n",
      "Epoch 2 Step 24/36 Loss: 5.5071\n",
      "Epoch 2 Step 25/36 Loss: 6.5938\n",
      "Epoch 2 Step 26/36 Loss: 4.6389\n",
      "Epoch 2 Step 27/36 Loss: 4.8854\n",
      "Epoch 2 Step 28/36 Loss: 5.3429\n",
      "Epoch 2 Step 29/36 Loss: 4.6806\n",
      "Epoch 2 Step 30/36 Loss: 5.3810\n",
      "Epoch 2 Step 31/36 Loss: 5.9581\n",
      "Epoch 2 Step 32/36 Loss: 5.3168\n",
      "Epoch 2 Step 33/36 Loss: 5.2357\n",
      "Epoch 2 Step 34/36 Loss: 5.3380\n",
      "Epoch 2 Step 35/36 Loss: 5.2780\n",
      "Epoch 2 Step 36/36 Loss: 6.7922\n",
      "Epoch 2 Step 1/36 Loss: 5.4242\n",
      "Epoch 2 Step 2/36 Loss: 4.7513\n",
      "Epoch 2 Step 3/36 Loss: 5.3085\n",
      "Epoch 2 Step 4/36 Loss: 4.9421\n",
      "Epoch 2 Step 5/36 Loss: 6.3844\n",
      "Epoch 2 Step 6/36 Loss: 7.4966\n",
      "Epoch 2 Step 7/36 Loss: 5.7357\n",
      "Epoch 2 Step 8/36 Loss: 5.0634\n",
      "Epoch 2 Step 9/36 Loss: 6.6139\n",
      "Epoch 2 Step 10/36 Loss: 6.2725\n",
      "Epoch 2 Step 11/36 Loss: 5.7111\n",
      "Epoch 2 Step 12/36 Loss: 6.4620\n",
      "Epoch 2 Step 13/36 Loss: 5.3143\n",
      "Epoch 2 Step 14/36 Loss: 5.3840\n",
      "Epoch 2 Step 15/36 Loss: 4.9515\n",
      "Epoch 2 Step 16/36 Loss: 6.0152\n",
      "Epoch 2 Step 17/36 Loss: 4.8455\n",
      "Epoch 2 Step 18/36 Loss: 4.5173\n",
      "Epoch 2 Step 19/36 Loss: 5.0503\n",
      "Epoch 2 Step 20/36 Loss: 5.2603\n",
      "Epoch 2 Step 21/36 Loss: 5.8193\n",
      "Epoch 2 Step 22/36 Loss: 4.8535\n",
      "Epoch 2 Step 23/36 Loss: 6.4716\n",
      "Epoch 2 Step 24/36 Loss: 4.6607\n",
      "Epoch 2 Step 25/36 Loss: 6.5074\n",
      "Epoch 2 Step 26/36 Loss: 4.6184\n",
      "Epoch 2 Step 27/36 Loss: 5.5590\n",
      "Epoch 2 Step 28/36 Loss: 5.4478\n",
      "Epoch 2 Step 29/36 Loss: 6.6347\n",
      "Epoch 2 Step 30/36 Loss: 4.8028\n",
      "Epoch 2 Step 31/36 Loss: 5.2471\n",
      "Epoch 2 Step 32/36 Loss: 4.9817\n",
      "Epoch 2 Step 33/36 Loss: 5.5469\n",
      "Epoch 2 Step 34/36 Loss: 4.6647\n",
      "Epoch 2 Step 35/36 Loss: 6.4591\n",
      "Epoch 2 Step 36/36 Loss: 4.5506\n",
      "Epoch 2 Step 1/36 Loss: 4.9921\n",
      "Epoch 2 Step 2/36 Loss: 5.4123\n",
      "Epoch 2 Step 3/36 Loss: 4.0276\n",
      "Epoch 2 Step 4/36 Loss: 5.8892\n",
      "Epoch 2 Step 5/36 Loss: 4.1264\n",
      "Epoch 2 Step 6/36 Loss: 5.6609\n",
      "Epoch 2 Step 7/36 Loss: 3.9437\n",
      "Epoch 2 Step 8/36 Loss: 6.1958\n",
      "Epoch 2 Step 9/36 Loss: 4.8601\n",
      "Epoch 2 Step 10/36 Loss: 6.1682\n",
      "Epoch 2 Step 11/36 Loss: 6.5542\n",
      "Epoch 2 Step 12/36 Loss: 5.2360\n",
      "Epoch 2 Step 13/36 Loss: 4.4794\n",
      "Epoch 2 Step 14/36 Loss: 5.8419\n",
      "Epoch 2 Step 15/36 Loss: 5.9657\n",
      "Epoch 2 Step 16/36 Loss: 5.8711\n",
      "Epoch 2 Step 17/36 Loss: 5.8178\n",
      "Epoch 2 Step 18/36 Loss: 5.4128\n",
      "Epoch 2 Step 19/36 Loss: 5.3685\n",
      "Epoch 2 Step 20/36 Loss: 5.2344\n",
      "Epoch 2 Step 21/36 Loss: 5.1800\n",
      "Epoch 2 Step 22/36 Loss: 6.7578\n",
      "Epoch 2 Step 23/36 Loss: 5.5045\n",
      "Epoch 2 Step 24/36 Loss: 4.6988\n",
      "Epoch 2 Step 25/36 Loss: 4.5020\n",
      "Epoch 2 Step 26/36 Loss: 5.0039\n",
      "Epoch 2 Step 27/36 Loss: 6.1773\n",
      "Epoch 2 Step 28/36 Loss: 5.5680\n",
      "Epoch 2 Step 29/36 Loss: 5.8066\n",
      "Epoch 2 Step 30/36 Loss: 6.0415\n",
      "Epoch 2 Step 31/36 Loss: 5.4479\n",
      "Epoch 2 Step 32/36 Loss: 5.0664\n",
      "Epoch 2 Step 33/36 Loss: 4.6437\n",
      "Epoch 2 Step 34/36 Loss: 4.2559\n",
      "Epoch 2 Step 35/36 Loss: 6.4659\n",
      "Epoch 2 Step 36/36 Loss: 4.8037\n",
      "Epoch 2 Step 1/36 Loss: 4.9075\n",
      "Epoch 2 Step 2/36 Loss: 6.4153\n",
      "Epoch 2 Step 3/36 Loss: 5.8030\n",
      "Epoch 2 Step 4/36 Loss: 6.2582\n",
      "Epoch 2 Step 5/36 Loss: 5.0670\n",
      "Epoch 2 Step 6/36 Loss: 5.1541\n",
      "Epoch 2 Step 7/36 Loss: 4.6257\n",
      "Epoch 2 Step 8/36 Loss: 6.7416\n",
      "Epoch 2 Step 9/36 Loss: 5.5606\n",
      "Epoch 2 Step 10/36 Loss: 5.7939\n",
      "Epoch 2 Step 11/36 Loss: 6.3923\n",
      "Epoch 2 Step 12/36 Loss: 4.9321\n",
      "Epoch 2 Step 13/36 Loss: 4.6391\n",
      "Epoch 2 Step 14/36 Loss: 4.4458\n",
      "Epoch 2 Step 15/36 Loss: 5.4947\n",
      "Epoch 2 Step 16/36 Loss: 5.9522\n",
      "Epoch 2 Step 17/36 Loss: 4.2950\n",
      "Epoch 2 Step 18/36 Loss: 4.9982\n",
      "Epoch 2 Step 19/36 Loss: 6.3722\n",
      "Epoch 2 Step 20/36 Loss: 5.2508\n",
      "Epoch 2 Step 21/36 Loss: 5.1299\n",
      "Epoch 2 Step 22/36 Loss: 4.3806\n",
      "Epoch 2 Step 23/36 Loss: 6.3166\n",
      "Epoch 2 Step 24/36 Loss: 4.7614\n",
      "Epoch 2 Step 25/36 Loss: 5.0051\n",
      "Epoch 2 Step 26/36 Loss: 5.9320\n",
      "Epoch 2 Step 27/36 Loss: 5.4215\n",
      "Epoch 2 Step 28/36 Loss: 5.8964\n",
      "Epoch 2 Step 29/36 Loss: 6.0417\n",
      "Epoch 2 Step 30/36 Loss: 7.6399\n",
      "Epoch 2 Step 31/36 Loss: 5.9543\n",
      "Epoch 2 Step 32/36 Loss: 4.8814\n",
      "Epoch 2 Step 33/36 Loss: 5.6121\n",
      "Epoch 2 Step 34/36 Loss: 5.1119\n",
      "Epoch 2 Step 35/36 Loss: 5.1450\n",
      "Epoch 2 Step 36/36 Loss: 5.3998\n",
      "Epoch 2 Step 1/36 Loss: 5.0613\n",
      "Epoch 2 Step 2/36 Loss: 5.6490\n",
      "Epoch 2 Step 3/36 Loss: 4.6134\n",
      "Epoch 2 Step 4/36 Loss: 6.2838\n",
      "Epoch 2 Step 5/36 Loss: 5.7087\n",
      "Epoch 2 Step 6/36 Loss: 5.1829\n",
      "Epoch 2 Step 7/36 Loss: 6.7360\n",
      "Epoch 2 Step 8/36 Loss: 4.7279\n",
      "Epoch 2 Step 9/36 Loss: 5.0011\n",
      "Epoch 2 Step 10/36 Loss: 4.6892\n",
      "Epoch 2 Step 11/36 Loss: 5.9651\n",
      "Epoch 2 Step 12/36 Loss: 5.8332\n",
      "Epoch 2 Step 13/36 Loss: 5.7424\n",
      "Epoch 2 Step 14/36 Loss: 5.5529\n",
      "Epoch 2 Step 15/36 Loss: 4.5238\n",
      "Epoch 2 Step 16/36 Loss: 5.8963\n",
      "Epoch 2 Step 17/36 Loss: 4.9487\n",
      "Epoch 2 Step 18/36 Loss: 5.7649\n",
      "Epoch 2 Step 19/36 Loss: 6.8248\n",
      "Epoch 2 Step 20/36 Loss: 5.8600\n",
      "Epoch 2 Step 21/36 Loss: 4.0471\n",
      "Epoch 2 Step 22/36 Loss: 5.2784\n",
      "Epoch 2 Step 23/36 Loss: 3.8154\n",
      "Epoch 2 Step 24/36 Loss: 4.0668\n",
      "Epoch 2 Step 25/36 Loss: 5.2427\n",
      "Epoch 2 Step 26/36 Loss: 5.9390\n",
      "Epoch 2 Step 27/36 Loss: 4.7911\n",
      "Epoch 2 Step 28/36 Loss: 5.0163\n",
      "Epoch 2 Step 29/36 Loss: 6.4618\n",
      "Epoch 2 Step 30/36 Loss: 5.1300\n",
      "Epoch 2 Step 31/36 Loss: 4.2147\n",
      "Epoch 2 Step 32/36 Loss: 5.7049\n",
      "Epoch 2 Step 33/36 Loss: 6.0758\n",
      "Epoch 2 Step 34/36 Loss: 6.8078\n",
      "Epoch 2 Step 35/36 Loss: 4.9141\n",
      "Epoch 2 Step 36/36 Loss: 5.7434\n",
      "Epoch 2 Step 1/36 Loss: 5.1973\n",
      "Epoch 2 Step 2/36 Loss: 6.7430\n",
      "Epoch 2 Step 3/36 Loss: 4.4507\n",
      "Epoch 2 Step 4/36 Loss: 5.9319\n",
      "Epoch 2 Step 5/36 Loss: 6.1348\n",
      "Epoch 2 Step 6/36 Loss: 5.3493\n",
      "Epoch 2 Step 7/36 Loss: 4.2354\n",
      "Epoch 2 Step 8/36 Loss: 4.7776\n",
      "Epoch 2 Step 9/36 Loss: 4.8899\n",
      "Epoch 2 Step 10/36 Loss: 6.4912\n",
      "Epoch 2 Step 11/36 Loss: 4.3874\n",
      "Epoch 2 Step 12/36 Loss: 7.8862\n",
      "Epoch 2 Step 13/36 Loss: 5.8314\n",
      "Epoch 2 Step 14/36 Loss: 4.5322\n",
      "Epoch 2 Step 15/36 Loss: 5.7943\n",
      "Epoch 2 Step 16/36 Loss: 5.6559\n",
      "Epoch 2 Step 17/36 Loss: 6.2247\n",
      "Epoch 2 Step 18/36 Loss: 5.3379\n",
      "Epoch 2 Step 19/36 Loss: 5.8009\n",
      "Epoch 2 Step 20/36 Loss: 6.3661\n",
      "Epoch 2 Step 21/36 Loss: 5.6332\n",
      "Epoch 2 Step 22/36 Loss: 4.1509\n",
      "Epoch 2 Step 23/36 Loss: 5.8065\n",
      "Epoch 2 Step 24/36 Loss: 4.5955\n",
      "Epoch 2 Step 25/36 Loss: 5.3459\n",
      "Epoch 2 Step 26/36 Loss: 4.6353\n",
      "Epoch 2 Step 27/36 Loss: 6.0790\n",
      "Epoch 2 Step 28/36 Loss: 5.3438\n",
      "Epoch 2 Step 29/36 Loss: 5.7787\n",
      "Epoch 2 Step 30/36 Loss: 4.5828\n",
      "Epoch 2 Step 31/36 Loss: 4.7818\n",
      "Epoch 2 Step 32/36 Loss: 4.6951\n",
      "Epoch 2 Step 33/36 Loss: 6.2706\n",
      "Epoch 2 Step 34/36 Loss: 5.1070\n",
      "Epoch 2 Step 35/36 Loss: 4.4363\n",
      "Epoch 2 Step 36/36 Loss: 5.9291\n",
      "Epoch 2 Step 1/36 Loss: 4.7669\n",
      "Epoch 2 Step 2/36 Loss: 6.0364\n",
      "Epoch 2 Step 3/36 Loss: 5.6832\n",
      "Epoch 2 Step 4/36 Loss: 6.3394\n",
      "Epoch 2 Step 5/36 Loss: 3.6225\n",
      "Epoch 2 Step 6/36 Loss: 5.8200\n",
      "Epoch 2 Step 7/36 Loss: 4.8113\n",
      "Epoch 2 Step 8/36 Loss: 6.0302\n",
      "Epoch 2 Step 9/36 Loss: 5.8735\n",
      "Epoch 2 Step 10/36 Loss: 7.5168\n",
      "Epoch 2 Step 11/36 Loss: 6.2446\n",
      "Epoch 2 Step 12/36 Loss: 5.0451\n",
      "Epoch 2 Step 13/36 Loss: 5.0322\n",
      "Epoch 2 Step 14/36 Loss: 5.6539\n",
      "Epoch 2 Step 15/36 Loss: 5.5001\n",
      "Epoch 2 Step 16/36 Loss: 5.8560\n",
      "Epoch 2 Step 17/36 Loss: 6.4001\n",
      "Epoch 2 Step 18/36 Loss: 6.0816\n",
      "Epoch 2 Step 19/36 Loss: 6.0365\n",
      "Epoch 2 Step 20/36 Loss: 5.6952\n",
      "Epoch 2 Step 21/36 Loss: 6.8761\n",
      "Epoch 2 Step 22/36 Loss: 5.9935\n",
      "Epoch 2 Step 23/36 Loss: 4.8498\n",
      "Epoch 2 Step 24/36 Loss: 4.9487\n",
      "Epoch 2 Step 25/36 Loss: 4.8560\n",
      "Epoch 2 Step 26/36 Loss: 5.3476\n",
      "Epoch 2 Step 27/36 Loss: 6.1145\n",
      "Epoch 2 Step 28/36 Loss: 6.3618\n",
      "Epoch 2 Step 29/36 Loss: 5.2525\n",
      "Epoch 2 Step 30/36 Loss: 5.7374\n",
      "Epoch 2 Step 31/36 Loss: 4.9521\n",
      "Epoch 2 Step 32/36 Loss: 4.8471\n",
      "Epoch 2 Step 33/36 Loss: 5.4769\n",
      "Epoch 2 Step 34/36 Loss: 4.7943\n",
      "Epoch 2 Step 35/36 Loss: 4.3255\n",
      "Epoch 2 Step 36/36 Loss: 4.8884\n",
      "Epoch 2 Step 1/36 Loss: 5.4947\n",
      "Epoch 2 Step 2/36 Loss: 4.6367\n",
      "Epoch 2 Step 3/36 Loss: 4.9663\n",
      "Epoch 2 Step 4/36 Loss: 5.4150\n",
      "Epoch 2 Step 5/36 Loss: 6.2648\n",
      "Epoch 2 Step 6/36 Loss: 6.5162\n",
      "Epoch 2 Step 7/36 Loss: 5.4348\n",
      "Epoch 2 Step 8/36 Loss: 5.2790\n",
      "Epoch 2 Step 9/36 Loss: 4.7875\n",
      "Epoch 2 Step 10/36 Loss: 5.3784\n",
      "Epoch 2 Step 11/36 Loss: 4.7211\n",
      "Epoch 2 Step 12/36 Loss: 4.2832\n",
      "Epoch 2 Step 13/36 Loss: 5.0175\n",
      "Epoch 2 Step 14/36 Loss: 7.7559\n",
      "Epoch 2 Step 15/36 Loss: 6.3013\n",
      "Epoch 2 Step 16/36 Loss: 5.2004\n",
      "Epoch 2 Step 17/36 Loss: 4.4026\n",
      "Epoch 2 Step 18/36 Loss: 6.0621\n",
      "Epoch 2 Step 19/36 Loss: 5.3216\n",
      "Epoch 2 Step 20/36 Loss: 5.0526\n",
      "Epoch 2 Step 21/36 Loss: 7.9483\n",
      "Epoch 2 Step 22/36 Loss: 6.4618\n",
      "Epoch 2 Step 23/36 Loss: 4.3353\n",
      "Epoch 2 Step 24/36 Loss: 5.8989\n",
      "Epoch 2 Step 25/36 Loss: 4.5994\n",
      "Epoch 2 Step 26/36 Loss: 6.6575\n",
      "Epoch 2 Step 27/36 Loss: 5.9022\n",
      "Epoch 2 Step 28/36 Loss: 5.3036\n",
      "Epoch 2 Step 29/36 Loss: 4.2917\n",
      "Epoch 2 Step 30/36 Loss: 7.8851\n",
      "Epoch 2 Step 31/36 Loss: 6.0206\n",
      "Epoch 2 Step 32/36 Loss: 6.4297\n",
      "Epoch 2 Step 33/36 Loss: 6.1327\n",
      "Epoch 2 Step 34/36 Loss: 7.4471\n",
      "Epoch 2 Step 35/36 Loss: 5.5540\n",
      "Epoch 2 Step 36/36 Loss: 4.2224\n",
      "Epoch 2 Step 1/36 Loss: 6.5678\n",
      "Epoch 2 Step 2/36 Loss: 6.1141\n",
      "Epoch 2 Step 3/36 Loss: 5.7150\n",
      "Epoch 2 Step 4/36 Loss: 6.1556\n",
      "Epoch 2 Step 5/36 Loss: 4.8827\n",
      "Epoch 2 Step 6/36 Loss: 4.4964\n",
      "Epoch 2 Step 7/36 Loss: 4.6400\n",
      "Epoch 2 Step 8/36 Loss: 5.2713\n",
      "Epoch 2 Step 9/36 Loss: 4.9542\n",
      "Epoch 2 Step 10/36 Loss: 4.7164\n",
      "Epoch 2 Step 11/36 Loss: 5.8866\n",
      "Epoch 2 Step 12/36 Loss: 4.9409\n",
      "Epoch 2 Step 13/36 Loss: 6.1509\n",
      "Epoch 2 Step 14/36 Loss: 6.0750\n",
      "Epoch 2 Step 15/36 Loss: 5.3234\n",
      "Epoch 2 Step 16/36 Loss: 5.6018\n",
      "Epoch 2 Step 17/36 Loss: 3.7924\n",
      "Epoch 2 Step 18/36 Loss: 5.3516\n",
      "Epoch 2 Step 19/36 Loss: 4.5959\n",
      "Epoch 2 Step 20/36 Loss: 5.1683\n",
      "Epoch 2 Step 21/36 Loss: 4.6400\n",
      "Epoch 2 Step 22/36 Loss: 5.6679\n",
      "Epoch 2 Step 23/36 Loss: 4.9164\n",
      "Epoch 2 Step 24/36 Loss: 5.7928\n",
      "Epoch 2 Step 25/36 Loss: 5.3100\n",
      "Epoch 2 Step 26/36 Loss: 5.4862\n",
      "Epoch 2 Step 27/36 Loss: 5.2816\n",
      "Epoch 2 Step 28/36 Loss: 5.0117\n",
      "Epoch 2 Step 29/36 Loss: 4.9450\n",
      "Epoch 2 Step 30/36 Loss: 4.6916\n",
      "Epoch 2 Step 31/36 Loss: 5.3401\n",
      "Epoch 2 Step 32/36 Loss: 4.1804\n",
      "Epoch 2 Step 33/36 Loss: 5.1783\n",
      "Epoch 2 Step 34/36 Loss: 5.2993\n",
      "Epoch 2 Step 35/36 Loss: 5.5883\n",
      "Epoch 2 Step 36/36 Loss: 3.6079\n",
      "Epoch 2 Step 1/36 Loss: 6.2590\n",
      "Epoch 2 Step 2/36 Loss: 4.1779\n",
      "Epoch 2 Step 3/36 Loss: 5.5205\n",
      "Epoch 2 Step 4/36 Loss: 7.2581\n",
      "Epoch 2 Step 5/36 Loss: 5.4874\n",
      "Epoch 2 Step 6/36 Loss: 6.6103\n",
      "Epoch 2 Step 7/36 Loss: 5.2677\n",
      "Epoch 2 Step 8/36 Loss: 4.7918\n",
      "Epoch 2 Step 9/36 Loss: 4.6268\n",
      "Epoch 2 Step 10/36 Loss: 5.3221\n",
      "Epoch 2 Step 11/36 Loss: 5.6984\n",
      "Epoch 2 Step 12/36 Loss: 4.7770\n",
      "Epoch 2 Step 13/36 Loss: 4.2460\n",
      "Epoch 2 Step 14/36 Loss: 5.4377\n",
      "Epoch 2 Step 15/36 Loss: 4.2218\n",
      "Epoch 2 Step 16/36 Loss: 5.3105\n",
      "Epoch 2 Step 17/36 Loss: 4.9224\n",
      "Epoch 2 Step 18/36 Loss: 5.8229\n",
      "Epoch 2 Step 19/36 Loss: 4.8497\n",
      "Epoch 2 Step 20/36 Loss: 5.9514\n",
      "Epoch 2 Step 21/36 Loss: 6.0905\n",
      "Epoch 2 Step 22/36 Loss: 5.8841\n",
      "Epoch 2 Step 23/36 Loss: 4.8587\n",
      "Epoch 2 Step 24/36 Loss: 6.0284\n",
      "Epoch 2 Step 25/36 Loss: 4.7181\n",
      "Epoch 2 Step 26/36 Loss: 5.2716\n",
      "Epoch 2 Step 27/36 Loss: 5.7041\n",
      "Epoch 2 Step 28/36 Loss: 4.6111\n",
      "Epoch 2 Step 29/36 Loss: 5.5978\n",
      "Epoch 2 Step 30/36 Loss: 5.8690\n",
      "Epoch 2 Step 31/36 Loss: 7.4959\n",
      "Epoch 2 Step 32/36 Loss: 5.9038\n",
      "Epoch 2 Step 33/36 Loss: 4.9411\n",
      "Epoch 2 Step 34/36 Loss: 5.1848\n",
      "Epoch 2 Step 35/36 Loss: 5.0025\n",
      "Epoch 2 Step 36/36 Loss: 6.7105\n",
      "Epoch 2 Step 1/36 Loss: 6.6514\n",
      "Epoch 2 Step 2/36 Loss: 4.7917\n",
      "Epoch 2 Step 3/36 Loss: 4.6738\n",
      "Epoch 2 Step 4/36 Loss: 5.4010\n",
      "Epoch 2 Step 5/36 Loss: 5.3688\n",
      "Epoch 2 Step 6/36 Loss: 7.2113\n",
      "Epoch 2 Step 7/36 Loss: 5.7759\n",
      "Epoch 2 Step 8/36 Loss: 4.7284\n",
      "Epoch 2 Step 9/36 Loss: 5.2075\n",
      "Epoch 2 Step 10/36 Loss: 3.8634\n",
      "Epoch 2 Step 11/36 Loss: 5.5434\n",
      "Epoch 2 Step 12/36 Loss: 6.4693\n",
      "Epoch 2 Step 13/36 Loss: 6.4883\n",
      "Epoch 2 Step 14/36 Loss: 6.6929\n",
      "Epoch 2 Step 15/36 Loss: 5.2674\n",
      "Epoch 2 Step 16/36 Loss: 4.9264\n",
      "Epoch 2 Step 17/36 Loss: 4.4462\n",
      "Epoch 2 Step 18/36 Loss: 6.0526\n",
      "Epoch 2 Step 19/36 Loss: 5.3416\n",
      "Epoch 2 Step 20/36 Loss: 5.0655\n",
      "Epoch 2 Step 21/36 Loss: 5.0199\n",
      "Epoch 2 Step 22/36 Loss: 5.5772\n",
      "Epoch 2 Step 23/36 Loss: 4.8431\n",
      "Epoch 2 Step 24/36 Loss: 5.3935\n",
      "Epoch 2 Step 25/36 Loss: 5.8918\n",
      "Epoch 2 Step 26/36 Loss: 5.8451\n",
      "Epoch 2 Step 27/36 Loss: 5.7062\n",
      "Epoch 2 Step 28/36 Loss: 5.0441\n",
      "Epoch 2 Step 29/36 Loss: 5.3917\n",
      "Epoch 2 Step 30/36 Loss: 5.2049\n",
      "Epoch 2 Step 31/36 Loss: 5.9971\n",
      "Epoch 2 Step 32/36 Loss: 4.7318\n",
      "Epoch 2 Step 33/36 Loss: 4.1684\n",
      "Epoch 2 Step 34/36 Loss: 4.9887\n",
      "Epoch 2 Step 35/36 Loss: 4.9460\n",
      "Epoch 2 Step 36/36 Loss: 4.7592\n",
      "Epoch 2 Step 1/36 Loss: 5.6321\n",
      "Epoch 2 Step 2/36 Loss: 4.5335\n",
      "Epoch 2 Step 3/36 Loss: 7.0674\n",
      "Epoch 2 Step 4/36 Loss: 6.3461\n",
      "Epoch 2 Step 5/36 Loss: 6.9367\n",
      "Epoch 2 Step 6/36 Loss: 5.5539\n",
      "Epoch 2 Step 7/36 Loss: 5.0771\n",
      "Epoch 2 Step 8/36 Loss: 4.8029\n",
      "Epoch 2 Step 9/36 Loss: 6.8341\n",
      "Epoch 2 Step 10/36 Loss: 6.0805\n",
      "Epoch 2 Step 11/36 Loss: 4.6858\n",
      "Epoch 2 Step 12/36 Loss: 4.9648\n",
      "Epoch 2 Step 13/36 Loss: 4.5861\n",
      "Epoch 2 Step 14/36 Loss: 5.4833\n",
      "Epoch 2 Step 15/36 Loss: 5.9466\n",
      "Epoch 2 Step 16/36 Loss: 4.9933\n",
      "Epoch 2 Step 17/36 Loss: 4.7865\n",
      "Epoch 2 Step 18/36 Loss: 4.6837\n",
      "Epoch 2 Step 19/36 Loss: 5.6467\n",
      "Epoch 2 Step 20/36 Loss: 5.1152\n",
      "Epoch 2 Step 21/36 Loss: 5.8885\n",
      "Epoch 2 Step 22/36 Loss: 4.8933\n",
      "Epoch 2 Step 23/36 Loss: 6.2192\n",
      "Epoch 2 Step 24/36 Loss: 4.8754\n",
      "Epoch 2 Step 25/36 Loss: 5.7673\n",
      "Epoch 2 Step 26/36 Loss: 6.4964\n",
      "Epoch 2 Step 27/36 Loss: 4.7322\n",
      "Epoch 2 Step 28/36 Loss: 5.4362\n",
      "Epoch 2 Step 29/36 Loss: 4.3107\n",
      "Epoch 2 Step 30/36 Loss: 4.8614\n",
      "Epoch 2 Step 31/36 Loss: 6.4000\n",
      "Epoch 2 Step 32/36 Loss: 4.8112\n",
      "Epoch 2 Step 33/36 Loss: 4.7953\n",
      "Epoch 2 Step 34/36 Loss: 5.2165\n",
      "Epoch 2 Step 35/36 Loss: 4.4305\n",
      "Epoch 2 Step 36/36 Loss: 5.4634\n",
      "Epoch 2 Step 1/36 Loss: 5.0042\n",
      "Epoch 2 Step 2/36 Loss: 5.4415\n",
      "Epoch 2 Step 3/36 Loss: 4.8133\n",
      "Epoch 2 Step 4/36 Loss: 5.3776\n",
      "Epoch 2 Step 5/36 Loss: 4.6307\n",
      "Epoch 2 Step 6/36 Loss: 6.4690\n",
      "Epoch 2 Step 7/36 Loss: 5.4084\n",
      "Epoch 2 Step 8/36 Loss: 6.6479\n",
      "Epoch 2 Step 9/36 Loss: 5.0737\n",
      "Epoch 2 Step 10/36 Loss: 6.5257\n",
      "Epoch 2 Step 11/36 Loss: 5.8207\n",
      "Epoch 2 Step 12/36 Loss: 5.9760\n",
      "Epoch 2 Step 13/36 Loss: 4.6717\n",
      "Epoch 2 Step 14/36 Loss: 5.3164\n",
      "Epoch 2 Step 15/36 Loss: 4.8885\n",
      "Epoch 2 Step 16/36 Loss: 5.9079\n",
      "Epoch 2 Step 17/36 Loss: 5.6984\n",
      "Epoch 2 Step 18/36 Loss: 5.9698\n",
      "Epoch 2 Step 19/36 Loss: 5.3450\n",
      "Epoch 2 Step 20/36 Loss: 4.1701\n",
      "Epoch 2 Step 21/36 Loss: 5.8380\n",
      "Epoch 2 Step 22/36 Loss: 6.0415\n",
      "Epoch 2 Step 23/36 Loss: 5.2456\n",
      "Epoch 2 Step 24/36 Loss: 6.2133\n",
      "Epoch 2 Step 25/36 Loss: 5.1117\n",
      "Epoch 2 Step 26/36 Loss: 4.6013\n",
      "Epoch 2 Step 27/36 Loss: 5.1256\n",
      "Epoch 2 Step 28/36 Loss: 5.3423\n",
      "Epoch 2 Step 29/36 Loss: 5.9505\n",
      "Epoch 2 Step 30/36 Loss: 5.1441\n",
      "Epoch 2 Step 31/36 Loss: 6.0176\n",
      "Epoch 2 Step 32/36 Loss: 5.0561\n",
      "Epoch 2 Step 33/36 Loss: 6.9877\n",
      "Epoch 2 Step 34/36 Loss: 4.4940\n",
      "Epoch 2 Step 35/36 Loss: 5.2277\n",
      "Epoch 2 Step 36/36 Loss: 5.7581\n",
      "Epoch 2 Step 1/36 Loss: 5.8144\n",
      "Epoch 2 Step 2/36 Loss: 5.3215\n",
      "Epoch 2 Step 3/36 Loss: 5.4396\n",
      "Epoch 2 Step 4/36 Loss: 4.8226\n",
      "Epoch 2 Step 5/36 Loss: 4.7703\n",
      "Epoch 2 Step 6/36 Loss: 4.6044\n",
      "Epoch 2 Step 7/36 Loss: 6.0546\n",
      "Epoch 2 Step 8/36 Loss: 4.6673\n",
      "Epoch 2 Step 9/36 Loss: 5.7418\n",
      "Epoch 2 Step 10/36 Loss: 6.0904\n",
      "Epoch 2 Step 11/36 Loss: 4.9677\n",
      "Epoch 2 Step 12/36 Loss: 3.9361\n",
      "Epoch 2 Step 13/36 Loss: 6.1774\n",
      "Epoch 2 Step 14/36 Loss: 4.4981\n",
      "Epoch 2 Step 15/36 Loss: 5.8783\n",
      "Epoch 2 Step 16/36 Loss: 5.5244\n",
      "Epoch 2 Step 17/36 Loss: 5.7939\n",
      "Epoch 2 Step 18/36 Loss: 4.7464\n",
      "Epoch 2 Step 19/36 Loss: 5.3571\n",
      "Epoch 2 Step 20/36 Loss: 5.4410\n",
      "Epoch 2 Step 21/36 Loss: 5.3145\n",
      "Epoch 2 Step 22/36 Loss: 5.7337\n",
      "Epoch 2 Step 23/36 Loss: 6.2423\n",
      "Epoch 2 Step 24/36 Loss: 5.9335\n",
      "Epoch 2 Step 25/36 Loss: 5.3846\n",
      "Epoch 2 Step 26/36 Loss: 5.5655\n",
      "Epoch 2 Step 27/36 Loss: 5.8987\n",
      "Epoch 2 Step 28/36 Loss: 5.6233\n",
      "Epoch 2 Step 29/36 Loss: 5.9129\n",
      "Epoch 2 Step 30/36 Loss: 4.8336\n",
      "Epoch 2 Step 31/36 Loss: 4.4513\n",
      "Epoch 2 Step 32/36 Loss: 4.8320\n",
      "Epoch 2 Step 33/36 Loss: 5.3410\n",
      "Epoch 2 Step 34/36 Loss: 4.2985\n",
      "Epoch 2 Step 35/36 Loss: 6.4055\n",
      "Epoch 2 Step 36/36 Loss: 4.4778\n",
      "Epoch 2 Step 1/36 Loss: 5.6796\n",
      "Epoch 2 Step 2/36 Loss: 7.1567\n",
      "Epoch 2 Step 3/36 Loss: 5.3381\n",
      "Epoch 2 Step 4/36 Loss: 5.3819\n",
      "Epoch 2 Step 5/36 Loss: 5.4405\n",
      "Epoch 2 Step 6/36 Loss: 5.7587\n",
      "Epoch 2 Step 7/36 Loss: 4.5469\n",
      "Epoch 2 Step 8/36 Loss: 6.4746\n",
      "Epoch 2 Step 9/36 Loss: 6.2482\n",
      "Epoch 2 Step 10/36 Loss: 5.9650\n",
      "Epoch 2 Step 11/36 Loss: 6.7523\n",
      "Epoch 2 Step 12/36 Loss: 6.1258\n",
      "Epoch 2 Step 13/36 Loss: 4.1807\n",
      "Epoch 2 Step 14/36 Loss: 5.6393\n",
      "Epoch 2 Step 15/36 Loss: 4.4554\n",
      "Epoch 2 Step 16/36 Loss: 5.9504\n",
      "Epoch 2 Step 17/36 Loss: 4.8015\n",
      "Epoch 2 Step 18/36 Loss: 5.6749\n",
      "Epoch 2 Step 19/36 Loss: 4.7496\n",
      "Epoch 2 Step 20/36 Loss: 5.7909\n",
      "Epoch 2 Step 21/36 Loss: 4.9459\n",
      "Epoch 2 Step 22/36 Loss: 4.8700\n",
      "Epoch 2 Step 23/36 Loss: 6.2858\n",
      "Epoch 2 Step 24/36 Loss: 6.3118\n",
      "Epoch 2 Step 25/36 Loss: 6.8106\n",
      "Epoch 2 Step 26/36 Loss: 4.2480\n",
      "Epoch 2 Step 27/36 Loss: 4.5538\n",
      "Epoch 2 Step 28/36 Loss: 4.8881\n",
      "Epoch 2 Step 29/36 Loss: 5.5218\n",
      "Epoch 2 Step 30/36 Loss: 5.0606\n",
      "Epoch 2 Step 31/36 Loss: 5.2723\n",
      "Epoch 2 Step 32/36 Loss: 4.6227\n",
      "Epoch 2 Step 33/36 Loss: 4.8780\n",
      "Epoch 2 Step 34/36 Loss: 4.3365\n",
      "Epoch 2 Step 35/36 Loss: 5.9683\n",
      "Epoch 2 Step 36/36 Loss: 4.6511\n",
      "Epoch 2 Step 1/36 Loss: 5.3032\n",
      "Epoch 2 Step 2/36 Loss: 5.0567\n",
      "Epoch 2 Step 3/36 Loss: 6.6019\n",
      "Epoch 2 Step 4/36 Loss: 6.4238\n",
      "Epoch 2 Step 5/36 Loss: 5.9915\n",
      "Epoch 2 Step 6/36 Loss: 5.2443\n",
      "Epoch 2 Step 7/36 Loss: 5.1121\n",
      "Epoch 2 Step 8/36 Loss: 5.2261\n",
      "Epoch 2 Step 9/36 Loss: 5.5267\n",
      "Epoch 2 Step 10/36 Loss: 4.2840\n",
      "Epoch 2 Step 11/36 Loss: 5.8508\n",
      "Epoch 2 Step 12/36 Loss: 5.4629\n",
      "Epoch 2 Step 13/36 Loss: 5.7665\n",
      "Epoch 2 Step 14/36 Loss: 5.6898\n",
      "Epoch 2 Step 15/36 Loss: 6.1634\n",
      "Epoch 2 Step 16/36 Loss: 4.7944\n",
      "Epoch 2 Step 17/36 Loss: 4.8446\n",
      "Epoch 2 Step 18/36 Loss: 5.9602\n",
      "Epoch 2 Step 19/36 Loss: 4.3140\n",
      "Epoch 2 Step 20/36 Loss: 5.1345\n",
      "Epoch 2 Step 21/36 Loss: 6.4217\n",
      "Epoch 2 Step 22/36 Loss: 3.9843\n",
      "Epoch 2 Step 23/36 Loss: 6.9341\n",
      "Epoch 2 Step 24/36 Loss: 5.3200\n",
      "Epoch 2 Step 25/36 Loss: 5.0249\n",
      "Epoch 2 Step 26/36 Loss: 7.1698\n",
      "Epoch 2 Step 27/36 Loss: 6.4658\n",
      "Epoch 2 Step 28/36 Loss: 4.8282\n",
      "Epoch 2 Step 29/36 Loss: 4.6467\n",
      "Epoch 2 Step 30/36 Loss: 4.5078\n",
      "Epoch 2 Step 31/36 Loss: 6.0539\n",
      "Epoch 2 Step 32/36 Loss: 4.9870\n",
      "Epoch 2 Step 33/36 Loss: 5.3291\n",
      "Epoch 2 Step 34/36 Loss: 5.0807\n",
      "Epoch 2 Step 35/36 Loss: 4.6357\n",
      "Epoch 2 Step 36/36 Loss: 6.4808\n",
      "Epoch 2 Step 1/36 Loss: 4.7713\n",
      "Epoch 2 Step 2/36 Loss: 5.1348\n",
      "Epoch 2 Step 3/36 Loss: 4.7076\n",
      "Epoch 2 Step 4/36 Loss: 5.4102\n",
      "Epoch 2 Step 5/36 Loss: 5.0450\n",
      "Epoch 2 Step 6/36 Loss: 5.3550\n",
      "Epoch 2 Step 7/36 Loss: 5.1113\n",
      "Epoch 2 Step 8/36 Loss: 4.3668\n",
      "Epoch 2 Step 9/36 Loss: 5.7771\n",
      "Epoch 2 Step 10/36 Loss: 4.0711\n",
      "Epoch 2 Step 11/36 Loss: 4.8460\n",
      "Epoch 2 Step 12/36 Loss: 4.8868\n",
      "Epoch 2 Step 13/36 Loss: 5.6905\n",
      "Epoch 2 Step 14/36 Loss: 5.9456\n",
      "Epoch 2 Step 15/36 Loss: 5.8797\n",
      "Epoch 2 Step 16/36 Loss: 5.9255\n",
      "Epoch 2 Step 17/36 Loss: 5.7993\n",
      "Epoch 2 Step 18/36 Loss: 5.2550\n",
      "Epoch 2 Step 19/36 Loss: 5.1074\n",
      "Epoch 2 Step 20/36 Loss: 6.3596\n",
      "Epoch 2 Step 21/36 Loss: 6.4432\n",
      "Epoch 2 Step 22/36 Loss: 5.8305\n",
      "Epoch 2 Step 23/36 Loss: 5.4765\n",
      "Epoch 2 Step 24/36 Loss: 5.2301\n",
      "Epoch 2 Step 25/36 Loss: 5.1369\n",
      "Epoch 2 Step 26/36 Loss: 5.6583\n",
      "Epoch 2 Step 27/36 Loss: 6.0416\n",
      "Epoch 2 Step 28/36 Loss: 5.6891\n",
      "Epoch 2 Step 29/36 Loss: 6.1608\n",
      "Epoch 2 Step 30/36 Loss: 5.3361\n",
      "Epoch 2 Step 31/36 Loss: 5.2197\n",
      "Epoch 2 Step 32/36 Loss: 7.0253\n",
      "Epoch 2 Step 33/36 Loss: 5.8218\n",
      "Epoch 2 Step 34/36 Loss: 4.5153\n",
      "Epoch 2 Step 35/36 Loss: 5.2290\n",
      "Epoch 2 Step 36/36 Loss: 5.7507\n",
      "Epoch 2 Step 1/36 Loss: 5.9517\n",
      "Epoch 2 Step 2/36 Loss: 5.2648\n",
      "Epoch 2 Step 3/36 Loss: 4.2384\n",
      "Epoch 2 Step 4/36 Loss: 6.5480\n",
      "Epoch 2 Step 5/36 Loss: 4.7396\n",
      "Epoch 2 Step 6/36 Loss: 4.9729\n",
      "Epoch 2 Step 7/36 Loss: 6.0975\n",
      "Epoch 2 Step 8/36 Loss: 4.7182\n",
      "Epoch 2 Step 9/36 Loss: 5.3371\n",
      "Epoch 2 Step 10/36 Loss: 4.9577\n",
      "Epoch 2 Step 11/36 Loss: 5.4943\n",
      "Epoch 2 Step 12/36 Loss: 5.3146\n",
      "Epoch 2 Step 13/36 Loss: 6.1191\n",
      "Epoch 2 Step 14/36 Loss: 4.8458\n",
      "Epoch 2 Step 15/36 Loss: 4.8464\n",
      "Epoch 2 Step 16/36 Loss: 5.3165\n",
      "Epoch 2 Step 17/36 Loss: 5.5548\n",
      "Epoch 2 Step 18/36 Loss: 5.2888\n",
      "Epoch 2 Step 19/36 Loss: 5.1309\n",
      "Epoch 2 Step 20/36 Loss: 5.9393\n",
      "Epoch 2 Step 21/36 Loss: 6.0897\n",
      "Epoch 2 Step 22/36 Loss: 5.7165\n",
      "Epoch 2 Step 23/36 Loss: 6.2677\n",
      "Epoch 2 Step 24/36 Loss: 5.2284\n",
      "Epoch 2 Step 25/36 Loss: 4.6482\n",
      "Epoch 2 Step 26/36 Loss: 5.3161\n",
      "Epoch 2 Step 27/36 Loss: 5.6751\n",
      "Epoch 2 Step 28/36 Loss: 6.4201\n",
      "Epoch 2 Step 29/36 Loss: 5.7742\n",
      "Epoch 2 Step 30/36 Loss: 5.3546\n",
      "Epoch 2 Step 31/36 Loss: 4.6932\n",
      "Epoch 2 Step 32/36 Loss: 5.7222\n",
      "Epoch 2 Step 33/36 Loss: 5.1180\n",
      "Epoch 2 Step 34/36 Loss: 6.3095\n",
      "Epoch 2 Step 35/36 Loss: 6.1260\n",
      "Epoch 2 Step 36/36 Loss: 5.3057\n",
      "Epoch 2 Step 1/36 Loss: 4.1299\n",
      "Epoch 2 Step 2/36 Loss: 6.1308\n",
      "Epoch 2 Step 3/36 Loss: 4.7204\n",
      "Epoch 2 Step 4/36 Loss: 5.1659\n",
      "Epoch 2 Step 5/36 Loss: 4.8232\n",
      "Epoch 2 Step 6/36 Loss: 4.8092\n",
      "Epoch 2 Step 7/36 Loss: 5.1122\n",
      "Epoch 2 Step 8/36 Loss: 6.0795\n",
      "Epoch 2 Step 9/36 Loss: 5.5597\n",
      "Epoch 2 Step 10/36 Loss: 6.5017\n",
      "Epoch 2 Step 11/36 Loss: 6.2687\n",
      "Epoch 2 Step 12/36 Loss: 5.0476\n",
      "Epoch 2 Step 13/36 Loss: 8.0651\n",
      "Epoch 2 Step 14/36 Loss: 6.4352\n",
      "Epoch 2 Step 15/36 Loss: 4.9516\n",
      "Epoch 2 Step 16/36 Loss: 5.0111\n",
      "Epoch 2 Step 17/36 Loss: 5.3830\n",
      "Epoch 2 Step 18/36 Loss: 5.9314\n",
      "Epoch 2 Step 19/36 Loss: 5.8188\n",
      "Epoch 2 Step 20/36 Loss: 6.4194\n",
      "Epoch 2 Step 21/36 Loss: 5.1198\n",
      "Epoch 2 Step 22/36 Loss: 4.7052\n",
      "Epoch 2 Step 23/36 Loss: 6.0821\n",
      "Epoch 2 Step 24/36 Loss: 5.7616\n",
      "Epoch 2 Step 25/36 Loss: 5.4969\n",
      "Epoch 2 Step 26/36 Loss: 5.7211\n",
      "Epoch 2 Step 27/36 Loss: 4.6168\n",
      "Epoch 2 Step 28/36 Loss: 5.1743\n",
      "Epoch 2 Step 29/36 Loss: 4.8885\n",
      "Epoch 2 Step 30/36 Loss: 5.3353\n",
      "Epoch 2 Step 31/36 Loss: 4.9460\n",
      "Epoch 2 Step 32/36 Loss: 4.9441\n",
      "Epoch 2 Step 33/36 Loss: 5.9854\n",
      "Epoch 2 Step 34/36 Loss: 4.9485\n",
      "Epoch 2 Step 35/36 Loss: 5.2762\n",
      "Epoch 2 Step 36/36 Loss: 5.2232\n",
      "Epoch 2 Step 1/36 Loss: 5.1623\n",
      "Epoch 2 Step 2/36 Loss: 5.3056\n",
      "Epoch 2 Step 3/36 Loss: 5.7015\n",
      "Epoch 2 Step 4/36 Loss: 6.0714\n",
      "Epoch 2 Step 5/36 Loss: 6.3820\n",
      "Epoch 2 Step 6/36 Loss: 6.1055\n",
      "Epoch 2 Step 7/36 Loss: 6.1723\n",
      "Epoch 2 Step 8/36 Loss: 7.1066\n",
      "Epoch 2 Step 9/36 Loss: 4.0233\n",
      "Epoch 2 Step 10/36 Loss: 4.1358\n",
      "Epoch 2 Step 11/36 Loss: 4.5699\n",
      "Epoch 2 Step 12/36 Loss: 4.8665\n",
      "Epoch 2 Step 13/36 Loss: 5.3822\n",
      "Epoch 2 Step 14/36 Loss: 4.7699\n",
      "Epoch 2 Step 15/36 Loss: 4.5195\n",
      "Epoch 2 Step 16/36 Loss: 5.6987\n",
      "Epoch 2 Step 17/36 Loss: 6.5923\n",
      "Epoch 2 Step 18/36 Loss: 5.2868\n",
      "Epoch 2 Step 19/36 Loss: 4.3828\n",
      "Epoch 2 Step 20/36 Loss: 4.9749\n",
      "Epoch 2 Step 21/36 Loss: 4.6700\n",
      "Epoch 2 Step 22/36 Loss: 4.8518\n",
      "Epoch 2 Step 23/36 Loss: 5.6471\n",
      "Epoch 2 Step 24/36 Loss: 4.9341\n",
      "Epoch 2 Step 25/36 Loss: 5.1393\n",
      "Epoch 2 Step 26/36 Loss: 6.0374\n",
      "Epoch 2 Step 27/36 Loss: 6.2922\n",
      "Epoch 2 Step 28/36 Loss: 6.3219\n",
      "Epoch 2 Step 29/36 Loss: 4.8883\n",
      "Epoch 2 Step 30/36 Loss: 4.7253\n",
      "Epoch 2 Step 31/36 Loss: 5.5201\n",
      "Epoch 2 Step 32/36 Loss: 6.3035\n",
      "Epoch 2 Step 33/36 Loss: 5.4460\n",
      "Epoch 2 Step 34/36 Loss: 6.0182\n",
      "Epoch 2 Step 35/36 Loss: 5.7495\n",
      "Epoch 2 Step 36/36 Loss: 6.9639\n",
      "Epoch 2 Step 1/36 Loss: 5.4580\n",
      "Epoch 2 Step 2/36 Loss: 5.8370\n",
      "Epoch 2 Step 3/36 Loss: 4.0976\n",
      "Epoch 2 Step 4/36 Loss: 6.2048\n",
      "Epoch 2 Step 5/36 Loss: 4.3341\n",
      "Epoch 2 Step 6/36 Loss: 5.8048\n",
      "Epoch 2 Step 7/36 Loss: 5.9388\n",
      "Epoch 2 Step 8/36 Loss: 4.6190\n",
      "Epoch 2 Step 9/36 Loss: 4.7401\n",
      "Epoch 2 Step 10/36 Loss: 5.0707\n",
      "Epoch 2 Step 11/36 Loss: 5.7895\n",
      "Epoch 2 Step 12/36 Loss: 6.1342\n",
      "Epoch 2 Step 13/36 Loss: 6.8377\n",
      "Epoch 2 Step 14/36 Loss: 7.0836\n",
      "Epoch 2 Step 15/36 Loss: 5.0040\n",
      "Epoch 2 Step 16/36 Loss: 4.6952\n",
      "Epoch 2 Step 17/36 Loss: 5.4831\n",
      "Epoch 2 Step 18/36 Loss: 6.1892\n",
      "Epoch 2 Step 19/36 Loss: 5.1290\n",
      "Epoch 2 Step 20/36 Loss: 5.4456\n",
      "Epoch 2 Step 21/36 Loss: 5.2545\n",
      "Epoch 2 Step 22/36 Loss: 7.0379\n",
      "Epoch 2 Step 23/36 Loss: 5.1795\n",
      "Epoch 2 Step 24/36 Loss: 5.6177\n",
      "Epoch 2 Step 25/36 Loss: 6.9096\n",
      "Epoch 2 Step 26/36 Loss: 6.0371\n",
      "Epoch 2 Step 27/36 Loss: 7.3106\n",
      "Epoch 2 Step 28/36 Loss: 4.8903\n",
      "Epoch 2 Step 29/36 Loss: 5.3207\n",
      "Epoch 2 Step 30/36 Loss: 4.6066\n",
      "Epoch 2 Step 31/36 Loss: 4.7782\n",
      "Epoch 2 Step 32/36 Loss: 6.1051\n",
      "Epoch 2 Step 33/36 Loss: 7.1608\n",
      "Epoch 2 Step 34/36 Loss: 5.4794\n",
      "Epoch 2 Step 35/36 Loss: 5.1482\n",
      "Epoch 2 Step 36/36 Loss: 6.0755\n",
      "Epoch 2 Step 1/36 Loss: 5.2330\n",
      "Epoch 2 Step 2/36 Loss: 5.1390\n",
      "Epoch 2 Step 3/36 Loss: 4.3048\n",
      "Epoch 2 Step 4/36 Loss: 6.2205\n",
      "Epoch 2 Step 5/36 Loss: 6.7192\n",
      "Epoch 2 Step 6/36 Loss: 4.3130\n",
      "Epoch 2 Step 7/36 Loss: 5.8746\n",
      "Epoch 2 Step 8/36 Loss: 6.6665\n",
      "Epoch 2 Step 9/36 Loss: 5.8370\n",
      "Epoch 2 Step 10/36 Loss: 6.0677\n",
      "Epoch 2 Step 11/36 Loss: 5.5608\n",
      "Epoch 2 Step 12/36 Loss: 5.0474\n",
      "Epoch 2 Step 13/36 Loss: 5.1396\n",
      "Epoch 2 Step 14/36 Loss: 5.5096\n",
      "Epoch 2 Step 15/36 Loss: 4.6121\n",
      "Epoch 2 Step 16/36 Loss: 5.0550\n",
      "Epoch 2 Step 17/36 Loss: 6.0966\n",
      "Epoch 2 Step 18/36 Loss: 4.9677\n",
      "Epoch 2 Step 19/36 Loss: 5.6555\n",
      "Epoch 2 Step 20/36 Loss: 4.8234\n",
      "Epoch 2 Step 21/36 Loss: 5.9290\n",
      "Epoch 2 Step 22/36 Loss: 5.1276\n",
      "Epoch 2 Step 23/36 Loss: 4.9810\n",
      "Epoch 2 Step 24/36 Loss: 5.6147\n",
      "Epoch 2 Step 25/36 Loss: 5.3424\n",
      "Epoch 2 Step 26/36 Loss: 5.0098\n",
      "Epoch 2 Step 27/36 Loss: 4.7119\n",
      "Epoch 2 Step 28/36 Loss: 6.0406\n",
      "Epoch 2 Step 29/36 Loss: 5.2098\n",
      "Epoch 2 Step 30/36 Loss: 4.5188\n",
      "Epoch 2 Step 31/36 Loss: 5.8801\n",
      "Epoch 2 Step 32/36 Loss: 4.7925\n",
      "Epoch 2 Step 33/36 Loss: 5.5120\n",
      "Epoch 2 Step 34/36 Loss: 5.7495\n",
      "Epoch 2 Step 35/36 Loss: 5.7001\n",
      "Epoch 2 Step 36/36 Loss: 5.7464\n",
      "Epoch 2 Step 1/36 Loss: 4.6175\n",
      "Epoch 2 Step 2/36 Loss: 4.9953\n",
      "Epoch 2 Step 3/36 Loss: 5.1592\n",
      "Epoch 2 Step 4/36 Loss: 4.7636\n",
      "Epoch 2 Step 5/36 Loss: 4.7078\n",
      "Epoch 2 Step 6/36 Loss: 6.4793\n",
      "Epoch 2 Step 7/36 Loss: 5.8027\n",
      "Epoch 2 Step 8/36 Loss: 5.4546\n",
      "Epoch 2 Step 9/36 Loss: 4.8276\n",
      "Epoch 2 Step 10/36 Loss: 3.9209\n",
      "Epoch 2 Step 11/36 Loss: 5.3617\n",
      "Epoch 2 Step 12/36 Loss: 4.2099\n",
      "Epoch 2 Step 13/36 Loss: 4.8512\n",
      "Epoch 2 Step 14/36 Loss: 5.8633\n",
      "Epoch 2 Step 15/36 Loss: 4.9297\n",
      "Epoch 2 Step 16/36 Loss: 6.0606\n",
      "Epoch 2 Step 17/36 Loss: 5.8071\n",
      "Epoch 2 Step 18/36 Loss: 6.4084\n",
      "Epoch 2 Step 19/36 Loss: 4.9033\n",
      "Epoch 2 Step 20/36 Loss: 5.1673\n",
      "Epoch 2 Step 21/36 Loss: 5.1182\n",
      "Epoch 2 Step 22/36 Loss: 5.2896\n",
      "Epoch 2 Step 23/36 Loss: 5.4240\n",
      "Epoch 2 Step 24/36 Loss: 5.7036\n",
      "Epoch 2 Step 25/36 Loss: 6.1683\n",
      "Epoch 2 Step 26/36 Loss: 6.7826\n",
      "Epoch 2 Step 27/36 Loss: 5.2863\n",
      "Epoch 2 Step 28/36 Loss: 5.2453\n",
      "Epoch 2 Step 29/36 Loss: 5.4495\n",
      "Epoch 2 Step 30/36 Loss: 5.1782\n",
      "Epoch 2 Step 31/36 Loss: 5.7850\n",
      "Epoch 2 Step 32/36 Loss: 6.0996\n",
      "Epoch 2 Step 33/36 Loss: 5.2605\n",
      "Epoch 2 Step 34/36 Loss: 5.2364\n",
      "Epoch 2 Step 35/36 Loss: 5.1944\n",
      "Epoch 2 Step 36/36 Loss: 4.6272\n",
      "Epoch 2 Step 1/36 Loss: 6.8682\n",
      "Epoch 2 Step 2/36 Loss: 5.8265\n",
      "Epoch 2 Step 3/36 Loss: 5.7880\n",
      "Epoch 2 Step 4/36 Loss: 4.9635\n",
      "Epoch 2 Step 5/36 Loss: 4.8001\n",
      "Epoch 2 Step 6/36 Loss: 6.2330\n",
      "Epoch 2 Step 7/36 Loss: 5.8104\n",
      "Epoch 2 Step 8/36 Loss: 5.3917\n",
      "Epoch 2 Step 9/36 Loss: 5.5968\n",
      "Epoch 2 Step 10/36 Loss: 4.6619\n",
      "Epoch 2 Step 11/36 Loss: 5.1320\n",
      "Epoch 2 Step 12/36 Loss: 6.6598\n",
      "Epoch 2 Step 13/36 Loss: 4.1863\n",
      "Epoch 2 Step 14/36 Loss: 4.6993\n",
      "Epoch 2 Step 15/36 Loss: 4.5722\n",
      "Epoch 2 Step 16/36 Loss: 6.9909\n",
      "Epoch 2 Step 17/36 Loss: 5.3464\n",
      "Epoch 2 Step 18/36 Loss: 5.5917\n",
      "Epoch 2 Step 19/36 Loss: 6.0093\n",
      "Epoch 2 Step 20/36 Loss: 5.3408\n",
      "Epoch 2 Step 21/36 Loss: 5.5700\n",
      "Epoch 2 Step 22/36 Loss: 5.9612\n",
      "Epoch 2 Step 23/36 Loss: 5.0285\n",
      "Epoch 2 Step 24/36 Loss: 5.1914\n",
      "Epoch 2 Step 25/36 Loss: 5.8869\n",
      "Epoch 2 Step 26/36 Loss: 5.2986\n",
      "Epoch 2 Step 27/36 Loss: 4.4539\n",
      "Epoch 2 Step 28/36 Loss: 5.2165\n",
      "Epoch 2 Step 29/36 Loss: 6.1426\n",
      "Epoch 2 Step 30/36 Loss: 5.6437\n",
      "Epoch 2 Step 31/36 Loss: 4.0724\n",
      "Epoch 2 Step 32/36 Loss: 6.6887\n",
      "Epoch 2 Step 33/36 Loss: 4.3282\n",
      "Epoch 2 Step 34/36 Loss: 5.0245\n",
      "Epoch 2 Step 35/36 Loss: 4.6722\n",
      "Epoch 2 Step 36/36 Loss: 6.3327\n",
      "Epoch 2 Step 1/36 Loss: 6.1228\n",
      "Epoch 2 Step 2/36 Loss: 6.4275\n",
      "Epoch 2 Step 3/36 Loss: 6.3511\n",
      "Epoch 2 Step 4/36 Loss: 4.2100\n",
      "Epoch 2 Step 5/36 Loss: 5.8984\n",
      "Epoch 2 Step 6/36 Loss: 5.7387\n",
      "Epoch 2 Step 7/36 Loss: 4.3419\n",
      "Epoch 2 Step 8/36 Loss: 5.3536\n",
      "Epoch 2 Step 9/36 Loss: 5.7620\n",
      "Epoch 2 Step 10/36 Loss: 5.3670\n",
      "Epoch 2 Step 11/36 Loss: 6.2508\n",
      "Epoch 2 Step 12/36 Loss: 5.1259\n",
      "Epoch 2 Step 13/36 Loss: 4.1087\n",
      "Epoch 2 Step 14/36 Loss: 5.4650\n",
      "Epoch 2 Step 15/36 Loss: 4.9444\n",
      "Epoch 2 Step 16/36 Loss: 5.4007\n",
      "Epoch 2 Step 17/36 Loss: 6.5486\n",
      "Epoch 2 Step 18/36 Loss: 4.0371\n",
      "Epoch 2 Step 19/36 Loss: 4.7073\n",
      "Epoch 2 Step 20/36 Loss: 6.1284\n",
      "Epoch 2 Step 21/36 Loss: 6.1338\n",
      "Epoch 2 Step 22/36 Loss: 5.3122\n",
      "Epoch 2 Step 23/36 Loss: 6.3511\n",
      "Epoch 2 Step 24/36 Loss: 6.1237\n",
      "Epoch 2 Step 25/36 Loss: 5.9171\n",
      "Epoch 2 Step 26/36 Loss: 5.7735\n",
      "Epoch 2 Step 27/36 Loss: 5.0026\n",
      "Epoch 2 Step 28/36 Loss: 5.8208\n",
      "Epoch 2 Step 29/36 Loss: 6.2531\n",
      "Epoch 2 Step 30/36 Loss: 5.7149\n",
      "Epoch 2 Step 31/36 Loss: 5.1544\n",
      "Epoch 2 Step 32/36 Loss: 5.4184\n",
      "Epoch 2 Step 33/36 Loss: 4.9329\n",
      "Epoch 2 Step 34/36 Loss: 5.7602\n",
      "Epoch 2 Step 35/36 Loss: 5.3605\n",
      "Epoch 2 Step 36/36 Loss: 5.4778\n",
      "Epoch 2 Step 1/36 Loss: 6.3568\n",
      "Epoch 2 Step 2/36 Loss: 5.7434\n",
      "Epoch 2 Step 3/36 Loss: 5.2209\n",
      "Epoch 2 Step 4/36 Loss: 5.9094\n",
      "Epoch 2 Step 5/36 Loss: 4.9561\n",
      "Epoch 2 Step 6/36 Loss: 6.8101\n",
      "Epoch 2 Step 7/36 Loss: 6.1880\n",
      "Epoch 2 Step 8/36 Loss: 4.0992\n",
      "Epoch 2 Step 9/36 Loss: 4.8348\n",
      "Epoch 2 Step 10/36 Loss: 6.3981\n",
      "Epoch 2 Step 11/36 Loss: 5.3365\n",
      "Epoch 2 Step 12/36 Loss: 5.4082\n",
      "Epoch 2 Step 13/36 Loss: 5.5361\n",
      "Epoch 2 Step 14/36 Loss: 5.5166\n",
      "Epoch 2 Step 15/36 Loss: 4.3957\n",
      "Epoch 2 Step 16/36 Loss: 5.9751\n",
      "Epoch 2 Step 17/36 Loss: 4.1291\n",
      "Epoch 2 Step 18/36 Loss: 6.3572\n",
      "Epoch 2 Step 19/36 Loss: 4.9180\n",
      "Epoch 2 Step 20/36 Loss: 4.8222\n",
      "Epoch 2 Step 21/36 Loss: 5.3141\n",
      "Epoch 2 Step 22/36 Loss: 5.5931\n",
      "Epoch 2 Step 23/36 Loss: 6.4102\n",
      "Epoch 2 Step 24/36 Loss: 5.7997\n",
      "Epoch 2 Step 25/36 Loss: 6.0227\n",
      "Epoch 2 Step 26/36 Loss: 4.9549\n",
      "Epoch 2 Step 27/36 Loss: 5.6787\n",
      "Epoch 2 Step 28/36 Loss: 5.3391\n",
      "Epoch 2 Step 29/36 Loss: 6.3668\n",
      "Epoch 2 Step 30/36 Loss: 5.4437\n",
      "Epoch 2 Step 31/36 Loss: 4.1991\n",
      "Epoch 2 Step 32/36 Loss: 4.5270\n",
      "Epoch 2 Step 33/36 Loss: 4.8954\n",
      "Epoch 2 Step 34/36 Loss: 6.6121\n",
      "Epoch 2 Step 35/36 Loss: 5.3422\n",
      "Epoch 2 Step 36/36 Loss: 4.6692\n",
      "Epoch 2 Step 1/36 Loss: 4.2524\n",
      "Epoch 2 Step 2/36 Loss: 5.1732\n",
      "Epoch 2 Step 3/36 Loss: 3.9124\n",
      "Epoch 2 Step 4/36 Loss: 5.8886\n",
      "Epoch 2 Step 5/36 Loss: 4.8842\n",
      "Epoch 2 Step 6/36 Loss: 5.0773\n",
      "Epoch 2 Step 7/36 Loss: 5.6381\n",
      "Epoch 2 Step 8/36 Loss: 5.3772\n",
      "Epoch 2 Step 9/36 Loss: 5.1129\n",
      "Epoch 2 Step 10/36 Loss: 5.1622\n",
      "Epoch 2 Step 11/36 Loss: 5.3564\n",
      "Epoch 2 Step 12/36 Loss: 4.6916\n",
      "Epoch 2 Step 13/36 Loss: 5.5340\n",
      "Epoch 2 Step 14/36 Loss: 4.8270\n",
      "Epoch 2 Step 15/36 Loss: 5.9166\n",
      "Epoch 2 Step 16/36 Loss: 6.5958\n",
      "Epoch 2 Step 17/36 Loss: 5.5037\n",
      "Epoch 2 Step 18/36 Loss: 5.5466\n",
      "Epoch 2 Step 19/36 Loss: 6.3643\n",
      "Epoch 2 Step 20/36 Loss: 5.3424\n",
      "Epoch 2 Step 21/36 Loss: 4.0704\n",
      "Epoch 2 Step 22/36 Loss: 5.8102\n",
      "Epoch 2 Step 23/36 Loss: 5.1646\n",
      "Epoch 2 Step 24/36 Loss: 5.9029\n",
      "Epoch 2 Step 25/36 Loss: 6.1784\n",
      "Epoch 2 Step 26/36 Loss: 4.9655\n",
      "Epoch 2 Step 27/36 Loss: 5.0996\n",
      "Epoch 2 Step 28/36 Loss: 5.9811\n",
      "Epoch 2 Step 29/36 Loss: 5.3938\n",
      "Epoch 2 Step 30/36 Loss: 5.1985\n",
      "Epoch 2 Step 31/36 Loss: 6.1665\n",
      "Epoch 2 Step 32/36 Loss: 6.1351\n",
      "Epoch 2 Step 33/36 Loss: 3.9259\n",
      "Epoch 2 Step 34/36 Loss: 5.1933\n",
      "Epoch 2 Step 35/36 Loss: 5.9837\n",
      "Epoch 2 Step 36/36 Loss: 6.3110\n",
      "Epoch 2 Step 1/36 Loss: 5.6124\n",
      "Epoch 2 Step 2/36 Loss: 5.7435\n",
      "Epoch 2 Step 3/36 Loss: 5.8312\n",
      "Epoch 2 Step 4/36 Loss: 5.3255\n",
      "Epoch 2 Step 5/36 Loss: 7.7802\n",
      "Epoch 2 Step 6/36 Loss: 4.5309\n",
      "Epoch 2 Step 7/36 Loss: 5.7919\n",
      "Epoch 2 Step 8/36 Loss: 4.9515\n",
      "Epoch 2 Step 9/36 Loss: 5.4119\n",
      "Epoch 2 Step 10/36 Loss: 5.2453\n",
      "Epoch 2 Step 11/36 Loss: 4.6405\n",
      "Epoch 2 Step 12/36 Loss: 5.4551\n",
      "Epoch 2 Step 13/36 Loss: 4.9381\n",
      "Epoch 2 Step 14/36 Loss: 5.1684\n",
      "Epoch 2 Step 15/36 Loss: 5.3000\n",
      "Epoch 2 Step 16/36 Loss: 6.0121\n",
      "Epoch 2 Step 17/36 Loss: 4.7858\n",
      "Epoch 2 Step 18/36 Loss: 4.8559\n",
      "Epoch 2 Step 19/36 Loss: 5.7274\n",
      "Epoch 2 Step 20/36 Loss: 5.4885\n",
      "Epoch 2 Step 21/36 Loss: 4.5063\n",
      "Epoch 2 Step 22/36 Loss: 6.1252\n",
      "Epoch 2 Step 23/36 Loss: 4.7489\n",
      "Epoch 2 Step 24/36 Loss: 6.7156\n",
      "Epoch 2 Step 25/36 Loss: 5.3163\n",
      "Epoch 2 Step 26/36 Loss: 4.4877\n",
      "Epoch 2 Step 27/36 Loss: 5.0726\n",
      "Epoch 2 Step 28/36 Loss: 5.5814\n",
      "Epoch 2 Step 29/36 Loss: 4.6702\n",
      "Epoch 2 Step 30/36 Loss: 5.3719\n",
      "Epoch 2 Step 31/36 Loss: 5.1429\n",
      "Epoch 2 Step 32/36 Loss: 4.0975\n",
      "Epoch 2 Step 33/36 Loss: 6.1065\n",
      "Epoch 2 Step 34/36 Loss: 5.5418\n",
      "Epoch 2 Step 35/36 Loss: 5.7683\n",
      "Epoch 2 Step 36/36 Loss: 5.7197\n",
      "Epoch 2 Step 1/36 Loss: 5.9397\n",
      "Epoch 2 Step 2/36 Loss: 5.0854\n",
      "Epoch 2 Step 3/36 Loss: 6.7845\n",
      "Epoch 2 Step 4/36 Loss: 5.6801\n",
      "Epoch 2 Step 5/36 Loss: 6.8474\n",
      "Epoch 2 Step 6/36 Loss: 6.2329\n",
      "Epoch 2 Step 7/36 Loss: 4.4970\n",
      "Epoch 2 Step 8/36 Loss: 6.2503\n",
      "Epoch 2 Step 9/36 Loss: 4.6719\n",
      "Epoch 2 Step 10/36 Loss: 6.6204\n",
      "Epoch 2 Step 11/36 Loss: 5.0275\n",
      "Epoch 2 Step 12/36 Loss: 5.1080\n",
      "Epoch 2 Step 13/36 Loss: 5.7914\n",
      "Epoch 2 Step 14/36 Loss: 5.3053\n",
      "Epoch 2 Step 15/36 Loss: 6.1077\n",
      "Epoch 2 Step 16/36 Loss: 4.7925\n",
      "Epoch 2 Step 17/36 Loss: 5.4345\n",
      "Epoch 2 Step 18/36 Loss: 5.9363\n",
      "Epoch 2 Step 19/36 Loss: 6.2443\n",
      "Epoch 2 Step 20/36 Loss: 4.7614\n",
      "Epoch 2 Step 21/36 Loss: 4.4720\n",
      "Epoch 2 Step 22/36 Loss: 5.9038\n",
      "Epoch 2 Step 23/36 Loss: 4.8677\n",
      "Epoch 2 Step 24/36 Loss: 6.2439\n",
      "Epoch 2 Step 25/36 Loss: 5.7609\n",
      "Epoch 2 Step 26/36 Loss: 5.1496\n",
      "Epoch 2 Step 27/36 Loss: 6.5237\n",
      "Epoch 2 Step 28/36 Loss: 5.0041\n",
      "Epoch 2 Step 29/36 Loss: 5.3392\n",
      "Epoch 2 Step 30/36 Loss: 4.4926\n",
      "Epoch 2 Step 31/36 Loss: 5.0729\n",
      "Epoch 2 Step 32/36 Loss: 5.9751\n",
      "Epoch 2 Step 33/36 Loss: 5.7163\n",
      "Epoch 2 Step 34/36 Loss: 5.0443\n",
      "Epoch 2 Step 35/36 Loss: 5.0434\n",
      "Epoch 2 Step 36/36 Loss: 4.9883\n",
      "Epoch 2 Step 1/36 Loss: 6.5760\n",
      "Epoch 2 Step 2/36 Loss: 6.1966\n",
      "Epoch 2 Step 3/36 Loss: 5.1651\n",
      "Epoch 2 Step 4/36 Loss: 7.7686\n",
      "Epoch 2 Step 5/36 Loss: 5.5780\n",
      "Epoch 2 Step 6/36 Loss: 4.7803\n",
      "Epoch 2 Step 7/36 Loss: 4.2030\n",
      "Epoch 2 Step 8/36 Loss: 4.2344\n",
      "Epoch 2 Step 9/36 Loss: 5.3934\n",
      "Epoch 2 Step 10/36 Loss: 5.6423\n",
      "Epoch 2 Step 11/36 Loss: 6.2447\n",
      "Epoch 2 Step 12/36 Loss: 4.9034\n",
      "Epoch 2 Step 13/36 Loss: 5.9622\n",
      "Epoch 2 Step 14/36 Loss: 4.9524\n",
      "Epoch 2 Step 15/36 Loss: 5.1752\n",
      "Epoch 2 Step 16/36 Loss: 5.5462\n",
      "Epoch 2 Step 17/36 Loss: 5.2368\n",
      "Epoch 2 Step 18/36 Loss: 5.6174\n",
      "Epoch 2 Step 19/36 Loss: 6.2158\n",
      "Epoch 2 Step 20/36 Loss: 5.9797\n",
      "Epoch 2 Step 21/36 Loss: 6.2490\n",
      "Epoch 2 Step 22/36 Loss: 4.5755\n",
      "Epoch 2 Step 23/36 Loss: 5.7701\n",
      "Epoch 2 Step 24/36 Loss: 5.5150\n",
      "Epoch 2 Step 25/36 Loss: 5.3976\n",
      "Epoch 2 Step 26/36 Loss: 5.7712\n",
      "Epoch 2 Step 27/36 Loss: 5.0056\n",
      "Epoch 2 Step 28/36 Loss: 5.5974\n",
      "Epoch 2 Step 29/36 Loss: 6.0965\n",
      "Epoch 2 Step 30/36 Loss: 4.9078\n",
      "Epoch 2 Step 31/36 Loss: 5.7824\n",
      "Epoch 2 Step 32/36 Loss: 6.5865\n",
      "Epoch 2 Step 33/36 Loss: 5.4660\n",
      "Epoch 2 Step 34/36 Loss: 5.0695\n",
      "Epoch 2 Step 35/36 Loss: 5.7288\n",
      "Epoch 2 Step 36/36 Loss: 4.1270\n",
      "Epoch 2 Step 1/36 Loss: 5.6011\n",
      "Epoch 2 Step 2/36 Loss: 5.2201\n",
      "Epoch 2 Step 3/36 Loss: 5.7140\n",
      "Epoch 2 Step 4/36 Loss: 4.7317\n",
      "Epoch 2 Step 5/36 Loss: 5.4090\n",
      "Epoch 2 Step 6/36 Loss: 6.2240\n",
      "Epoch 2 Step 7/36 Loss: 5.5726\n",
      "Epoch 2 Step 8/36 Loss: 5.8960\n",
      "Epoch 2 Step 9/36 Loss: 5.2559\n",
      "Epoch 2 Step 10/36 Loss: 4.4303\n",
      "Epoch 2 Step 11/36 Loss: 5.1949\n",
      "Epoch 2 Step 12/36 Loss: 5.5846\n",
      "Epoch 2 Step 13/36 Loss: 5.6667\n",
      "Epoch 2 Step 14/36 Loss: 4.5216\n",
      "Epoch 2 Step 15/36 Loss: 4.5680\n",
      "Epoch 2 Step 16/36 Loss: 4.3405\n",
      "Epoch 2 Step 17/36 Loss: 5.6447\n",
      "Epoch 2 Step 18/36 Loss: 4.8175\n",
      "Epoch 2 Step 19/36 Loss: 4.0320\n",
      "Epoch 2 Step 20/36 Loss: 4.5698\n",
      "Epoch 2 Step 21/36 Loss: 5.3597\n",
      "Epoch 2 Step 22/36 Loss: 5.2878\n",
      "Epoch 2 Step 23/36 Loss: 5.9744\n",
      "Epoch 2 Step 24/36 Loss: 4.5906\n",
      "Epoch 2 Step 25/36 Loss: 5.5843\n",
      "Epoch 2 Step 26/36 Loss: 4.9874\n",
      "Epoch 2 Step 27/36 Loss: 5.2314\n",
      "Epoch 2 Step 28/36 Loss: 4.8934\n",
      "Epoch 2 Step 29/36 Loss: 4.9016\n",
      "Epoch 2 Step 30/36 Loss: 5.9790\n",
      "Epoch 2 Step 31/36 Loss: 5.7484\n",
      "Epoch 2 Step 32/36 Loss: 4.9768\n",
      "Epoch 2 Step 33/36 Loss: 6.1842\n",
      "Epoch 2 Step 34/36 Loss: 4.9198\n",
      "Epoch 2 Step 35/36 Loss: 5.4038\n",
      "Epoch 2 Step 36/36 Loss: 5.8724\n",
      "Epoch 2 Step 1/36 Loss: 6.0236\n",
      "Epoch 2 Step 2/36 Loss: 6.7428\n",
      "Epoch 2 Step 3/36 Loss: 5.2658\n",
      "Epoch 2 Step 4/36 Loss: 4.9121\n",
      "Epoch 2 Step 5/36 Loss: 5.1150\n",
      "Epoch 2 Step 6/36 Loss: 4.5179\n",
      "Epoch 2 Step 7/36 Loss: 6.3271\n",
      "Epoch 2 Step 8/36 Loss: 6.4242\n",
      "Epoch 2 Step 9/36 Loss: 4.7181\n",
      "Epoch 2 Step 10/36 Loss: 4.6850\n",
      "Epoch 2 Step 11/36 Loss: 5.0957\n",
      "Epoch 2 Step 12/36 Loss: 4.7353\n",
      "Epoch 2 Step 13/36 Loss: 6.0213\n",
      "Epoch 2 Step 14/36 Loss: 5.4556\n",
      "Epoch 2 Step 15/36 Loss: 5.5867\n",
      "Epoch 2 Step 16/36 Loss: 5.4854\n",
      "Epoch 2 Step 17/36 Loss: 4.5941\n",
      "Epoch 2 Step 18/36 Loss: 5.8459\n",
      "Epoch 2 Step 19/36 Loss: 5.9097\n",
      "Epoch 2 Step 20/36 Loss: 4.7556\n",
      "Epoch 2 Step 21/36 Loss: 5.8555\n",
      "Epoch 2 Step 22/36 Loss: 7.1719\n",
      "Epoch 2 Step 23/36 Loss: 4.4765\n",
      "Epoch 2 Step 24/36 Loss: 5.6785\n",
      "Epoch 2 Step 25/36 Loss: 6.2040\n",
      "Epoch 2 Step 26/36 Loss: 6.5169\n",
      "Epoch 2 Step 27/36 Loss: 4.3236\n",
      "Epoch 2 Step 28/36 Loss: 3.9176\n",
      "Epoch 2 Step 29/36 Loss: 4.5156\n",
      "Epoch 2 Step 30/36 Loss: 5.5412\n",
      "Epoch 2 Step 31/36 Loss: 6.7185\n",
      "Epoch 2 Step 32/36 Loss: 4.8379\n",
      "Epoch 2 Step 33/36 Loss: 5.1466\n",
      "Epoch 2 Step 34/36 Loss: 5.1661\n",
      "Epoch 2 Step 35/36 Loss: 4.8497\n",
      "Epoch 2 Step 36/36 Loss: 6.0383\n",
      "Epoch 2 Step 1/36 Loss: 4.6263\n",
      "Epoch 2 Step 2/36 Loss: 4.6392\n",
      "Epoch 2 Step 3/36 Loss: 5.2328\n",
      "Epoch 2 Step 4/36 Loss: 5.1993\n",
      "Epoch 2 Step 5/36 Loss: 5.8690\n",
      "Epoch 2 Step 6/36 Loss: 5.0602\n",
      "Epoch 2 Step 7/36 Loss: 4.3771\n",
      "Epoch 2 Step 8/36 Loss: 6.0681\n",
      "Epoch 2 Step 9/36 Loss: 5.9313\n",
      "Epoch 2 Step 10/36 Loss: 5.9080\n",
      "Epoch 2 Step 11/36 Loss: 5.5045\n",
      "Epoch 2 Step 12/36 Loss: 5.1969\n",
      "Epoch 2 Step 13/36 Loss: 6.1917\n",
      "Epoch 2 Step 14/36 Loss: 4.6333\n",
      "Epoch 2 Step 15/36 Loss: 5.9915\n",
      "Epoch 2 Step 16/36 Loss: 4.6521\n",
      "Epoch 2 Step 17/36 Loss: 6.6210\n",
      "Epoch 2 Step 18/36 Loss: 5.3816\n",
      "Epoch 2 Step 19/36 Loss: 4.2289\n",
      "Epoch 2 Step 20/36 Loss: 6.2330\n",
      "Epoch 2 Step 21/36 Loss: 6.2830\n",
      "Epoch 2 Step 22/36 Loss: 4.1736\n",
      "Epoch 2 Step 23/36 Loss: 5.1262\n",
      "Epoch 2 Step 24/36 Loss: 5.5439\n",
      "Epoch 2 Step 25/36 Loss: 5.1434\n",
      "Epoch 2 Step 26/36 Loss: 4.8070\n",
      "Epoch 2 Step 27/36 Loss: 6.5122\n",
      "Epoch 2 Step 28/36 Loss: 5.0966\n",
      "Epoch 2 Step 29/36 Loss: 4.7723\n",
      "Epoch 2 Step 30/36 Loss: 5.3993\n",
      "Epoch 2 Step 31/36 Loss: 6.0998\n",
      "Epoch 2 Step 32/36 Loss: 6.4309\n",
      "Epoch 2 Step 33/36 Loss: 6.1460\n",
      "Epoch 2 Step 34/36 Loss: 5.5307\n",
      "Epoch 2 Step 35/36 Loss: 6.3225\n",
      "Epoch 2 Step 36/36 Loss: 4.2904\n",
      "Epoch 2 Step 1/36 Loss: 5.5340\n",
      "Epoch 2 Step 2/36 Loss: 5.3860\n",
      "Epoch 2 Step 3/36 Loss: 4.8640\n",
      "Epoch 2 Step 4/36 Loss: 4.7459\n",
      "Epoch 2 Step 5/36 Loss: 5.4938\n",
      "Epoch 2 Step 6/36 Loss: 6.5630\n",
      "Epoch 2 Step 7/36 Loss: 7.2948\n",
      "Epoch 2 Step 8/36 Loss: 5.2735\n",
      "Epoch 2 Step 9/36 Loss: 5.5030\n",
      "Epoch 2 Step 10/36 Loss: 4.6181\n",
      "Epoch 2 Step 11/36 Loss: 4.6728\n",
      "Epoch 2 Step 12/36 Loss: 4.7791\n",
      "Epoch 2 Step 13/36 Loss: 4.8540\n",
      "Epoch 2 Step 14/36 Loss: 4.9550\n",
      "Epoch 2 Step 15/36 Loss: 4.5614\n",
      "Epoch 2 Step 16/36 Loss: 5.1574\n",
      "Epoch 2 Step 17/36 Loss: 7.4889\n",
      "Epoch 2 Step 18/36 Loss: 4.6358\n",
      "Epoch 2 Step 19/36 Loss: 6.6373\n",
      "Epoch 2 Step 20/36 Loss: 4.4106\n",
      "Epoch 2 Step 21/36 Loss: 5.5633\n",
      "Epoch 2 Step 22/36 Loss: 5.6341\n",
      "Epoch 2 Step 23/36 Loss: 4.5888\n",
      "Epoch 2 Step 24/36 Loss: 5.5305\n",
      "Epoch 2 Step 25/36 Loss: 4.4481\n",
      "Epoch 2 Step 26/36 Loss: 5.2360\n",
      "Epoch 2 Step 27/36 Loss: 5.8495\n",
      "Epoch 2 Step 28/36 Loss: 4.9703\n",
      "Epoch 2 Step 29/36 Loss: 5.3529\n",
      "Epoch 2 Step 30/36 Loss: 7.4700\n",
      "Epoch 2 Step 31/36 Loss: 4.7760\n",
      "Epoch 2 Step 32/36 Loss: 5.0883\n",
      "Epoch 2 Step 33/36 Loss: 4.2825\n",
      "Epoch 2 Step 34/36 Loss: 5.2183\n",
      "Epoch 2 Step 35/36 Loss: 5.1706\n",
      "Epoch 2 Step 36/36 Loss: 5.9523\n",
      "Epoch 2 Step 1/36 Loss: 4.5250\n",
      "Epoch 2 Step 2/36 Loss: 6.3529\n",
      "Epoch 2 Step 3/36 Loss: 5.3911\n",
      "Epoch 2 Step 4/36 Loss: 5.4099\n",
      "Epoch 2 Step 5/36 Loss: 6.2099\n",
      "Epoch 2 Step 6/36 Loss: 5.3230\n",
      "Epoch 2 Step 7/36 Loss: 4.4514\n",
      "Epoch 2 Step 8/36 Loss: 6.4857\n",
      "Epoch 2 Step 9/36 Loss: 5.5428\n",
      "Epoch 2 Step 10/36 Loss: 4.6941\n",
      "Epoch 2 Step 11/36 Loss: 5.1499\n",
      "Epoch 2 Step 12/36 Loss: 6.4777\n",
      "Epoch 2 Step 13/36 Loss: 4.9237\n",
      "Epoch 2 Step 14/36 Loss: 5.8327\n",
      "Epoch 2 Step 15/36 Loss: 5.0247\n",
      "Epoch 2 Step 16/36 Loss: 5.7171\n",
      "Epoch 2 Step 17/36 Loss: 4.0120\n",
      "Epoch 2 Step 18/36 Loss: 4.6269\n",
      "Epoch 2 Step 19/36 Loss: 5.7774\n",
      "Epoch 2 Step 20/36 Loss: 5.5915\n",
      "Epoch 2 Step 21/36 Loss: 4.7644\n",
      "Epoch 2 Step 22/36 Loss: 6.0345\n",
      "Epoch 2 Step 23/36 Loss: 5.1582\n",
      "Epoch 2 Step 24/36 Loss: 6.4335\n",
      "Epoch 2 Step 25/36 Loss: 5.2628\n",
      "Epoch 2 Step 26/36 Loss: 4.8354\n",
      "Epoch 2 Step 27/36 Loss: 6.2467\n",
      "Epoch 2 Step 28/36 Loss: 5.5998\n",
      "Epoch 2 Step 29/36 Loss: 6.2956\n",
      "Epoch 2 Step 30/36 Loss: 5.4723\n",
      "Epoch 2 Step 31/36 Loss: 4.4247\n",
      "Epoch 2 Step 32/36 Loss: 5.2390\n",
      "Epoch 2 Step 33/36 Loss: 6.3485\n",
      "Epoch 2 Step 34/36 Loss: 5.5707\n",
      "Epoch 2 Step 35/36 Loss: 6.1375\n",
      "Epoch 2 Step 36/36 Loss: 5.2081\n",
      "Epoch 2 Step 1/36 Loss: 4.9157\n",
      "Epoch 2 Step 2/36 Loss: 6.4820\n",
      "Epoch 2 Step 3/36 Loss: 5.4320\n",
      "Epoch 2 Step 4/36 Loss: 5.1796\n",
      "Epoch 2 Step 5/36 Loss: 5.4837\n",
      "Epoch 2 Step 6/36 Loss: 5.9451\n",
      "Epoch 2 Step 7/36 Loss: 5.2672\n",
      "Epoch 2 Step 8/36 Loss: 5.2957\n",
      "Epoch 2 Step 9/36 Loss: 5.9735\n",
      "Epoch 2 Step 10/36 Loss: 6.1126\n",
      "Epoch 2 Step 11/36 Loss: 5.3198\n",
      "Epoch 2 Step 12/36 Loss: 4.4686\n",
      "Epoch 2 Step 13/36 Loss: 4.9430\n",
      "Epoch 2 Step 14/36 Loss: 5.4838\n",
      "Epoch 2 Step 15/36 Loss: 5.8043\n",
      "Epoch 2 Step 16/36 Loss: 5.2483\n",
      "Epoch 2 Step 17/36 Loss: 5.2558\n",
      "Epoch 2 Step 18/36 Loss: 4.5645\n",
      "Epoch 2 Step 19/36 Loss: 4.6600\n",
      "Epoch 2 Step 20/36 Loss: 4.0755\n",
      "Epoch 2 Step 21/36 Loss: 6.9592\n",
      "Epoch 2 Step 22/36 Loss: 5.4108\n",
      "Epoch 2 Step 23/36 Loss: 6.0040\n",
      "Epoch 2 Step 24/36 Loss: 5.2399\n",
      "Epoch 2 Step 25/36 Loss: 4.8256\n",
      "Epoch 2 Step 26/36 Loss: 6.1493\n",
      "Epoch 2 Step 27/36 Loss: 5.5231\n",
      "Epoch 2 Step 28/36 Loss: 5.6454\n",
      "Epoch 2 Step 29/36 Loss: 6.2829\n",
      "Epoch 2 Step 30/36 Loss: 5.1482\n",
      "Epoch 2 Step 31/36 Loss: 6.3035\n",
      "Epoch 2 Step 32/36 Loss: 5.3041\n",
      "Epoch 2 Step 33/36 Loss: 5.0345\n",
      "Epoch 2 Step 34/36 Loss: 4.5565\n",
      "Epoch 2 Step 35/36 Loss: 4.7163\n",
      "Epoch 2 Step 36/36 Loss: 6.1735\n",
      "Epoch 2 Step 1/36 Loss: 5.1125\n",
      "Epoch 2 Step 2/36 Loss: 6.5070\n",
      "Epoch 2 Step 3/36 Loss: 6.2405\n",
      "Epoch 2 Step 4/36 Loss: 5.2589\n",
      "Epoch 2 Step 5/36 Loss: 5.4779\n",
      "Epoch 2 Step 6/36 Loss: 6.6275\n",
      "Epoch 2 Step 7/36 Loss: 4.3610\n",
      "Epoch 2 Step 8/36 Loss: 6.4464\n",
      "Epoch 2 Step 9/36 Loss: 5.8960\n",
      "Epoch 2 Step 10/36 Loss: 6.0922\n",
      "Epoch 2 Step 11/36 Loss: 6.6915\n",
      "Epoch 2 Step 12/36 Loss: 4.8098\n",
      "Epoch 2 Step 13/36 Loss: 5.4677\n",
      "Epoch 2 Step 14/36 Loss: 5.2047\n",
      "Epoch 2 Step 15/36 Loss: 5.2652\n",
      "Epoch 2 Step 16/36 Loss: 5.9037\n",
      "Epoch 2 Step 17/36 Loss: 4.7365\n",
      "Epoch 2 Step 18/36 Loss: 5.1877\n",
      "Epoch 2 Step 19/36 Loss: 4.8352\n",
      "Epoch 2 Step 20/36 Loss: 3.9684\n",
      "Epoch 2 Step 21/36 Loss: 5.4703\n",
      "Epoch 2 Step 22/36 Loss: 5.9335\n",
      "Epoch 2 Step 23/36 Loss: 6.4656\n",
      "Epoch 2 Step 24/36 Loss: 5.2504\n",
      "Epoch 2 Step 25/36 Loss: 5.8823\n",
      "Epoch 2 Step 26/36 Loss: 5.6680\n",
      "Epoch 2 Step 27/36 Loss: 5.1421\n",
      "Epoch 2 Step 28/36 Loss: 5.5252\n",
      "Epoch 2 Step 29/36 Loss: 5.3299\n",
      "Epoch 2 Step 30/36 Loss: 5.1863\n",
      "Epoch 2 Step 31/36 Loss: 4.9766\n",
      "Epoch 2 Step 32/36 Loss: 4.7088\n",
      "Epoch 2 Step 28/36 Loss: 5.4022\n",
      "Epoch 2 Step 29/36 Loss: 6.4556\n",
      "Epoch 2 Step 30/36 Loss: 4.4936\n",
      "Epoch 2 Step 31/36 Loss: 6.0285\n",
      "Epoch 2 Step 32/36 Loss: 5.2398\n",
      "Epoch 2 Step 33/36 Loss: 4.8429\n",
      "Epoch 2 Step 34/36 Loss: 5.8513\n",
      "Epoch 2 Step 35/36 Loss: 6.2282\n",
      "Epoch 2 Step 36/36 Loss: 5.3092\n",
      "Epoch 2 Step 1/36 Loss: 5.0583\n",
      "Epoch 2 Step 2/36 Loss: 4.2456\n",
      "Epoch 2 Step 3/36 Loss: 4.8432\n",
      "Epoch 2 Step 4/36 Loss: 5.9724\n",
      "Epoch 2 Step 5/36 Loss: 5.4788\n",
      "Epoch 2 Step 6/36 Loss: 4.3211\n",
      "Epoch 2 Step 7/36 Loss: 5.9828\n",
      "Epoch 2 Step 8/36 Loss: 5.0459\n",
      "Epoch 2 Step 9/36 Loss: 6.1019\n",
      "Epoch 2 Step 10/36 Loss: 5.1549\n",
      "Epoch 2 Step 11/36 Loss: 6.0409\n",
      "Epoch 2 Step 12/36 Loss: 4.7741\n",
      "Epoch 2 Step 13/36 Loss: 4.9250\n",
      "Epoch 2 Step 14/36 Loss: 4.3912\n",
      "Epoch 2 Step 15/36 Loss: 5.0620\n",
      "Epoch 2 Step 16/36 Loss: 5.1726\n",
      "Epoch 2 Step 17/36 Loss: 4.7776\n",
      "Epoch 2 Step 18/36 Loss: 6.1322\n",
      "Epoch 2 Step 19/36 Loss: 5.9603\n",
      "Epoch 2 Step 20/36 Loss: 4.3234\n",
      "Epoch 2 Step 21/36 Loss: 5.3370\n",
      "Epoch 2 Step 22/36 Loss: 4.3595\n",
      "Epoch 2 Step 23/36 Loss: 5.6082\n",
      "Epoch 2 Step 24/36 Loss: 6.2611\n",
      "Epoch 2 Step 25/36 Loss: 5.3240\n",
      "Epoch 2 Step 26/36 Loss: 5.2588\n",
      "Epoch 2 Step 27/36 Loss: 5.0316\n",
      "Epoch 2 Step 28/36 Loss: 4.7408\n",
      "Epoch 2 Step 29/36 Loss: 6.5920\n",
      "Epoch 2 Step 30/36 Loss: 4.1889\n",
      "Epoch 2 Step 31/36 Loss: 6.5292\n",
      "Epoch 2 Step 32/36 Loss: 6.1488\n",
      "Epoch 2 Step 33/36 Loss: 6.2570\n",
      "Epoch 2 Step 34/36 Loss: 5.6073\n",
      "Epoch 2 Step 35/36 Loss: 5.7817\n",
      "Epoch 2 Step 36/36 Loss: 5.0553\n",
      "Epoch 2 Step 1/36 Loss: 5.2128\n",
      "Epoch 2 Step 2/36 Loss: 4.7710\n",
      "Epoch 2 Step 3/36 Loss: 6.1552\n",
      "Epoch 2 Step 4/36 Loss: 4.9836\n",
      "Epoch 2 Step 5/36 Loss: 5.2923\n",
      "Epoch 2 Step 6/36 Loss: 4.7604\n",
      "Epoch 2 Step 7/36 Loss: 5.9471\n",
      "Epoch 2 Step 8/36 Loss: 5.9520\n",
      "Epoch 2 Step 9/36 Loss: 5.4420\n",
      "Epoch 2 Step 10/36 Loss: 5.6068\n",
      "Epoch 2 Step 11/36 Loss: 5.1865\n",
      "Epoch 2 Step 12/36 Loss: 4.9390\n",
      "Epoch 2 Step 13/36 Loss: 4.1821\n",
      "Epoch 2 Step 14/36 Loss: 5.2312\n",
      "Epoch 2 Step 15/36 Loss: 5.7945\n",
      "Epoch 2 Step 16/36 Loss: 4.3208\n",
      "Epoch 2 Step 17/36 Loss: 4.4720\n",
      "Epoch 2 Step 18/36 Loss: 6.0924\n",
      "Epoch 2 Step 19/36 Loss: 5.6304\n",
      "Epoch 2 Step 20/36 Loss: 5.4739\n",
      "Epoch 2 Step 21/36 Loss: 7.1628\n",
      "Epoch 2 Step 22/36 Loss: 4.3215\n",
      "Epoch 2 Step 23/36 Loss: 6.7248\n",
      "Epoch 2 Step 24/36 Loss: 6.6822\n",
      "Epoch 2 Step 25/36 Loss: 6.4933\n",
      "Epoch 2 Step 26/36 Loss: 4.4577\n",
      "Epoch 2 Step 27/36 Loss: 5.6666\n",
      "Epoch 2 Step 28/36 Loss: 5.5106\n",
      "Epoch 2 Step 29/36 Loss: 4.4025\n",
      "Epoch 2 Step 30/36 Loss: 5.9385\n",
      "Epoch 2 Step 31/36 Loss: 3.8842\n",
      "Epoch 2 Step 32/36 Loss: 5.3280\n",
      "Epoch 2 Step 33/36 Loss: 4.6904\n",
      "Epoch 2 Step 34/36 Loss: 5.0101\n",
      "Epoch 2 Step 35/36 Loss: 5.7164\n",
      "Epoch 2 Step 36/36 Loss: 5.4701\n",
      "Epoch 2 Step 1/36 Loss: 5.0978\n",
      "Epoch 2 Step 2/36 Loss: 5.2613\n",
      "Epoch 2 Step 3/36 Loss: 3.8486\n",
      "Epoch 2 Step 4/36 Loss: 6.4053\n",
      "Epoch 2 Step 5/36 Loss: 6.3778\n",
      "Epoch 2 Step 6/36 Loss: 5.9524\n",
      "Epoch 2 Step 7/36 Loss: 4.1677\n",
      "Epoch 2 Step 8/36 Loss: 6.7294\n",
      "Epoch 2 Step 9/36 Loss: 5.1543\n",
      "Epoch 2 Step 10/36 Loss: 5.2590\n",
      "Epoch 2 Step 11/36 Loss: 5.4823\n",
      "Epoch 2 Step 12/36 Loss: 6.6624\n",
      "Epoch 2 Step 13/36 Loss: 6.3604\n",
      "Epoch 2 Step 14/36 Loss: 4.1886\n",
      "Epoch 2 Step 15/36 Loss: 5.7040\n",
      "Epoch 2 Step 16/36 Loss: 5.3281\n",
      "Epoch 2 Step 17/36 Loss: 4.8098\n",
      "Epoch 2 Step 18/36 Loss: 5.3646\n",
      "Epoch 2 Step 19/36 Loss: 5.2400\n",
      "Epoch 2 Step 20/36 Loss: 5.0499\n",
      "Epoch 2 Step 21/36 Loss: 5.3096\n",
      "Epoch 2 Step 22/36 Loss: 6.9210\n",
      "Epoch 2 Step 23/36 Loss: 5.8716\n",
      "Epoch 2 Step 24/36 Loss: 4.9454\n",
      "Epoch 2 Step 25/36 Loss: 5.6858\n",
      "Epoch 2 Step 26/36 Loss: 5.7475\n",
      "Epoch 2 Step 27/36 Loss: 5.7092\n",
      "Epoch 2 Step 28/36 Loss: 6.6325\n",
      "Epoch 2 Step 29/36 Loss: 6.1240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 140\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# just test\u001b[39;00m\n\u001b[1;32m    138\u001b[0m advantages \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;66;03m# (ri - mean(R))/std(R) to do\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m old_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Get logits from both models\u001b[39;00m\n\u001b[1;32m    143\u001b[0m model_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# shape: (batch, seq_len, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/starcoder2/modeling_starcoder2.py:839\u001b[0m, in \u001b[0;36mStarcoder2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/starcoder2/modeling_starcoder2.py:542\u001b[0m, in \u001b[0;36mStarcoder2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 542\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    547\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    548\u001b[0m     hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    549\u001b[0m )  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/starcoder2/modeling_starcoder2.py:637\u001b[0m, in \u001b[0;36mStarcoder2Model._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    636\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mdtype, input_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 637\u001b[0m min_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    638\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# SlidingWindowCache or StaticCache\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        # Combine Q and A into a single text\n",
    "        content = f\"Q: {q}\\nA: {a}\"  \n",
    "        train_examples.append({\"content\": content})\n",
    "\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "\n",
    "# Use the load_qa_dataset function\n",
    "train_data = load_qa_dataset(qa_json_path)\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# -------------------------------\n",
    "# Model & Tokenizer Setup\n",
    "# -------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Set pad token if not present (using EOS token as pad token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenization Function\n",
    "# -------------------------------\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",  # pad all examples to max_length\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# -------------------------------\n",
    "# Data Collator and DataLoader\n",
    "# -------------------------------\n",
    "# DataCollatorForLanguageModeling automatically creates a \"labels\" field equal to input_ids\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "batch_size = 1  # Increased batch size\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model & Set Training Mode\n",
    "# -------------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizer Setup with Adafactor\n",
    "# -------------------------------\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    lr=1e-4,               # Learning rate can be tuned\n",
    "    relative_step=False,   # Set to True to use relative step sizes\n",
    "    scale_parameter=False  # Adjust scaling based on model size\n",
    ")\n",
    "\n",
    "epsilon = 0.01\n",
    "\n",
    "# Setup AMP GradScaler for 16-bit training (float16)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a reference model (deep copy) and set it to eval mode.\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model = ref_model.half()\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "kl_lambda = 0.2  # Weight for the KL divergence term\n",
    "\n",
    "# -------------------------------\n",
    "# Manual Training Loop with AMP (float16)\n",
    "# -------------------------------\n",
    "num_epochs = 1000\n",
    "num_grpo = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    old_model = None\n",
    "        \n",
    "    old_model = copy.deepcopy(model)\n",
    "    old_model = old_model.half()\n",
    "    old_model.eval()\n",
    "    for param in old_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for grpo_idx in range(num_grpo):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # Move batch tensors to device\n",
    "            batch = {k: v.to(device).repeat_interleave(3, dim=0) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with AMP autocast (float16)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**batch)  # Data collator already provides \"labels\"\n",
    "                loss = outputs.loss\n",
    "    \n",
    "                # just test\n",
    "                advantages = loss # (ri - mean(R))/std(R) to do\n",
    "    \n",
    "                old_outputs = old_model(**batch)\n",
    "    \n",
    "                # Get logits from both models\n",
    "                model_logits = outputs.logits  # shape: (batch, seq_len, vocab_size)\n",
    "                old_model_logits = outputs.logits\n",
    "                ref_outputs = ref_model(**batch)\n",
    "                ref_logits = ref_outputs.logits  # same shape as model_logits\n",
    "                \n",
    "                probability_ratio = model_logits / old_model_logits\n",
    "    \n",
    "                # Calculate the unclipped objective\n",
    "                unclipped_objective = probability_ratio * advantages\n",
    "                \n",
    "                # Calculate the clipped objective\n",
    "                clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                clipped_objective = clipped_ratio * advantages\n",
    "                \n",
    "                # Take the minimum of the unclipped and clipped objectives\n",
    "                ppo_loss = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "            \n",
    "                # Compute log-probabilities and probabilities\n",
    "                model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                ref_log_probs = F.softmax(ref_logits, dim=-1)\n",
    "            \n",
    "                # Compute KL divergence (using batchmean reduction)\n",
    "                kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "    \n",
    "                # Combine the primary loss and the KL divergence loss\n",
    "                combined_loss = ppo_loss + kl_lambda * kl_div\n",
    "    \n",
    "            \n",
    "            # Backward pass with scaled loss\n",
    "            scaler.scale(combined_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += combined_loss.item()\n",
    "\n",
    "            if step%10==0:\n",
    "                print(f\"Epoch {epoch+1} Grpo {grpo_idx} Step {step+1}/{len(dataloader)} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "output_dir = \"./starcoder2-3b-finetuned_adafactor_fp16\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a02c088-0c66-4e14-a259-d953beb37ecb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d043a78db45e4fcab5f2801c066ea029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2215/524043958.py:99: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_2215/524043958.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Grpo 0 Step 1/36 Loss: 6.5010  PPO_Loss: -6.4648  KL: 1707.8895\n",
      "Epoch 1 Grpo 0 Step 11/36 Loss: 5.7144  PPO_Loss: -5.7070  KL: 2159.4663\n",
      "Epoch 1 Grpo 0 Step 21/36 Loss: 23.6110  PPO_Loss: -23.3906  KL: 5228.3813\n",
      "Epoch 1 Grpo 0 Step 31/36 Loss: 7.9457  PPO_Loss: -7.9219  KL: 1789.4861\n",
      "Epoch 1 Grpo 1 Step 1/36 Loss: 7.3138  PPO_Loss: -7.2461  KL: 910.1036\n",
      "Epoch 1 Grpo 1 Step 11/36 Loss: 8.2675  PPO_Loss: -8.2500  KL: 1505.7478\n",
      "Epoch 1 Grpo 1 Step 21/36 Loss: 6.8513  PPO_Loss: -6.7969  KL: 1011.8757\n",
      "Epoch 1 Grpo 1 Step 31/36 Loss: 8.4621  PPO_Loss: -8.3906  KL: 1135.5599\n",
      "Epoch 1 Grpo 2 Step 1/36 Loss: 8.3984  PPO_Loss: -8.3281  KL: 1114.5754\n",
      "Epoch 1 Grpo 2 Step 11/36 Loss: 7.6475  PPO_Loss: -7.6250  KL: 1377.2168\n",
      "Epoch 1 Grpo 2 Step 21/36 Loss: 7.4992  PPO_Loss: -7.4453  KL: 1041.7146\n",
      "Epoch 1 Grpo 2 Step 31/36 Loss: 6.6669  PPO_Loss: -6.6055  KL: 1267.8304\n",
      "Epoch 1 Grpo 3 Step 1/36 Loss: 7.3089  PPO_Loss: -7.2734  KL: 1055.5793\n",
      "Epoch 1 Grpo 3 Step 11/36 Loss: 7.0689  PPO_Loss: -7.0039  KL: 895.2922\n",
      "Epoch 1 Grpo 3 Step 21/36 Loss: 6.3299  PPO_Loss: -6.3125  KL: 1381.7087\n",
      "Epoch 1 Grpo 3 Step 31/36 Loss: 8.0998  PPO_Loss: -8.0312  KL: 729.5737\n",
      "Epoch 1 Grpo 4 Step 1/36 Loss: 8.4263  PPO_Loss: -8.3672  KL: 1140.9287\n",
      "Epoch 1 Grpo 4 Step 11/36 Loss: 7.1709  PPO_Loss: -7.1328  KL: 1150.8433\n",
      "Epoch 1 Grpo 4 Step 21/36 Loss: 6.8667  PPO_Loss: -6.8047  KL: 822.3977\n",
      "Epoch 1 Grpo 4 Step 31/36 Loss: 6.4988  PPO_Loss: -6.4375  KL: 561.7172\n",
      "Epoch 1 Grpo 5 Step 1/36 Loss: 6.5954  PPO_Loss: -6.5703  KL: 1124.1859\n",
      "Epoch 1 Grpo 5 Step 11/36 Loss: 7.7565  PPO_Loss: -7.6875  KL: 713.4058\n",
      "Epoch 1 Grpo 5 Step 21/36 Loss: 7.4438  PPO_Loss: -7.3828  KL: 830.8142\n",
      "Epoch 1 Grpo 5 Step 31/36 Loss: 7.9005  PPO_Loss: -7.8633  KL: 1552.7485\n",
      "Epoch 1 Grpo 6 Step 1/36 Loss: 7.5947  PPO_Loss: -7.5352  KL: 771.1943\n",
      "Epoch 1 Grpo 6 Step 11/36 Loss: 6.9665  PPO_Loss: -6.9023  KL: 560.3923\n",
      "Epoch 1 Grpo 6 Step 21/36 Loss: 6.9887  PPO_Loss: -6.9375  KL: 865.3859\n",
      "Epoch 1 Grpo 6 Step 31/36 Loss: 6.7713  PPO_Loss: -6.7188  KL: 1003.1211\n",
      "Epoch 1 Grpo 7 Step 1/36 Loss: 6.5581  PPO_Loss: -6.5117  KL: 789.4164\n",
      "Epoch 1 Grpo 7 Step 11/36 Loss: 7.6020  PPO_Loss: -7.5508  KL: 1061.3771\n",
      "Epoch 1 Grpo 7 Step 21/36 Loss: 6.5873  PPO_Loss: -6.5234  KL: 840.4584\n",
      "Epoch 1 Grpo 7 Step 31/36 Loss: 9.1374  PPO_Loss: -9.0703  KL: 696.2426\n",
      "Epoch 1 Grpo 8 Step 1/36 Loss: 6.1148  PPO_Loss: -6.0664  KL: 699.7111\n",
      "Epoch 1 Grpo 8 Step 11/36 Loss: 8.2517  PPO_Loss: -8.1797  KL: 740.4221\n",
      "Epoch 1 Grpo 8 Step 21/36 Loss: 7.4535  PPO_Loss: -7.3945  KL: 1040.8778\n",
      "Epoch 1 Grpo 8 Step 31/36 Loss: 7.2215  PPO_Loss: -7.1523  KL: 594.7300\n",
      "Epoch 1 Grpo 9 Step 1/36 Loss: 7.0295  PPO_Loss: -6.9961  KL: 826.0951\n",
      "Epoch 1 Grpo 9 Step 11/36 Loss: 7.9479  PPO_Loss: -7.8750  KL: 792.7255\n",
      "Epoch 1 Grpo 9 Step 21/36 Loss: 6.9131  PPO_Loss: -6.8516  KL: 751.9869\n",
      "Epoch 1 Grpo 9 Step 31/36 Loss: 8.2573  PPO_Loss: -8.1953  KL: 798.2114\n",
      "Epoch 1 Grpo 10 Step 1/36 Loss: 9.1898  PPO_Loss: -9.1016  KL: 704.3132\n",
      "Epoch 1 Grpo 10 Step 11/36 Loss: 10.6455  PPO_Loss: -10.6016  KL: 1623.6547\n",
      "Epoch 1 Grpo 10 Step 21/36 Loss: 11.1187  PPO_Loss: -11.0703  KL: 1777.0004\n",
      "Epoch 1 Grpo 10 Step 31/36 Loss: 9.3259  PPO_Loss: -9.2812  KL: 1144.7324\n",
      "Epoch 1 Grpo 11 Step 1/36 Loss: 11.9022  PPO_Loss: -11.8516  KL: 3108.3438\n",
      "Epoch 1 Grpo 11 Step 11/36 Loss: 8.6678  PPO_Loss: -8.5859  KL: 623.3687\n",
      "Epoch 1 Grpo 11 Step 21/36 Loss: 6.3785  PPO_Loss: -6.3242  KL: 1041.3903\n",
      "Epoch 1 Grpo 11 Step 31/36 Loss: 11.0611  PPO_Loss: -10.9844  KL: 1282.7150\n",
      "Epoch 1 Grpo 12 Step 1/36 Loss: 9.3752  PPO_Loss: -9.3281  KL: 1404.3574\n",
      "Epoch 1 Grpo 12 Step 11/36 Loss: 11.1397  PPO_Loss: -11.0391  KL: 1574.4275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Backward pass with AMP\u001b[39;00m\n\u001b[1;32m    177\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(combined_loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 178\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    181\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m combined_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:352\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 352\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py:500\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             )\n\u001b[0;32m--> 500\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/optimization.py:940\u001b[0m, in \u001b[0;36mAdafactor.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    938\u001b[0m     exp_avg_sq \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 940\u001b[0m     \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(update, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2t))\n\u001b[1;32m    941\u001b[0m     update \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39mrsqrt()\u001b[38;5;241m.\u001b[39mmul_(grad)\n\u001b[1;32m    943\u001b[0m update\u001b[38;5;241m.\u001b[39mdiv_((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rms(update) \u001b[38;5;241m/\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter  # <-- (1) Import TensorBoard\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        # Combine Q and A into a single text\n",
    "        content = f\"Q: {q}\\nA: {a}\"\n",
    "        train_examples.append({\"content\": content})\n",
    "\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "\n",
    "# Use the load_qa_dataset function\n",
    "train_data = load_qa_dataset(qa_json_path)\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# -------------------------------\n",
    "# Model & Tokenizer Setup\n",
    "# -------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Set pad token if not present (using EOS token as pad token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenization Function\n",
    "# -------------------------------\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",  # pad all examples to max_length\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# -------------------------------\n",
    "# Data Collator and DataLoader\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "batch_size = 1  # You can adjust this\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model & Set Training Mode\n",
    "# -------------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizer Setup with Adafactor\n",
    "# -------------------------------\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,               # Learning rate can be tuned\n",
    "    relative_step=False,   # Set to True to use relative step sizes\n",
    "    scale_parameter=False  # Adjust scaling based on model size\n",
    ")\n",
    "\n",
    "epsilon = 0.01\n",
    "\n",
    "# Setup AMP GradScaler for 16-bit training (float16)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a reference model (deep copy) and set it to eval mode.\n",
    "ref_model = copy.deepcopy(model).half().eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "kl_lambda = 0.2  # Weight for the KL divergence term\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize TensorBoard Writer\n",
    "# -------------------------------\n",
    "writer = SummaryWriter(log_dir=\"runs/starcoder2_experiment\")  # (2) Initialize\n",
    "\n",
    "# -------------------------------\n",
    "# Manual Training Loop with AMP\n",
    "# -------------------------------\n",
    "num_epochs = 1000\n",
    "num_grpo = 100\n",
    "\n",
    "global_step = 0  # We'll track this across epochs for TensorBoard\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Keep a copy of the model for PPO-like ratio calculation\n",
    "    old_model = copy.deepcopy(model).half().eval()\n",
    "    for param in old_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for grpo_idx in range(num_grpo):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # Move batch tensors to device\n",
    "            batch = {k: v.to(device).repeat_interleave(3, dim=0) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Just an example advantage placeholder\n",
    "                advantages = loss  # If you have a real advantage calculation, replace it here\n",
    "\n",
    "                old_outputs = old_model(**batch)\n",
    "\n",
    "                # Get logits from both models\n",
    "                model_logits = outputs.logits          # shape: (batch, seq_len, vocab_size)\n",
    "                old_model_logits = old_outputs.logits  # shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "                ref_outputs = ref_model(**batch)\n",
    "                ref_logits = ref_outputs.logits\n",
    "\n",
    "                # Probability ratio\n",
    "                # In a real PPO scenario, you would convert logits -> log_probs -> exp(log_probs)\n",
    "                # for ratio calculations. This snippet is just a placeholder, illustrating usage.\n",
    "                probability_ratio = model_logits / (old_model_logits + 1e-8)\n",
    "\n",
    "                # Unclipped objective\n",
    "                unclipped_objective = probability_ratio * advantages\n",
    "\n",
    "                # Clipped objective\n",
    "                clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                clipped_objective = clipped_ratio * advantages\n",
    "\n",
    "                # PPO loss\n",
    "                _ppo_loss = clipped_objective # torch.min(unclipped_objective, clipped_objective)\n",
    "                ppo_loss= -_ppo_loss.mean()\n",
    "\n",
    "                # Calculate KL divergence\n",
    "                model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                ref_log_probs   = F.softmax(ref_logits, dim=-1)\n",
    "                kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "\n",
    "                # Combine the primary loss (ppo_loss) and the KL divergence loss\n",
    "                combined_loss = ppo_loss + kl_lambda * kl_div\n",
    "\n",
    "            # Backward pass with AMP\n",
    "            scaler.scale(combined_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += combined_loss.item()\n",
    "\n",
    "            # -------------------------------\n",
    "            # TensorBoard logging\n",
    "            # -------------------------------\n",
    "            writer.add_scalar(\"Loss/combined_loss\", combined_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/ppo_loss\", ppo_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/kl_div\", kl_div.item(), global_step)\n",
    "            # You can also log 'loss' from outputs if you want:\n",
    "            writer.add_scalar(\"Loss/original_loss\", loss.item(), global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} Grpo {grpo_idx} Step {step+1}/{len(dataloader)} \"\n",
    "                      f\"Loss: {loss.item():.4f}  PPO_Loss: {ppo_loss.item():.4f}  KL: {kl_div.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Close the TensorBoard writer at the end of training\n",
    "writer.close()\n",
    "\n",
    "# -------------------------------\n",
    "# Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "output_dir = \"./starcoder2-3b-finetuned_adafactor_fp16\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145ce4a5-2703-4960-97b9-b6ca27a9a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"runs/starcoder2_optuna_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce04779-389b-46e5-af66-91a876bfd02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c48160561c4bc6ab6208ce25033995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-18 01:56:08,190] A new study created in memory with name: no-name-34e90352-3486-4718-8417-2c9c3d18c81a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Trial hyperparameters -> lr: 6.43875785595471e-05, kl_lambda: 0.09934036423050208, epsilon: 0.07765466935288792, num_grpo: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16666/3952006775.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_16666/3952006775.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 304.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-18 01:58:07,700] Trial 0 failed with parameters: {'lr': 6.43875785595471e-05, 'kl_lambda': 0.09934036423050208, 'epsilon': 0.07765466935288792, 'num_grpo': 2} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_16666/3952006775.py\", line 222, in objective\n",
      "    final_avg_loss = train_and_evaluate(\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_16666/3952006775.py\", line 146, in train_and_evaluate\n",
      "    scaler.step(optimizer)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\", line 457, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\", line 352, in _maybe_opt_step\n",
      "    retval = optimizer.step(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 500, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/optimization.py\", line 944, in step\n",
      "    update.mul_(lr)\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-18 01:58:07,705] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Create study to minimize final loss\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 245\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can increase n_trials\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[1], line 222\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    219\u001b[0m scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Train & get final metric\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m final_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_grpo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_grpo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkl_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Return the final average loss to Optuna\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_avg_loss\n",
      "Cell \u001b[0;32mIn[1], line 146\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, ref_model, dataloader, optimizer, device, num_epochs, num_grpo, epsilon, kl_lambda, scaler)\u001b[0m\n\u001b[1;32m    143\u001b[0m     combined_loss \u001b[38;5;241m=\u001b[39m ppo_loss \u001b[38;5;241m+\u001b[39m kl_lambda \u001b[38;5;241m*\u001b[39m kl_div\n\u001b[1;32m    145\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(combined_loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 146\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    149\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m combined_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:352\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 352\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py:500\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             )\n\u001b[0;32m--> 500\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/optimization.py:944\u001b[0m, in \u001b[0;36mAdafactor.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    941\u001b[0m     update \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39mrsqrt()\u001b[38;5;241m.\u001b[39mmul_(grad)\n\u001b[1;32m    943\u001b[0m update\u001b[38;5;241m.\u001b[39mdiv_((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rms(update) \u001b[38;5;241m/\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m))\n\u001b[0;32m--> 944\u001b[0m \u001b[43mupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_first_moment:\n\u001b[1;32m    947\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"Q: {q}\\nA: {a}\"\n",
    "        train_examples.append({\"content\": content})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "train_data = load_qa_dataset(qa_json_path)\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    ref_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    num_grpo,\n",
    "    epsilon,\n",
    "    kl_lambda,\n",
    "    scaler\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for `num_epochs` with `num_grpo` PPO groups each epoch,\n",
    "    and return a metric (e.g., final average loss).\n",
    "    \"\"\"\n",
    "    # Initialize TensorBoard writer (optional)\n",
    "    writer = SummaryWriter(log_dir=\"runs/starcoder2_optuna_experiment\")\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        old_model = None\n",
    "        old_model = copy.deepcopy(model)\n",
    "        old_model = old_model.half()\n",
    "        old_model.eval()\n",
    "        for param in old_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for grpo_idx in range(num_grpo):\n",
    "            for step, batch in enumerate(dataloader):\n",
    "                batch = {k: v.to(device).repeat_interleave(3, dim=0) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    # A placeholder for advantage (you'd replace this with real advantage if doing PPO)\n",
    "                    advantages = -loss\n",
    "\n",
    "                    # old model forward\n",
    "                    with torch.no_grad():\n",
    "                        old_outputs = old_model(**batch)\n",
    "\n",
    "                    model_logits     = outputs.logits\n",
    "                    old_model_logits = old_outputs.logits\n",
    "\n",
    "                    # reference model forward\n",
    "                    with torch.no_grad():\n",
    "                        ref_outputs = ref_model(**batch)\n",
    "                    ref_logits = ref_outputs.logits\n",
    "\n",
    "                    # Probability ratio\n",
    "                    # In real PPO, you'd convert logits -> log_probs, then ratio = exp(new_log_prob - old_log_prob)\n",
    "                    probability_ratio = model_logits / (old_model_logits + 1e-8)\n",
    "\n",
    "                    # Unclipped objective\n",
    "                    unclipped_objective = probability_ratio * advantages\n",
    "\n",
    "                    # Clipped objective\n",
    "                    clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                    clipped_objective = clipped_ratio * advantages\n",
    "\n",
    "                    _ppo_loss = clipped_objective # torch.min(unclipped_objective, clipped_objective)\n",
    "                    ppo_loss = -_ppo_loss.mean()\n",
    "\n",
    "                    # KL\n",
    "                    model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                    ref_log_probs   = F.softmax(ref_logits, dim=-1)\n",
    "                    kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "\n",
    "                    combined_loss = ppo_loss + kl_lambda * kl_div\n",
    "\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += combined_loss.item()\n",
    "\n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar(\"Loss/combined_loss\", combined_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/ppo_loss\", ppo_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/kl_div\", kl_div.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/original_loss\", loss.item(), global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "    # Return final average loss as the metric to minimize\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Optuna Objective Function\n",
    "# ------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines how Optuna will run each trial:\n",
    "    - sample hyperparameters\n",
    "    - set up the model & optimizer with those\n",
    "    - run a short training loop\n",
    "    - return a metric (the final avg loss) to minimize\n",
    "    \"\"\"\n",
    "    # Shortened training for demonstration:\n",
    "    num_epochs = 2   # or 2–3, to save time during hyperparameter search\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    kl_lambda = trial.suggest_float(\"kl_lambda\", 0.0, 1.0)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.2)\n",
    "    num_grpo = trial.suggest_int(\"num_grpo\", 1, 3, step=1)\n",
    "\n",
    "    print(f\"[Optuna] Trial hyperparameters -> lr: {lr}, kl_lambda: {kl_lambda}, epsilon: {epsilon}, num_grpo: {num_grpo}\")\n",
    "\n",
    "    # Model & device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Reference model (for KL)\n",
    "    old_model = None\n",
    "    ref_model = copy.deepcopy(model).half().eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # DataLoader\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Adafactor(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        relative_step=False,\n",
    "        scale_parameter=False\n",
    "    )\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        dataloader=dataloader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        num_grpo=num_grpo,\n",
    "        epsilon=epsilon,\n",
    "        kl_lambda=kl_lambda,\n",
    "        scaler=scaler\n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Create study to minimize final loss\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "\n",
    "    print(\"Study completed!\")\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  Value: {best_trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Optionally, after the best params are found, you can do a full training\n",
    "    # run with the best hyperparameters, e.g.:\n",
    "    # best_params = best_trial.params\n",
    "    # ...\n",
    "    # re-initialize a new model & train fully using best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
