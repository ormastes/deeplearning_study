{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3803a7d-3036-4d33-80d0-3b16c39b92fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import and setup\n",
    "# ------------------------------------------------\n",
    "\n",
    "import pickle\n",
    "from Config import SimpleConfig\n",
    "\n",
    "config = SimpleConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df73530-3a74-4703-bf82-c027c639830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample prompt and sample data.\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Load and tokenize the prompt\n",
    "with open(\"manual_data_set/ExactSampleTrainSamplePrompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data_prompt = f.read()\n",
    "\n",
    "# Process test cases from file.\n",
    "with open(\"manual_data_set/ExactSampleTrainSample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a41e71-b542-4967-b41c-4ff148829b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = raw_data.split(\"\\n\")\n",
    "testcases = []\n",
    "heads = []\n",
    "head = []\n",
    "current_testcase = []\n",
    "in_Test_target_object = False\n",
    "in_Test_target = False\n",
    "got_head = False\n",
    "idx = 0\n",
    "for line in lines:\n",
    "    # print(line)\n",
    "    line = line.strip()\n",
    "    if line == \"<Test Case>\":\n",
    "        current_testcase.append(config.breason)\n",
    "        continue\n",
    "    if line == \"<Test Target Object>\":\n",
    "        in_Test_target_object = True\n",
    "    \n",
    "    if line == \"<Test Target>\":\n",
    "        in_Test_target = True\n",
    "\n",
    "    if line == \"<Clang-repl Test>\":\n",
    "        current_testcase.append(config.ereason)\n",
    "        current_testcase.append(config.banswer)\n",
    "\n",
    "    if got_head == False and (in_Test_target_object or in_Test_target):\n",
    "        head.append(line)\n",
    "        \n",
    "    if line == \"</Test Case>\":\n",
    "        current_testcase.append(config.eanswer)\n",
    "        testcases.append(current_testcase)\n",
    "        heads.append(head)\n",
    "        current_testcase = []\n",
    "        head = []\n",
    "        got_head = False\n",
    "    else:\n",
    "        current_testcase.append(line)\n",
    "    \n",
    "    if line == \"</Test Target Object>\":\n",
    "        in_Test_target_object = False\n",
    "        got_head = True\n",
    "    if line == \"</Test Target>\":\n",
    "        in_Test_target = False\n",
    "        got_head = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672dc7f6-7c2c-4753-922e-6d64c44a5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_samples = []\n",
    "samples = []\n",
    "sample_heads = []\n",
    "\n",
    "def check_eq(head1, head2):\n",
    "    if head1 is None:\n",
    "        return False\n",
    "    if len(head1) != len(head2):\n",
    "        return False\n",
    "    for idx in range(len(head1)):\n",
    "        if head1[idx] != head2[idx]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "last_head = None\n",
    "# Tokenize each test case\n",
    "for idx in range(len(testcases)):\n",
    "    testcase = testcases[idx]\n",
    "    text_case_text = \"\\n\".join(testcase).strip() + '\\n'\n",
    "    if check_eq(last_head, heads[idx]):\n",
    "        samples[-1] = samples[-1] + \"\\n<Need More Test/>\\n\" + text_case_text\n",
    "    else:\n",
    "        samples.append(text_case_text)\n",
    "        sample_heads.append(\"\\n\".join(heads[idx]))\n",
    "    last_head = heads[idx]\n",
    "\n",
    "#samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f1db3d-7106-4b26-9953-9e98adb9c673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 86])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://colab.research.google.com/github/TrelisResearch/llama-2-setup/blob/main/Llama_2_Prompt_and_Tokenizer_Format.ipynb#scrollTo=sQ4dBAJOovzz\n",
    "#\n",
    "# prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
    "# inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# <s> [INST] <<SYS>>\n",
    "# You are a helpful assistant.\n",
    "# <</SYS>>\n",
    "#\n",
    "# Howdy! [/INST]\n",
    "#\n",
    "# Well, howdy there! *adjusts cowboy hat* It's a pleasure to meet you! How can I help you today? Do you have any questions or tasks you'd like me to assist you with? Just let me\n",
    "\n",
    "# use startcoder\n",
    "#         content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_full_name)\n",
    "\n",
    "# For each sample, concatenate prompt tokens, sample tokens, and pad tokens for both fields.\n",
    "for i in range(len(samples)):\n",
    "    _sample = samples[i]\n",
    "    # config.bos + << automatically added to start\n",
    "    sample_front = \"### Instruction\\n\\n\" + sample_heads[i] + \"\\nWrtie a Clang-repl Test\\n\"\n",
    "    sample = sample_front \\\n",
    "        + \"\\n### Response\\n\\n\" \\\n",
    "        + _sample\n",
    "    samples[i] = sample\n",
    "\n",
    "# [print(sample) for sample in samples]\n",
    "tokenizer(sample_front, return_tensors=\"pt\")['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2f1eb4-fcef-4630-8904-53c75f0cf220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Config import SimpleConfig\n",
    "\n",
    "config = SimpleConfig()\n",
    "with open(config.dataset_file, \"wb\") as f:\n",
    "    pickle.dump(samples, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1df1694-13cc-490b-96ed-9bc903af3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for _sample in samples:\n",
    "    sample_token = tokenizer(_sample, return_tensors=\"pt\")\n",
    "    input_ids = sample_token['input_ids'][0]\n",
    "    global_samples.append(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4fce095-ddc7-46c6-b292-d1aa8c9b558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_samples\n",
    "global_samples = [atensor.numpy() for atensor in global_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d7785d-79f5-41b0-84a1-68e259082376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Config import SimpleConfig\n",
    "\n",
    "config = SimpleConfig()\n",
    "#with open(config.dataset_file, \"wb\") as f:\n",
    "#    pickle.dump(global_samples, f)\n",
    "#global_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cc171e-ac7d-458b-a2a0-f288610d4b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  222,    65,   396, 12513,  2023,   499]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(config.esys, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d5e73f-7bc6-4ba4-8da2-fcc142048164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502\n",
      "21052\n",
      "222\n",
      "222\n",
      "65\n",
      "1024\n",
      "10069\n",
      "2210\n",
      "67\n",
      "222\n",
      "1121\n",
      "3161\n",
      "18789\n",
      "51\n",
      "480\n",
      "461\n",
      "341\n",
      "3668\n",
      "51\n",
      "222\n",
      "396\n",
      "1024\n",
      "10069\n",
      "2210\n",
      "67\n",
      "222\n",
      "92\n",
      "2069\n",
      "1093\n",
      "331\n",
      "409\n",
      "1600\n",
      "50\n",
      "22101\n",
      "2128\n",
      "222\n",
      "222\n",
      "1502\n",
      "5178\n",
      "222\n",
      "222\n",
      "96\n",
      "36923\n",
      "98\n",
      "222\n",
      "65\n",
      "1024\n",
      "10069\n",
      "2210\n",
      "67\n",
      "222\n",
      "1121\n",
      "3161\n",
      "18789\n",
      "51\n",
      "480\n",
      "461\n",
      "341\n",
      "3668\n",
      "51\n",
      "222\n",
      "396\n",
      "1024\n",
      "10069\n",
      "2210\n",
      "67\n",
      "222\n",
      "65\n",
      "1024\n",
      "10069\n",
      "67\n",
      "222\n",
      "429\n",
      "1035\n",
      "45\n",
      "429\n",
      "331\n",
      "49\n",
      "648\n",
      "342\n",
      "46\n",
      "320\n",
      "222\n",
      "620\n",
      "331\n",
      "494\n",
      "342\n",
      "64\n",
      "222\n",
      "130\n",
      "222\n",
      "396\n",
      "1024\n",
      "10069\n",
      "67\n",
      "222\n",
      "65\n",
      "1024\n",
      "2210\n",
      "67\n",
      "222\n",
      "10574\n",
      "708\n",
      "1035\n",
      "3235\n",
      "341\n",
      "3831\n",
      "3668\n",
      "51\n",
      "222\n",
      "396\n",
      "1024\n",
      "2210\n",
      "67\n",
      "222\n",
      "65\n",
      "1630\n",
      "2426\n",
      "67\n",
      "222\n",
      "11978\n",
      "648\n",
      "331\n",
      "299\n",
      "244\n",
      "57\n",
      "64\n",
      "222\n",
      "11978\n",
      "648\n",
      "342\n",
      "299\n",
      "244\n",
      "59\n",
      "64\n",
      "222\n",
      "396\n",
      "1630\n",
      "2426\n",
      "67\n",
      "222\n",
      "65\n",
      "7705\n",
      "5601\n",
      "67\n",
      "222\n",
      "11978\n",
      "925\n",
      "3586\n",
      "1074\n",
      "630\n",
      "244\n",
      "54\n",
      "53\n",
      "64\n",
      "222\n",
      "1527\n",
      "222\n",
      "396\n",
      "7705\n",
      "5601\n",
      "67\n",
      "222\n",
      "36212\n",
      "36923\n",
      "98\n",
      "222\n",
      "96\n",
      "946\n",
      "5696\n",
      "546\n",
      "98\n",
      "222\n",
      "65\n",
      "72\n",
      "1600\n",
      "50\n",
      "22101\n",
      "2128\n",
      "67\n",
      "222\n",
      "11978\n",
      "453\n",
      "6566\n",
      "9925\n",
      "63\n",
      "222\n",
      "11978\n",
      "222\n",
      "11978\n",
      "453\n",
      "2128\n",
      "2210\n",
      "63\n",
      "12082\n",
      "708\n",
      "1035\n",
      "3235\n",
      "341\n",
      "3831\n",
      "3668\n",
      "51\n",
      "222\n",
      "11978\n",
      "453\n",
      "2128\n",
      "12029\n",
      "63\n",
      "2242\n",
      "2466\n",
      "1024\n",
      "222\n",
      "11978\n",
      "648\n",
      "331\n",
      "299\n",
      "244\n",
      "57\n",
      "64\n",
      "222\n",
      "11978\n",
      "648\n",
      "342\n",
      "299\n",
      "244\n",
      "59\n",
      "64\n",
      "222\n",
      "11978\n",
      "648\n",
      "1074\n",
      "299\n",
      "1035\n",
      "45\n",
      "102\n",
      "49\n",
      "342\n",
      "312\n",
      "222\n",
      "11978\n",
      "925\n",
      "3586\n",
      "1074\n",
      "630\n",
      "244\n",
      "54\n",
      "53\n",
      "64\n",
      "222\n",
      "1527\n",
      "222\n",
      "396\n",
      "72\n",
      "1600\n",
      "50\n",
      "22101\n",
      "2128\n",
      "67\n",
      "222\n",
      "36212\n",
      "946\n",
      "5696\n",
      "546\n",
      "98\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(global_samples[0])):\n",
    "    print(global_samples[0][i])\n",
    "    if config.esys == global_samples[0][i]:\n",
    "        print(i)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1cfba5-6bdc-4a44-8f48-6c0040fe546c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
