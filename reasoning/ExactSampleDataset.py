import torch
from torch.utils.data import Dataset, DataLoader
import json

class ExactSampleDataset(Dataset):
    """
    Custom Dataset to load training samples from a plain text dataset file.
    Each sample is delimited by the marker '####SAMPLE_END####'.
    """

    def __init__(self, tokenizer, config, transform=None):
        with open("ExactSampleTrainSamplePrompt.txt", "r", encoding="utf-8") as f:
            raw_data_prompt = f.read()
            raw_data_prompt_ids = tokenizer(raw_data_prompt, return_tensors="pt", padding=True)
        self.samples = []
        samples = []
        last = None
        with open("ExactSampleTrainSample.txt", "r", encoding="utf-8") as f:
            import xml.etree.ElementTree as ET
            raw_data = f.read()
            tree = ET.parse(raw_data)
            root = tree.getroot()
            for test in root.findall('TestCase'):
                if last is not None:
                    if last.find('Test Target').text.strip() == test.find('Test Target').text.strip():
                        samples[-1] = samples[-1] + ET.tostring(last, encoding="utf-8")
                    else:
                        samples.append(ET.tostring(last, encoding="utf-8"))
                last = test
        pad_token = tokenizer.pad_token_id
        pad_tensor = torch.tensor([pad_token*128], dtype=torch.long)
        for i in range(len(samples)):
            sample = torch.tensor(samples[i], dtype=torch.long)
            samples[i] = torch.cat([raw_data_prompt_ids, sample, pad_tensor], dim=1)

        for i in range(len(samples)):
            sample = samples[i]
            for i in range(1, len(sample)-128-1):
                start = min(0, i-128)
                self.samples.append(
                    (sample[start:i + config.context_len],
                     sample[start+1:i + config.context_len+1]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample_text, next_text = self.samples[idx]
        return idx, sample_text, next_text


# Example usage for training:
if __name__ == "__main__":
    dataset_file = "your_dataset_file.txt"  # Path to the file generated by the converter
    dataset = ExactSampleDataset(dataset_file)

    # Example: if you want to tokenize text before training, you can define a transform.
    # from transformers import AutoTokenizer
    # tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    # def tokenize_text(text):
    #     return tokenizer(text, return_tensors="pt", padding='max_length', truncation=True, max_length=128)
    # dataset = TextDataset(dataset_file, transform=tokenize_text)

    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    for batch in dataloader:
        # Each batch is a list of processed sample texts (or tokenized outputs)
        print(batch)
        # Proceed with your training loop here...
        break
