{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037d972-fa15-42bb-807e-10ffd3de8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets torch_optimizer lion_pytorch --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a399b2ba-6133-4e75-b6f9-2b903bae12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = None\n",
    "model = None\n",
    "old_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145ce4a5-2703-4960-97b9-b6ca27a9a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"runs/starcoder2_optuna_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e6eac3-718b-4ca7-82c9-8a745e065560",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./saved_models/prompt/prompt_epoch_91/checkpoint.pt ./saved_models/prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "238b7afe-d05e-4ff2-914c-85d01d64eb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the first transformer block: 95979008\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "    \n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    original_first_block =  self.model.layers[0]\n",
    "    \n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "    \n",
    "    self.model.layers.insert(0, new_block)\n",
    "    \n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def get_layer_params(self, layer_index: int = 0):\n",
    "    return list(self.model.layers[layer_index].parameters())\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage:\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "\n",
    "# Load the original model and its config.\n",
    "orig_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "config = orig_model.config\n",
    "\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "\n",
    "# Suppose your config model_type is \"starcoder2\".\n",
    "config.model_type = \"Starcoder2Model2\"\n",
    "\n",
    "\n",
    "# Register your custom model class.\n",
    "MODEL_FOR_CAUSAL_LM_MAPPING.register(CustomStarcoder2ForCausalLM, \"Starcoder2Model2\")\n",
    "\n",
    "# Now from_config will return your custom class.\n",
    "custom_model = CustomStarcoder2ForCausalLM.from_config(config)\n",
    "\n",
    "# (Optionally) load weights from the original model using strict=False.\n",
    "custom_model.load_state_dict(orig_model.state_dict(), strict=False)\n",
    "\n",
    "add_front_transformer_block(custom_model, copy_weights=True)\n",
    "first_block_params = get_layer_params(custom_model, layer_index=0)\n",
    "print(\"Number of parameters in the first transformer block:\",\n",
    "      sum(p.numel() for p in first_block_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf06e342-e541-484c-a00c-1e571e80d09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from saved checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2ef7118d50420a94794591f93f64e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n",
      "/tmp/ipykernel_74870/161677198.py:346: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "### REPLY\n",
      "\n",
      "### REPLY\n",
      "\n",
      "### REPLY \n",
      "\n",
      "### REPLY\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "Expected: Custom Clang-repl takes only one line input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74870/161677198.py:196: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 7.4236\n",
      "Checkpoint saved at epoch 1 to ./saved_models/prompt_epoch_1/checkpoint.pt\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "```\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "\n",
      "###\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 1): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "```\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "\n",
      "### Instruction\n",
      "\n",
      "```\n",
      "\n",
      "###\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 2 completed. Average Loss: 6.3832\n",
      "Epoch 3 completed. Average Loss: 5.8306\n",
      "Epoch 4 completed. Average Loss: 5.5871\n",
      "Epoch 5 completed. Average Loss: 5.4378\n",
      "Epoch 6 completed. Average Loss: 5.2738\n",
      "Epoch 7 completed. Average Loss: 4.9969\n",
      "Epoch 8 completed. Average Loss: 4.7247\n",
      "Epoch 9 completed. Average Loss: 4.5717\n",
      "Epoch 10 completed. Average Loss: 4.4958\n",
      "Epoch 11 completed. Average Loss: 4.3619\n",
      "Checkpoint saved at epoch 11 to ./saved_models/prompt_epoch_11/checkpoint.pt\n",
      "Sample Output (Epoch 11): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is the same as the prompt in the REPL.\n",
      "\n",
      "### Original\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 11): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "No, we don't.\n",
      "\n",
      "### Original\n",
      "\n",
      "### Original\n",
      "\n",
      "I'm\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 11): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "```python\n",
      "def reverse(s):\n",
      "\treturn s[::-1]\n",
      "```\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 12 completed. Average Loss: 4.1532\n",
      "Epoch 13 completed. Average Loss: 4.1835\n",
      "Epoch 14 completed. Average Loss: 4.0566\n",
      "Epoch 15 completed. Average Loss: 3.9009\n",
      "Epoch 16 completed. Average Loss: 3.7911\n",
      "Epoch 17 completed. Average Loss: 3.6690\n",
      "Epoch 18 completed. Average Loss: 3.6378\n",
      "Epoch 19 completed. Average Loss: 3.5340\n",
      "Epoch 20 completed. Average Loss: 3.4345\n",
      "Epoch 21 completed. Average Loss: 3.3479\n",
      "Checkpoint saved at epoch 21 to ./saved_models/prompt_epoch_21/checkpoint.pt\n",
      "Sample Output (Epoch 21): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is the same as the prompt in the REPL.\n",
      "\n",
      "### Original Proposal\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 21): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes, we allow multiline comments and backslash-extended lines in Custom Clang-repl\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 21): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "The reverse function is a function that reverses the string.\n",
      "\n",
      "### Response\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 22 completed. Average Loss: 3.4423\n",
      "Epoch 23 completed. Average Loss: 3.1556\n",
      "Epoch 24 completed. Average Loss: 3.2858\n",
      "Epoch 25 completed. Average Loss: 2.9763\n",
      "Epoch 26 completed. Average Loss: 2.9790\n",
      "Epoch 27 completed. Average Loss: 2.8560\n",
      "Epoch 28 completed. Average Loss: 2.7755\n",
      "Epoch 29 completed. Average Loss: 2.7457\n",
      "Epoch 30 completed. Average Loss: 2.6905\n",
      "Epoch 31 completed. Average Loss: 2.5592\n",
      "Checkpoint saved at epoch 31 to ./saved_models/prompt_epoch_31/checkpoint.pt\n",
      "Sample Output (Epoch 31): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is the text that appears before the prompt.\n",
      "### Response\n",
      "\n",
      "The prompt\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 31): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes, we allow multiline comments and backslash-extended lines in Custom Clang-repl\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 31): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "The reverse function is a function that reverses the string.\n",
      "\n",
      "### Original Problem\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 32 completed. Average Loss: 2.6262\n",
      "Epoch 33 completed. Average Loss: 2.4585\n",
      "Epoch 34 completed. Average Loss: 2.6547\n",
      "Epoch 35 completed. Average Loss: 2.4473\n",
      "Epoch 36 completed. Average Loss: 2.3966\n",
      "Epoch 37 completed. Average Loss: 2.5222\n",
      "Epoch 38 completed. Average Loss: 2.2937\n",
      "Epoch 39 completed. Average Loss: 2.3710\n",
      "Epoch 40 completed. Average Loss: 2.3064\n",
      "Epoch 41 completed. Average Loss: 2.2490\n",
      "Checkpoint saved at epoch 41 to ./saved_models/prompt_epoch_41/checkpoint.pt\n",
      "Sample Output (Epoch 41): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is the first line of the test file.\n",
      "### Response\n",
      "\n",
      "The prompt\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 41): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes.\n",
      "### Code\n",
      "\n",
      "```\n",
      "// Comment\n",
      "```\n",
      "\n",
      "### Response\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 41): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "The reverse function is used to reverse the string.\n",
      "### Response\n",
      "\n",
      "The reverse function\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 42 completed. Average Loss: 2.2223\n",
      "Epoch 43 completed. Average Loss: 2.1687\n",
      "Epoch 44 completed. Average Loss: 2.1986\n",
      "Epoch 45 completed. Average Loss: 2.1021\n",
      "Epoch 46 completed. Average Loss: 2.0437\n",
      "Epoch 47 completed. Average Loss: 2.1380\n",
      "Epoch 48 completed. Average Loss: 2.0440\n",
      "Epoch 49 completed. Average Loss: 1.9597\n",
      "Epoch 50 completed. Average Loss: 2.0013\n",
      "Epoch 51 completed. Average Loss: 1.9178\n",
      "Checkpoint saved at epoch 51 to ./saved_models/prompt_epoch_51/checkpoint.pt\n",
      "Sample Output (Epoch 51): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is the command to run in the test.\n",
      "### Response\n",
      "\n",
      "The prompt\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 51): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes.\n",
      "### Code\n",
      "\n",
      "```\n",
      "// This is a multiline comment.\n",
      "```\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 51): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "Use the following code:\n",
      "\n",
      "```\n",
      "def reverse(s):\n",
      "return s[\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 52 completed. Average Loss: 1.9661\n",
      "Epoch 53 completed. Average Loss: 1.9494\n",
      "Epoch 54 completed. Average Loss: 1.9833\n",
      "Epoch 55 completed. Average Loss: 1.8791\n",
      "Epoch 56 completed. Average Loss: 1.8369\n",
      "Epoch 57 completed. Average Loss: 1.8573\n",
      "Epoch 58 completed. Average Loss: 1.8289\n",
      "Epoch 59 completed. Average Loss: 1.8113\n",
      "Epoch 60 completed. Average Loss: 1.8016\n",
      "Epoch 61 completed. Average Loss: 1.7484\n",
      "Checkpoint saved at epoch 61 to ./saved_models/prompt_epoch_61/checkpoint.pt\n",
      "Sample Output (Epoch 61): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is used to separate the test code from the test output.\n",
      "### Response\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 61): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes, but only single-line comments are allowed.\n",
      "### Response\n",
      "\n",
      "Yes.\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 61): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "Use the following code:\n",
      "```\n",
      "def reverse(s):\n",
      "\treturn s[\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 62 completed. Average Loss: 1.7445\n",
      "Epoch 63 completed. Average Loss: 1.7411\n",
      "Epoch 64 completed. Average Loss: 1.7275\n",
      "Epoch 65 completed. Average Loss: 1.7708\n",
      "Epoch 66 completed. Average Loss: 1.7063\n",
      "Epoch 67 completed. Average Loss: 1.7281\n",
      "Epoch 68 completed. Average Loss: 1.7196\n",
      "Epoch 69 completed. Average Loss: 1.6961\n",
      "Epoch 70 completed. Average Loss: 1.6574\n",
      "Epoch 71 completed. Average Loss: 1.7336\n",
      "Checkpoint saved at epoch 71 to ./saved_models/prompt_epoch_71/checkpoint.pt\n",
      "Sample Output (Epoch 71): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is used to separate sections of code.\n",
      "### Response\n",
      "\n",
      "The prompt is\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 71): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes, but only single-line comments are allowed.\n",
      "### Response\n",
      "\n",
      "Yes.\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 71): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "Use the following code:\n",
      "```\n",
      "def reverse(s):\n",
      "return s[::-\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 72 completed. Average Loss: 1.6682\n",
      "Epoch 73 completed. Average Loss: 1.6828\n",
      "Epoch 74 completed. Average Loss: 1.6736\n",
      "Epoch 75 completed. Average Loss: 1.6111\n",
      "Epoch 76 completed. Average Loss: 1.6195\n",
      "Epoch 77 completed. Average Loss: 1.6113\n",
      "Epoch 78 completed. Average Loss: 1.6193\n",
      "Epoch 79 completed. Average Loss: 1.6157\n",
      "Epoch 80 completed. Average Loss: 1.5791\n",
      "Epoch 81 completed. Average Loss: 1.5985\n",
      "Checkpoint saved at epoch 81 to ./saved_models/prompt_epoch_81/checkpoint.pt\n",
      "Sample Output (Epoch 81): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is used to separate sections of code.\n",
      "### Response\n",
      "\n",
      "The prompt is\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 81): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes, but only single-line comments are allowed.\n",
      "### Response\n",
      "\n",
      "Yes.\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 81): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "''.join(reversed(python_string))\n",
      "### Response\n",
      "\n",
      "''.join\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 82 completed. Average Loss: 1.5655\n",
      "Epoch 83 completed. Average Loss: 1.5274\n",
      "Epoch 84 completed. Average Loss: 1.5603\n",
      "Epoch 85 completed. Average Loss: 1.5147\n",
      "Epoch 86 completed. Average Loss: 1.4777\n",
      "Epoch 87 completed. Average Loss: 1.5508\n",
      "Epoch 88 completed. Average Loss: 1.4641\n",
      "Epoch 89 completed. Average Loss: 1.4315\n",
      "Epoch 90 completed. Average Loss: 1.4818\n",
      "Epoch 91 completed. Average Loss: 1.5282\n",
      "Checkpoint saved at epoch 91 to ./saved_models/prompt_epoch_91/checkpoint.pt\n",
      "Sample Output (Epoch 91): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, What is the prompt in Custom Clang-repl?\n",
      "\n",
      "### Response\n",
      "\n",
      "The prompt is '>>> '.\n",
      "### Response\n",
      "\n",
      "The prompt is used to indicate the\n",
      "Expected: ```\n",
      ">>> (prompt)\n",
      "```\n",
      "Sample Output (Epoch 91): ### Instruction\n",
      "\n",
      "In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\n",
      "\n",
      "### Response\n",
      "\n",
      "Yes, but only single-line comments are allowed.\n",
      "### Response\n",
      "\n",
      "Yes.\n",
      "Expected: Custom Clang-repl takes only one line input.\n",
      "Sample Output (Epoch 91): ### Instruction\n",
      "\n",
      "Make python string reverse function\n",
      "\n",
      "### Response\n",
      "\n",
      "Use the'reversed' function.\n",
      "### Response\n",
      "\n",
      "Use the'reversed' function\n",
      "Expected: def reverse(text):\n",
      "    return reverse(text[1:])+text[0]\n",
      "=====================================================================================================\n",
      "Epoch 92 completed. Average Loss: 1.4041\n",
      "Epoch 93 completed. Average Loss: 1.4666\n",
      "Epoch 94 completed. Average Loss: 1.3867\n",
      "Epoch 95 completed. Average Loss: 1.3794\n",
      "Epoch 96 completed. Average Loss: 1.4414\n",
      "Epoch 97 completed. Average Loss: 1.3986\n",
      "Epoch 98 completed. Average Loss: 1.3990\n",
      "Epoch 99 completed. Average Loss: 1.3436\n",
      "Epoch 100 completed. Average Loss: 1.3313\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from datasets import Dataset\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "last_checkpoint_path = \"./saved_models/prompt/checkpoint.pt\"\n",
    "checkpoint_dir_pre = \"./saved_models/prompt/epoch_\"\n",
    "\n",
    "# num_iterations=1, num_steps=500, batch_size=4, num_generations=4, max_completion_length=128, kl=0.1,\n",
    "# learning_rate=5e-6, mu=3, epsilon=0.2,\n",
    "#\n",
    "# lr: 7.205691481165551e-05 kl_lambda: 0.2654706177039008 epsilon: 0.019437902361559744 num_grpo: 1\n",
    "# lr: 1.1111588431283189e-06 kl_lambda: 0.15842765249477542 epsilon: 0.11144786260484413 num_grpo: 3\n",
    "is_finding_opt=False\n",
    "if not is_finding_opt:\n",
    "    num_epochs=100\n",
    "    lr=1.1111588431283189e-06\n",
    "    kl_lambda=0.15842765249477542\n",
    "    epsilon=0.11144786260484413\n",
    "    num_grpo=1\n",
    "    save_epochs=10\n",
    "\n",
    "def object_hiper_param(trial):\n",
    "    # Shortened training for demonstration:\n",
    "    num_epochs = 1#2   # or 2–3, to save time during hyperparameter search\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    kl_lambda = trial.suggest_float(\"kl_lambda\", 0.0, 1.0)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.2)\n",
    "    num_grpo = trial.suggest_int(\"num_grpo\", 1, 3, step=1)\n",
    "\n",
    "    return num_epochs, lr, kl_lambda, epsilon, num_grpo\n",
    "\n",
    "\n",
    "def samping(model, tokenizer, device, epoch, writer, sample_prompt, expected):\n",
    "    # Include attention_mask in the tokenization\n",
    "    sample_prompt = f\"### Instruction\\n\\n{sample_prompt}\\n\\n### Response\"\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Pass the attention_mask and explicitly set pad_token_id to eos_token_id for reliable generation\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    sample_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    sample_text = sample_text.strip()\n",
    "    print(f\"Sample Output (Epoch {epoch+1}): {sample_text}\")\n",
    "    print(\"Expected:\", expected)\n",
    "    writer.add_text(\"Sample Output\", f\"Epoch {epoch+1}: {sample_text}\", epoch)\n",
    "\n",
    "def selective_log_softmax(logits, input_ids):\n",
    "    # https://blog.gopenai.com/coding-grpo-from-scratch-a-guide-to-distributed-implementation-with-qwen2-5-1-5b-instruct-59b34227edac\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "def add_front_transformer_block(self, copy_weights: bool = True):\n",
    "    # Retrieve the current first transformer block.\n",
    "    layer_index = 1\n",
    "    original_first_block =  self.model.layers[layer_index]\n",
    "    \n",
    "    # Create a new block.\n",
    "    new_block = copy.deepcopy(original_first_block) if copy_weights else type(original_first_block)()\n",
    "    \n",
    "    self.model.layers.insert(layer_index, new_block)\n",
    "    \n",
    "    self.config.num_hidden_layers += 1\n",
    "\n",
    "def get_layer_params(self, layer_index: int = 0):\n",
    "    first_params = list(self.model.layers[0].parameters())\n",
    "    sec_params = list(self.model.layers[0].parameters())\n",
    "    #last_params = list(self.model.layers[-1].parameters())\n",
    "    return first_params + sec_params# + last_params\n",
    "    \n",
    "# ------------------------------------------------\n",
    "# Load Q&A from JSON file (manual_data_set/QA.json)\n",
    "# and create a list of {\"content\": \"...\"}\n",
    "# ------------------------------------------------\n",
    "def load_qa_dataset(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_examples = []\n",
    "    for item in data:\n",
    "        q = item.get(\"Q\", \"\")\n",
    "        a = item.get(\"A\", \"\")\n",
    "        content = f\"### Instruction\\n\\n{q}\\n### Response\\n\\n{a}\\n\"\n",
    "        train_examples.append({\"content\": content})\n",
    "    return train_examples\n",
    "\n",
    "\n",
    "# Provide the path to your Q&A JSON file\n",
    "qa_json_path = \"manual_data_set/QA.json\"\n",
    "train_data = load_qa_dataset(qa_json_path)\n",
    "\n",
    "# Create a Hugging Face Dataset from the list\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Tokenization\n",
    "# ------------------------------------------------\n",
    "model_id = \"bigcode/starcoder2-3b\"\n",
    "# Load tokenizer from saved directory if exists; otherwise, load from pretrained.\n",
    "tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "if os.path.exists(tokenizer_save_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_dir)\n",
    "    print(\"Loaded tokenizer from saved checkpoint.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"content\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Training Function\n",
    "# ------------------------------------------------\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    ref_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    num_grpo,\n",
    "    epsilon,\n",
    "    kl_lambda,\n",
    "    scaler,\n",
    "    save_epochs,\n",
    "    start_epoch\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for `num_epochs` with `num_grpo` PPO groups each epoch,\n",
    "    and return a metric (e.g., final average loss).\n",
    "    \"\"\"\n",
    "    # Initialize TensorBoard writer (optional)\n",
    "    writer = SummaryWriter(log_dir=\"runs/starcoder2_optuna_experiment\")\n",
    "\n",
    "    # --- Generate sample output text after each epoch ---\n",
    "    model.eval()  # Switch to eval mode for generation\n",
    "    with torch.no_grad():\n",
    "        samping(model, tokenizer, device, 0, writer, \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\", \"```\\n>>> (prompt)\\n```\")\n",
    "        samping(model, tokenizer, device, 0, writer, \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\", \"Custom Clang-repl takes only one line input.\")\n",
    "    model.train()  # Switch back to training mode\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        old_model = None\n",
    "        old_model = copy.deepcopy(model)\n",
    "        old_model = old_model.half()\n",
    "        old_model.eval()\n",
    "        for param in old_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for grpo_idx in range(num_grpo):\n",
    "            for step, batch in enumerate(dataloader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    # A placeholder for advantage (you'd replace this with real advantage if doing PPO)\n",
    "                    advantages = -loss\n",
    "\n",
    "                    # old model forward\n",
    "                    with torch.no_grad():\n",
    "                        old_outputs = old_model(**batch)\n",
    "\n",
    "                    model_logits     = outputs.logits\n",
    "                    old_model_logits = old_outputs.logits\n",
    "\n",
    "                    # reference model forward\n",
    "                    with torch.no_grad():\n",
    "                        ref_outputs = ref_model(**batch)\n",
    "                    ref_logits = ref_outputs.logits\n",
    "\n",
    "                    # Probability ratio\n",
    "                    # In real PPO, you'd convert logits -> log_probs, then ratio = exp(new_log_prob - old_log_prob)\n",
    "                    probability_ratio = model_logits / (old_model_logits + 1e-8)\n",
    "\n",
    "                    # Unclipped objective\n",
    "                    unclipped_objective = probability_ratio * advantages\n",
    "\n",
    "                    # Clipped objective\n",
    "                    clipped_ratio = torch.clamp(probability_ratio, 1 - epsilon, 1 + epsilon)\n",
    "                    clipped_objective = clipped_ratio * advantages\n",
    "\n",
    "                    #ppo_loss = -clipped_objective.mean()\n",
    "                    #ppo_loss = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "                    ppo_loss = loss.mean()\n",
    "\n",
    "                    # KL\n",
    "                    model_log_probs = F.log_softmax(model_logits, dim=-1)\n",
    "                    ref_log_probs   = F.softmax(ref_logits, dim=-1)\n",
    "                    kl_div = F.kl_div(model_log_probs, ref_log_probs, reduction='batchmean')\n",
    "\n",
    "                    combined_loss = ppo_loss #+ kl_lambda * kl_div\n",
    "\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += combined_loss.item()\n",
    "\n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar(\"Loss/combined_loss\", combined_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/ppo_loss\", ppo_loss.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/kl_div\", kl_div.item(), global_step)\n",
    "                writer.add_scalar(\"Loss/original_loss\", loss.item(), global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Epoch/Average_Loss\", avg_loss, epoch+1)\n",
    "\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1\n",
    "        }\n",
    "        torch.save(checkpoint, last_checkpoint_path)\n",
    "\n",
    "        if save_epochs is not None and epoch%save_epochs == 0:\n",
    "            global checkpoint_dir_pre\n",
    "            checkpoint_dir = checkpoint_dir_pre + str(epoch+1)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch + 1\n",
    "            }\n",
    "            checkpoint_path = os.path.join(last_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1} to {checkpoint_path}\")\n",
    "\n",
    "            # Save tokenizer only if it has not been saved before.\n",
    "            tokenizer_save_dir = \"./saved_models/tokenizer\"\n",
    "            if not os.path.exists(tokenizer_save_dir):\n",
    "                os.makedirs(tokenizer_save_dir, exist_ok=True)\n",
    "                tokenizer.save_pretrained(tokenizer_save_dir)\n",
    "                print(\"Tokenizer saved.\")\n",
    "\n",
    "            # --- Generate sample output text after each epoch ---\n",
    "            model.eval()  # Switch to eval mode for generation\n",
    "            with torch.no_grad():\n",
    "                samping(model, tokenizer, device, epoch, writer, \"In Custom Clang-repl, What is the prompt in Custom Clang-repl?\", \"```\\n>>> (prompt)\\n```\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"In Custom Clang-repl, Do we allow multiline comments or backslash-extended lines in Custom Clang-repl Test?\", \"Custom Clang-repl takes only one line input.\")\n",
    "                samping(model, tokenizer, device, epoch, writer, \"Make python string reverse function\", \"def reverse(text):\\n    return reverse(text[1:])+text[0]\")\n",
    "\n",
    "                print(\"=====================================================================================================\")\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "    # Return final average loss as the metric to minimize\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        kl_lambda,\n",
    "        epsilon,\n",
    "        num_grpo,\n",
    "        save_epochs=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Check if a latest checkpoint exists to load model and optimizer states\n",
    "    if os.path.exists(last_checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path} at epoch {start_epoch}\")\n",
    "    else:\n",
    "        _model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        config = _model.config\n",
    "        config.num_hidden_layers += 2\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        model.load_state_dict(_model.state_dict(), strict=False)\n",
    "        for i in range(len(model.model.layers)-1, 0, -1):\n",
    "            model.model.layers[i].load_state_dict(model.model.layers[i-1].state_dict())\n",
    "        for i in range(len(model.model.layers)-1, 0, -1):\n",
    "            model.model.layers[i].load_state_dict(model.model.layers[i-1].state_dict())\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer = Adafactor(get_layer_params(model), lr=lr, relative_step=False, scale_parameter=False)\n",
    "        start_epoch = 0\n",
    "        \n",
    "    #for name, module in model.named_modules():\n",
    "    #    print(f\"{name}: {module}\")    \n",
    "\n",
    "    # Reference model (for KL)\n",
    "    old_model = None\n",
    "    ref_model = copy.deepcopy(model).half().eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # DataLoader\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # AMP GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Train & get final metric\n",
    "    final_avg_loss = train_and_evaluate(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        dataloader=dataloader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        num_grpo=num_grpo,\n",
    "        epsilon=epsilon,\n",
    "        kl_lambda=kl_lambda,\n",
    "        scaler=scaler,\n",
    "        save_epochs=save_epochs,\n",
    "        start_epoch=start_epoch\n",
    "    )\n",
    "\n",
    "    # Return the final average loss to Optuna\n",
    "    return final_avg_loss\n",
    "    \n",
    "# ------------------------------------------------\n",
    "# Optuna Objective Function\n",
    "# ------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines how Optuna will run each trial:\n",
    "    - sample hyperparameters\n",
    "    - set up the model & optimizer with those\n",
    "    - run a short training loop\n",
    "    - return a metric (the final avg loss) to minimize\n",
    "    \"\"\"\n",
    "    num_epochs, lr, kl_lambda, epsilon, num_grpo = object_hiper_param(trial)\n",
    "\n",
    "    print(f\"[Optuna] Trial hyperparameters -> lr: {lr}, kl_lambda: {kl_lambda}, epsilon: {epsilon}, num_grpo: {num_grpo}\")\n",
    "    return train( \n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        kl_lambda=kl_lambda,\n",
    "        epsilon=epsilon,\n",
    "        num_grpo=num_grpo)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    if is_finding_opt:\n",
    "        # Create study to minimize final loss\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=5)  # You can increase n_trials\n",
    "    \n",
    "        print(\"Study completed!\")\n",
    "        print(\"Best trial:\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Value: {best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"#    {key}: {value}\")\n",
    "        # Study completed!\n",
    "        # Best trial:\n",
    "        #  Value: 715.3611988491482\n",
    "        #  Params: \n",
    "        #    lr: 0.0002746775018590349\n",
    "        #    kl_lambda: 0.10527608699361579\n",
    "        #    epsilon: 0.12442505216944565\n",
    "        #    num_grpo: 2\n",
    "    else:\n",
    "        train(\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            kl_lambda=kl_lambda,\n",
    "            epsilon=epsilon,\n",
    "            num_grpo=num_grpo,\n",
    "            save_epochs=save_epochs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71048004-5b1c-4335-a0cd-bbafca1d9433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d453b-7836-427d-a3f7-522bf7b4d197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
